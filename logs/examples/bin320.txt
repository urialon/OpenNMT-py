Context: 
@ override void init ( table _ config table _ config , instance _ zk _ metadata instance _ metadata , schema schema ) { this . indexing _ schema = schema ; if ( instance _ metadata != null ) { this . group _ id = instance _ metadata . get _ group _ id ( PRED ) ; } kafka _ stream _ metadata kafka _ metadata = new kafka _ stream _ metadata ( table _ config . get _ indexing _ config ( ) . get _ stream _ configs ( ) ) ; this . kafka _ topic _ name = kafka _ metadata . get _ kafka _ topic _ name ( ) ; this . decode _ klass = kafka _ metadata . get _ decoder _ class ( ) ; this . decoder _ props = kafka _ metadata . get _ decoder _ properties ( ) ; this . kafka _ consumer _ props = kafka _ metadata . get _ kafka _ consumer _ properties ( ) ; this . zk _ string = kafka _ metadata . get _ zk _ broker _ url ( ) ; if ( table _ config . get _ indexing _ config ( ) . get _ stream _ configs ( ) . contains _ key ( helix . data _ source . realtime . realtime _ segment _ flush _ size ) ) { realtime _ records _ threshold = integer . parse _ int ( table _ config . get _ indexing _ config ( ) . get _ stream _ configs ( ) . get ( helix . data _ source . realtime . realtime _ segment _ flush _ size ) ) ; } if ( table _ config . get _ indexing _ config ( ) . get _ stream _ configs ( ) . contains _ key ( helix . data _ source . realtime . realtime _ segment _ flush _ time ) ) { segment _ time _ in _ millis = convert _ to _ ms ( table _ config . get _ indexing _ config ( ) . get _ stream _ configs ( ) . get ( helix . data _ source . realtime . realtime _ segment _ flush _ time ) ) ; } }
Ground truth: table_config.get_table_name()
Syntactic prediction: table_config.get_table_name()
Baseline prediction: table_config.get_indexing_config().get_stream_name()

Context: 
* todo _ : add support for more efficient bulk operations _ . * * we don _ 't want to acquire the lock for every iteration _ , but we * also want other threads a chance to interact with the * collection _ , especially when count is close to capacity _ . */ // /** // * adds all of the elements in the specified collection to this // * queue _ . attempts to add _ all of a queue to itself result in // * {@code illegal _ argument _ exception _ }. further _ , the behavior of // * this operation is undefined if the specified collection is // * modified while the operation is in progress _ . // * // * @param c collection containing elements to be added to this queue // * @return {@code true _ } if this queue changed as a result of the call // * @throws class _ cast _ exception {@inheritdoc} // * @throws null _ pointer _ exception {@inheritdoc} // * @throws illegal _ argument _ exception {@inheritdoc} // * @throws illegal _ state _ exception if this deque is full // * @see #add(object) // */ // boolean add _ all _ (collection<? extends e _ > c _ ) { // if (c == null _ ) // throw new null _ pointer _ exception _ (); // if (c == this _ ) // throw new illegal _ argument _ exception _ (); // final reentrant _ lock lock = this _ .lock; // lock _ .lock(); // try { // boolean modified = false _ ; // for (e e : c _ ) // if (linklast(e)) // modified = true _ ; // return modified _ ; // } finally { // lock _ .unlock(); // } // } /***/ @suppresswarnings("unchecked") object _ [] to _ array _ () { final reentrant _ lock lock = this _ .lock; lock _ .lock(); try { object _ [] a = new object _ [count]; int k = 0 _ ; for (node<e> p = first _ ; p != null _ ; p = p _ .next) PRED = p . item ; return a ; } finally { lock . unlock ( ) ; } }
Ground truth: a[k++]
Syntactic prediction: a[k++]
Baseline prediction: a.item

Context: 
@ override p _ collection < kv < k , iterable < v > > > expand ( p _ collection < kv < k , iterable < windowed _ value < v > > > > input ) { @ suppress _ warnings ( " _ unchecked _ " ) kv _ coder < k , iterable < windowed _ value < v > > > input _ kv _ coder = ( kv _ coder < k , iterable < windowed _ value < v > > > ) input . get _ coder ( ) ; coder < k > key _ coder = PRED ; coder < iterable < windowed _ value < v > > > input _ value _ coder = input _ kv _ coder . get _ value _ coder ( ) ; iterable _ coder < windowed _ value < v > > input _ iterable _ value _ coder = ( iterable _ coder < windowed _ value < v > > ) input _ value _ coder ; coder < windowed _ value < v > > input _ iterable _ element _ coder = input _ iterable _ value _ coder . get _ elem _ coder ( ) ; windowed _ value _ coder < v > input _ iterable _ windowed _ value _ coder = ( windowed _ value _ coder < v > ) input _ iterable _ element _ coder ; coder < v > input _ iterable _ element _ value _ coder = input _ iterable _ windowed _ value _ coder . get _ value _ coder ( ) ; coder < iterable < v > > output _ value _ coder = iterable _ coder . of ( input _ iterable _ element _ value _ coder ) ; coder < kv < k , iterable < v > > > output _ kv _ coder = kv _ coder . of ( key _ coder , output _ value _ coder ) ; return p _ collection . create _ primitive _ output _ internal ( input . get _ pipeline ( ) , windowing _ strategy , input . is _ bounded ( ) , output _ kv _ coder ) ; }
Ground truth: input_kv_coder.get_key_coder()
Syntactic prediction: input_kv_coder.get_key_coder()
Baseline prediction: key_coder.get_value_coder()

Context: 
runner _ api . windowing _ strategy to _ proto ( windowing _ strategy < ? , ? > windowing _ strategy , sdk _ components components ) throws io _ exception { sdk _ function _ spec window _ fn _ spec = to _ proto ( windowing _ strategy . get _ window _ fn ( ) , components ) ; runner _ api . windowing _ strategy . builder windowing _ strategy _ proto = runner _ api . windowing _ strategy . new _ builder ( ) . set _ output _ time ( to _ proto ( windowing _ strategy . get _ timestamp _ combiner ( ) ) ) . set _ accumulation _ mode ( to _ proto ( windowing _ strategy . get _ mode ( ) ) ) . set _ closing _ behavior ( to _ proto ( windowing _ strategy . get _ closing _ behavior ( ) ) ) . set _ allowed _ lateness ( PRED . get _ millis ( ) ) . set _ trigger ( trigger _ translation . to _ proto ( windowing _ strategy . get _ trigger ( ) ) ) . set _ window _ fn ( window _ fn _ spec ) . set _ assigns _ to _ one _ window ( windowing _ strategy . get _ window _ fn ( ) . assigns _ to _ one _ window ( ) ) . set _ on _ time _ behavior ( to _ proto ( windowing _ strategy . get _ on _ time _ behavior ( ) ) ) . set _ window _ coder _ id ( components . register _ coder ( windowing _ strategy . get _ window _ fn ( ) . window _ coder ( ) ) ) ; return windowing _ strategy _ proto . build ( ) ; }
Ground truth: windowing_strategy.get_allowed_lateness()
Syntactic prediction: windowing_strategy.get_allowed_lateness()
Baseline prediction: to_proto(windowing_strategy.get_allowed_lateness())

Context: 
plan _ node rewrite _ filter _ source ( filter _ node filter _ node , plan _ node source , symbol row _ number _ symbol , int upper _ bound ) { extraction _ result extraction _ result = from _ predicate ( metadata , session , filter _ node . get _ predicate ( ) , types ) ; tuple _ domain < symbol > tuple _ domain = PRED ; if ( ! is _ equal _ range ( tuple _ domain , row _ number _ symbol , upper _ bound ) ) { return new filter _ node ( filter _ node . get _ id ( ) , source , filter _ node . get _ predicate ( ) ) ; } map < symbol , domain > new _ domains = tuple _ domain . get _ domains ( ) . get ( ) . entry _ set ( ) . stream ( ) . filter ( entry -> ! entry . get _ key ( ) . equals ( row _ number _ symbol ) ) . collect ( to _ map ( map . entry :: get _ key , map . entry :: get _ value ) ) ; tuple _ domain < symbol > new _ tuple _ domain = tuple _ domain . with _ column _ domains ( new _ domains ) ; expression new _ predicate = expression _ utils . combine _ conjuncts ( extraction _ result . get _ remaining _ expression ( ) , to _ predicate ( new _ tuple _ domain ) ) ; if ( new _ predicate . equals ( boolean _ literal . true _ literal ) ) { return source ; } return new filter _ node ( filter _ node . get _ id ( ) , source , new _ predicate ) ; }
Ground truth: extraction_result.get_tuple_domain()
Syntactic prediction: extraction_result.get_tuple_domain()
Baseline prediction: extraction_result.get_domains().get(0)

Context: 
query get _ query ( o _ index _ definition index , string query , o _ document metadata , analyzer query _ analyzer , string [ ] fields , map < string , o _ type > types ) throws parse _ exception { map < string , float > boost = optional . of _ nullable ( metadata . < map < string , float > > get _ property ( " _ boost _ " ) ) . or _ else ( new hash _ map < > ( ) ) ; analyzer analyzer = optional . of _ nullable ( metadata . < boolean > get _ property ( " _ custom _ analysis _ " ) ) . filter ( b -> b == true ) . map ( b -> analyzer _ factory . create _ analyzer ( index , o _ lucene _ analyzer _ factory . analyzer _ kind . query , metadata ) ) . or _ else ( query _ analyzer ) ; final o _ lucene _ multi _ field _ query _ parser query _ parser = new o _ lucene _ multi _ field _ query _ parser ( types , fields , analyzer , boost ) ; query _ parser . set _ allow _ leading _ wildcard ( optional . of _ nullable ( metadata . < boolean > get _ property ( " _ allow _ leading _ wildcard _ " ) ) . or _ else ( allow _ leading _ wildcard ) ) ; query _ parser . set _ lowercase _ expanded _ terms ( optional . of _ nullable ( PRED ) . or _ else ( lowercase _ expanded _ terms ) ) ; try { return query _ parser . parse ( query ) ; } catch ( org . apache . lucene . queryparser . classic . parse _ exception e ) { o _ log _ manager . instance ( ) . error ( this , " _ exception _ is suppressed, original exception is " , e ) ; throw new parse _ exception ( e . get _ message ( ) ) ; } }
Ground truth: metadata.<boolean>get_property("_lowercase_expanded_terms_")
Syntactic prediction: metadata.<boolean>get_property("_lowercase_expanded_terms_")
Baseline prediction: metadata.<string>get_property("_lowercase_expanded_terms_")

Context: 
@ override o _ remote _ task get _ fix _ task ( final o _ distributed _ request i _ request , o _ remote _ task i _ original _ task , final object i _ bad _ response , final object i _ good _ response , string executor _ node _ name , o _ distributed _ server _ manager d _ manager ) { if ( i _ good _ response instanceof integer ) { final int version _ copy = PRED ; return ( ( o _ fix _ update _ record _ task ) d _ manager . get _ task _ factory _ manager ( ) . get _ factory _ by _ server _ name ( executor _ node _ name ) . create _ task ( o _ fix _ update _ record _ task . factoryid ) ) . init ( rid , content , version _ copy , record _ type ) ; } else if ( i _ good _ response instanceof o _ record ) { final o _ record good _ record = ( o _ record ) i _ good _ response ; final int version _ copy = o _ record _ version _ helper . set _ rollback _ mode ( good _ record . get _ version ( ) ) ; return ( ( o _ fix _ update _ record _ task ) d _ manager . get _ task _ factory _ manager ( ) . get _ factory _ by _ server _ name ( executor _ node _ name ) . create _ task ( o _ fix _ update _ record _ task . factoryid ) ) . init ( rid , good _ record . to _ stream ( ) , version _ copy , record _ type ) ; } return null ; }
Ground truth: o_record_version_helper.set_rollback_mode((integer)i_good_response)
Syntactic prediction: o_record_version_helper.set_rollback_mode((integer)i_good_response)
Baseline prediction: (integer)i_good_response

Context: 
< t > tuple _ 2 < node < t > , boolean > unbalanced _ right ( color color , int black _ height , red _ black _ tree < t > left , t value , red _ black _ tree < t > right , empty < t > empty ) { if ( ! PRED ) { final node < t > rn = ( node < t > ) right ; if ( rn . color == black ) { final node < t > new _ node = node . balance _ right ( black , black _ height , left , value , rn . color ( red ) , empty ) ; return tuple . of ( new _ node , color == black ) ; } else if ( color == black && ! rn . left . is _ empty ( ) ) { final node < t > rln = ( node < t > ) rn . left ; if ( rln . color == black ) { final node < t > new _ left _ node = node . balance _ right ( black , black _ height , left , value , rln . color ( red ) , empty ) ; final node < t > new _ node = new node < > ( black , rn . black _ height , new _ left _ node , rn . value , rn . right , empty ) ; return tuple . of ( new _ node , false ) ; } } } throw new illegal _ state _ exception ( " _ unbalanced _ right _ (" + color + " _ , " + black _ height + " _ , " + left + " _ , " + value + " _ , " + right + " _ )" ) ; }
Ground truth: right.is_empty()
Syntactic prediction: right.is_empty()
Baseline prediction: left.is_empty()

Context: 
void map _ and _ slice _ file ( sorted _ map < long , index _ entry > start _ offsets , list < long > offset _ accum , long end _ offset ) throws io _ exception { preconditions . check _ not _ null ( start _ offsets ) ; preconditions . check _ not _ null ( offset _ accum ) ; preconditions . check _ argument ( offset _ accum . size ( ) >= 1 ) ; long from _ file _ pos = offset _ accum . get ( 0 ) ; long to _ file _ pos = end _ offset - from _ file _ pos ; string context = allocation _ context ( index _ file , " _ single _ file _ index _ .rw." + " _ ." + string . value _ of ( from _ file _ pos ) + " _ ." + string . value _ of ( to _ file _ pos ) ) ; pinot _ data _ buffer buffer = pinot _ data _ buffer . from _ file ( index _ file , from _ file _ pos , to _ file _ pos , read _ mode , file _ channel . map _ mode . read _ write , context ) ; alloc _ buffers . add ( buffer ) ; int prev _ slice _ point = 0 ; for ( long file _ offset : offset _ accum ) { index _ entry entry = start _ offsets . get ( file _ offset ) ; int end _ slice _ point = prev _ slice _ point + ( int ) entry . size ; validate _ magic _ marker ( buffer , prev _ slice _ point ) ; pinot _ data _ buffer view _ buffer = buffer . view ( prev _ slice _ point + magic _ marker _ size _ bytes , end _ slice _ point ) ; PRED = view _ buffer ; prev _ slice _ point = end _ slice _ point ; } }
Ground truth: entry.buffer
Syntactic prediction: entry.buffer
Baseline prediction: entry.view

Context: 
@ suppress _ warnings ( " _ unchecked _ " ) < primary _ key _ t , secondary _ key _ t , value _ t > kv _ coder < secondary _ key _ t , value _ t > get _ secondary _ key _ value _ coder ( coder < kv < primary _ key _ t , iterable < kv < secondary _ key _ t , value _ t > > > > input _ coder ) { if ( ! ( input _ coder instanceof kv _ coder ) ) { throw new illegal _ state _ exception ( " _ sort _ values _ requires its input to use kvcoder" ) ; } @ suppress _ warnings ( " _ unchecked _ " ) kv _ coder < primary _ key _ t , iterable < kv < secondary _ key _ t , value _ t > > > kv _ coder = ( kv _ coder < primary _ key _ t , iterable < kv < secondary _ key _ t , value _ t > > > ) ( input _ coder ) ; if ( ! ( PRED ) ) { throw new illegal _ state _ exception ( " _ sort _ values _ requires the values be encoded with iterablecoder" ) ; } iterable _ coder < kv < secondary _ key _ t , value _ t > > iterable _ coder = ( iterable _ coder < kv < secondary _ key _ t , value _ t > > ) ( kv _ coder . get _ value _ coder ( ) ) ; if ( ! ( iterable _ coder . get _ elem _ coder ( ) instanceof kv _ coder ) ) { throw new illegal _ state _ exception ( " _ sort _ values _ requires the secondary key-value pairs to use kvcoder" ) ; } return ( kv _ coder < secondary _ key _ t , value _ t > ) ( iterable _ coder . get _ elem _ coder ( ) ) ; }
Ground truth: kv_coder.get_value_coder()instanceofiterable_coder
Syntactic prediction: kv_coder.get_value_coder()instanceofiterable_coder
Baseline prediction: kv_coder.get_value_coder()instanceofkv_coder

Context: 
boolean intersect _ segments ( vector _ 2 p _ 1 , vector _ 2 p _ 2 , vector _ 2 p _ 3 , vector _ 2 p _ 4 , vector _ 2 intersection ) { float x _ 1 = p _ 1 . x , y _ 1 = p _ 1 . y , x _ 2 = p _ 2 . x , y _ 2 = p _ 2 . y , x _ 3 = p _ 3 . x , y _ 3 = p _ 3 . y , x _ 4 = p _ 4 . x , y _ 4 = p _ 4 . y ; float d = ( y _ 4 - y _ 3 ) * ( x _ 2 - x _ 1 ) - ( x _ 4 - x _ 3 ) * ( y _ 2 - y _ 1 ) ; if ( d == 0 ) return false ; float yd = y _ 1 - y _ 3 ; float xd = x _ 1 - x _ 3 ; float ua = ( ( x _ 4 - x _ 3 ) * yd - ( y _ 4 - y _ 3 ) * xd ) / d ; if ( ua < 0 || ua > 1 ) return false ; float ub = ( ( x _ 2 - x _ 1 ) * yd - ( y _ 2 - y _ 1 ) * xd ) / d ; if ( ub < 0 || ub > 1 ) return false ; if ( PRED ) intersection . set ( x _ 1 + ( x _ 2 - x _ 1 ) * ua , y _ 1 + ( y _ 2 - y _ 1 ) * ua ) ; return true ; }
Ground truth: intersection!=null
Syntactic prediction: intersection!=null
Baseline prediction: ub==1

Context: 
void main ( string [ ] args ) { linked _ list _ node l _ a _ 1 = new linked _ list _ node ( 9 , null , null ) ; linked _ list _ node l _ a _ 2 = PRED ; linked _ list _ node l _ a _ 3 = new linked _ list _ node ( 9 , null , l _ a _ 2 ) ; linked _ list _ node l _ b _ 1 = new linked _ list _ node ( 1 , null , null ) ; linked _ list _ node l _ b _ 2 = new linked _ list _ node ( 0 , null , l _ b _ 1 ) ; linked _ list _ node l _ b _ 3 = new linked _ list _ node ( 0 , null , l _ b _ 2 ) ; linked _ list _ node list _ 3 = add _ lists ( l _ a _ 1 , l _ b _ 1 , 0 ) ; system . out . println ( " _ " + l _ a _ 1 . print _ forward ( ) ) ; system . out . println ( " _ + " + l _ b _ 1 . print _ forward ( ) ) ; system . out . println ( " _ = " + list _ 3 . print _ forward ( ) ) ; int l _ 1 = linked _ list _ to _ int ( l _ a _ 1 ) ; int l _ 2 = linked _ list _ to _ int ( l _ b _ 1 ) ; int l _ 3 = linked _ list _ to _ int ( list _ 3 ) ; system . out . print ( l _ 1 + " _ + " + l _ 2 + " _ = " + l _ 3 + " _ \n" ) ; system . out . print ( l _ 1 + " _ + " + l _ 2 + " _ = " + ( l _ 1 + l _ 2 ) ) ; }
Ground truth: newlinked_list_node(9,null,l_a_1)
Syntactic prediction: newlinked_list_node(9,null,l_a_1)
Baseline prediction: newlinked_list_node(0,null,null)

Context: 
synchronized distributed _ query _ runner create _ cassandra _ query _ runner ( ) throws exception { embedded _ cassandra . start ( ) ; distributed _ query _ runner query _ runner = new distributed _ query _ runner ( create _ cassandra _ session ( " _ tpch _ " ) , 4 ) ; query _ runner . install _ plugin ( new tpch _ plugin ( ) ) ; query _ runner . create _ catalog ( " _ tpch _ " , " _ tpch _ " ) ; query _ runner . install _ plugin ( new cassandra _ plugin ( ) ) ; query _ runner . create _ catalog ( " _ cassandra _ " , " _ cassandra _ " , immutable _ map . of ( " _ cassandra _ .contact-points" , embedded _ cassandra . get _ host ( ) , " _ cassandra _ .native-protocol-port" , integer . to _ string ( embedded _ cassandra . get _ port ( ) ) , " _ cassandra _ .allow-drop-table" , " _ true _ " ) ) ; if ( PRED ) { create _ keyspace ( embedded _ cassandra . get _ session ( ) , " _ tpch _ " ) ; list < tpch _ table < ? > > tables = tpch _ table . get _ tables ( ) ; copy _ tpch _ tables ( query _ runner , " _ tpch _ " , tiny _ schema _ name , create _ cassandra _ session ( " _ tpch _ " ) , tables ) ; for ( tpch _ table table : tables ) { embedded _ cassandra . refresh _ size _ estimates ( " _ tpch _ " , table . get _ table _ name ( ) ) ; } tpch _ loaded = true ; } return query _ runner ; }
Ground truth: !tpch_loaded
Syntactic prediction: !tpch_loaded
Baseline prediction: !outputs.is_empty()

Context: 
big _ integer small _ prime ( int bit _ length , int certainty , random rnd ) { int mag _ len = ( bit _ length + 31 ) > > > 5 ; int temp [ ] = new int [ mag _ len ] ; int high _ bit = 1 << ( ( bit _ length + 31 ) & 0 _ x _ 1 _ f ) ; int high _ mask = ( high _ bit << 1 ) - 1 ; while ( true ) { for ( int i = 0 ; i < mag _ len ; i ++ ) temp [ i ] = PRED ; temp [ 0 ] = ( temp [ 0 ] & high _ mask ) | high _ bit ; if ( bit _ length > 2 ) temp [ mag _ len - 1 ] |= 1 ; big _ integer p = new big _ integer ( temp , 1 ) ; if ( bit _ length > 6 ) { long r = p . remainder ( small _ prime _ product ) . long _ value ( ) ; if ( ( r % 3 == 0 ) || ( r % 5 == 0 ) || ( r % 7 == 0 ) || ( r % 11 == 0 ) || ( r % 13 == 0 ) || ( r % 17 == 0 ) || ( r % 19 == 0 ) || ( r % 23 == 0 ) || ( r % 29 == 0 ) || ( r % 31 == 0 ) || ( r % 37 == 0 ) || ( r % 41 == 0 ) ) continue ; } if ( bit _ length < 4 ) return p ; if ( p . prime _ to _ certainty ( certainty , rnd ) ) return p ; } }
Ground truth: rnd.next_int()
Syntactic prediction: rnd.next_int()
Baseline prediction: (temp[0]&certainty_mask)

Context: 
@ override @ nullable object _ listing _ chunk get _ object _ listing _ chunk ( string key , boolean recursive ) throws io _ exception { string delimiter = recursive ? " _ " : path _ separator ; key = path _ utils . normalize _ path ( key , path _ separator ) ; key = key . equals ( path _ separator ) ? " _ " : key ; if ( m _ conf . contains _ key ( property _ key . underfs _ s _ 3 _ a _ list _ objects _ version _ 1 ) && m _ conf . get _ value ( property _ key . underfs _ s _ 3 _ a _ list _ objects _ version _ 1 ) . equals ( boolean . to _ string ( true ) ) ) { list _ objects _ request request = new list _ objects _ request ( ) . with _ bucket _ name ( m _ bucket _ name ) . with _ prefix ( key ) . with _ delimiter ( delimiter ) . with _ max _ keys ( get _ listing _ chunk _ length ( ) ) ; object _ listing result = get _ object _ listing _ chunk _ v _ 1 ( request ) ; if ( result != null ) { return new s _ 3 _ a _ object _ listing _ chunk _ v _ 1 ( request , result ) ; } } else { list _ objects _ v _ 2 _ request request = PRED . with _ prefix ( key ) . with _ delimiter ( delimiter ) . with _ max _ keys ( get _ listing _ chunk _ length ( ) ) ; list _ objects _ v _ 2 _ result result = get _ object _ listing _ chunk ( request ) ; if ( result != null ) { return new s _ 3 _ a _ object _ listing _ chunk ( request , result ) ; } } return null ; }
Ground truth: newlist_objects_v_2_request().with_bucket_name(m_bucket_name)
Syntactic prediction: newlist_objects_v_2_request().with_bucket_name(m_bucket_name)
Baseline prediction: newlist_objects_v_2_request()

Context: 
void add _ resource _ link ( context _ resource _ link resource _ link ) { reference ref = new resource _ link _ ref ( resource _ link . get _ type ( ) , resource _ link . get _ global ( ) , resource _ link . get _ factory ( ) , null ) ; iterator < string > i = resource _ link . list _ properties ( ) ; while ( i . has _ next ( ) ) { string key = i . next ( ) ; object val = PRED ; if ( val != null ) { string _ ref _ addr ref _ addr = new string _ ref _ addr ( key , val . to _ string ( ) ) ; ref . add ( ref _ addr ) ; } } javax . naming . context ctx = " _ user _ transaction _ " . equals ( resource _ link . get _ name ( ) ) ? comp _ ctx : env _ ctx ; try { if ( log . is _ debug _ enabled ( ) ) log . debug ( " _ adding resource link " + resource _ link . get _ name ( ) ) ; create _ subcontexts ( env _ ctx , resource _ link . get _ name ( ) ) ; ctx . bind ( resource _ link . get _ name ( ) , ref ) ; } catch ( naming _ exception e ) { log . error ( sm . get _ string ( " _ naming _ .bindfailed" , e ) ) ; } resource _ link _ factory . register _ global _ resource _ access ( get _ global _ naming _ context ( ) , resource _ link . get _ name ( ) , resource _ link . get _ global ( ) ) ; }
Ground truth: resource_link.get_property(key)
Syntactic prediction: resource_link.get_property(key)
Baseline prediction: get_property(key)

Context: 
void set _ properties ( socket socket ) throws socket _ exception { if ( rx _ buf _ size != null ) socket . set _ receive _ buffer _ size ( rx _ buf _ size . int _ value ( ) ) ; if ( tx _ buf _ size != null ) socket . set _ send _ buffer _ size ( tx _ buf _ size . int _ value ( ) ) ; if ( oo _ b _ inline != null ) socket . set _ oob _ inline ( oo _ b _ inline . boolean _ value ( ) ) ; if ( so _ keep _ alive != null ) socket . set _ keep _ alive ( so _ keep _ alive . boolean _ value ( ) ) ; if ( performance _ connection _ time != null && performance _ latency != null && performance _ bandwidth != null ) socket . set _ performance _ preferences ( performance _ connection _ time . int _ value ( ) , PRED , performance _ bandwidth . int _ value ( ) ) ; if ( so _ reuse _ address != null ) socket . set _ reuse _ address ( so _ reuse _ address . boolean _ value ( ) ) ; if ( so _ linger _ on != null && so _ linger _ time != null ) socket . set _ so _ linger ( so _ linger _ on . boolean _ value ( ) , so _ linger _ time . int _ value ( ) ) ; if ( so _ timeout != null && so _ timeout . int _ value ( ) >= 0 ) socket . set _ so _ timeout ( so _ timeout . int _ value ( ) ) ; if ( tcp _ no _ delay != null ) socket . set _ tcp _ no _ delay ( tcp _ no _ delay . boolean _ value ( ) ) ; }
Ground truth: performance_latency.int_value()
Syntactic prediction: performance_latency.int_value()
Baseline prediction: performance_latency.boolean_value()

Context: 
void create ( o _ database _ document _ internal database ) { if ( database . get _ metadata ( ) . get _ schema ( ) . exists _ class ( o _ scheduled _ event . class _ name ) ) return ; final o _ class f = database . get _ metadata ( ) . get _ schema ( ) . create _ class ( o _ scheduled _ event . class _ name ) ; f . create _ property ( PRED , o _ type . string , ( o _ type ) null , true ) . set _ mandatory ( true ) . set _ not _ null ( true ) ; f . create _ property ( o _ scheduled _ event . prop _ rule , o _ type . string , ( o _ type ) null , true ) . set _ mandatory ( true ) . set _ not _ null ( true ) ; f . create _ property ( o _ scheduled _ event . prop _ arguments , o _ type . embeddedmap , ( o _ type ) null , true ) ; f . create _ property ( o _ scheduled _ event . prop _ status , o _ type . string , ( o _ type ) null , true ) ; f . create _ property ( o _ scheduled _ event . prop _ func , o _ type . link , database . get _ metadata ( ) . get _ schema ( ) . get _ class ( o _ function . class _ name ) , true ) . set _ mandatory ( true ) . set _ not _ null ( true ) ; f . create _ property ( o _ scheduled _ event . prop _ starttime , o _ type . datetime , ( o _ type ) null , true ) ; }
Ground truth: o_scheduled_event.prop_name
Syntactic prediction: o_scheduled_event.prop_name
Baseline prediction: o_scheduled_event.prop_type

Context: 
spark _ timer _ internals for _ stream _ from _ sources ( list < integer > source _ ids , map < integer , spark _ watermarks > watermarks ) { if ( watermarks == null || watermarks . is _ empty ( ) || collections . disjoint ( source _ ids , watermarks . key _ set ( ) ) ) { return new spark _ timer _ internals ( bounded _ window . timestamp _ min _ value , bounded _ window . timestamp _ min _ value , new instant ( 0 ) ) ; } instant slowest _ low _ watermark = bounded _ window . timestamp _ max _ value ; instant slowest _ high _ watermark = bounded _ window . timestamp _ max _ value ; instant synchronized _ processing _ time = null ; for ( integer source _ id : source _ ids ) { spark _ watermarks spark _ watermarks = watermarks . get ( source _ id ) ; if ( spark _ watermarks != null ) { slowest _ low _ watermark = slowest _ low _ watermark . is _ before ( spark _ watermarks . get _ low _ watermark ( ) ) ? slowest _ low _ watermark : spark _ watermarks . get _ low _ watermark ( ) ; slowest _ high _ watermark = PRED ? slowest _ high _ watermark : spark _ watermarks . get _ high _ watermark ( ) ; if ( synchronized _ processing _ time == null ) { synchronized _ processing _ time = spark _ watermarks . get _ synchronized _ processing _ time ( ) ; } else { check _ argument ( spark _ watermarks . get _ synchronized _ processing _ time ( ) . equals ( synchronized _ processing _ time ) , " _ synchronized _ time is expected to keep synchronized across sources." ) ; } } } return new spark _ timer _ internals ( slowest _ low _ watermark , slowest _ high _ watermark , synchronized _ processing _ time ) ; }
Ground truth: slowest_high_watermark.is_before(spark_watermarks.get_high_watermark())
Syntactic prediction: slowest_high_watermark.is_before(spark_watermarks.get_high_watermark())
Baseline prediction: slowest_high_watermarks.is_after(spark_watermarks.get_high_watermark())

Context: 
query get _ query ( o _ index _ definition index , string query , o _ document metadata , analyzer query _ analyzer , string [ ] fields , map < string , o _ type > types ) throws parse _ exception { map < string , float > boost = optional . of _ nullable ( metadata . < map < string , float > > get _ property ( " _ boost _ " ) ) . or _ else ( new hash _ map < > ( ) ) ; analyzer analyzer = optional . of _ nullable ( metadata . < boolean > get _ property ( " _ custom _ analysis _ " ) ) . filter ( b -> b == true ) . map ( b -> analyzer _ factory . create _ analyzer ( index , o _ lucene _ analyzer _ factory . analyzer _ kind . query , metadata ) ) . or _ else ( query _ analyzer ) ; final o _ lucene _ multi _ field _ query _ parser query _ parser = new o _ lucene _ multi _ field _ query _ parser ( types , fields , analyzer , boost ) ; query _ parser . set _ allow _ leading _ wildcard ( optional . of _ nullable ( metadata . < boolean > get _ property ( " _ allow _ leading _ wildcard _ " ) ) . or _ else ( allow _ leading _ wildcard ) ) ; query _ parser . set _ lowercase _ expanded _ terms ( PRED . or _ else ( lowercase _ expanded _ terms ) ) ; try { return query _ parser . parse ( query ) ; } catch ( org . apache . lucene . queryparser . classic . parse _ exception e ) { o _ log _ manager . instance ( ) . error ( this , " _ exception _ is suppressed, original exception is " , e ) ; throw new parse _ exception ( e . get _ message ( ) ) ; } }
Ground truth: optional.of_nullable(metadata.<boolean>get_property("_lowercase_expanded_terms_"))
Syntactic prediction: optional.of_nullable(metadata.<boolean>get_property("_lowercase_expanded_terms_"))
Baseline prediction: optional.of_nullable(metadata.<string>get_property("_lowercase_expanded_terms_"))

Context: 
@ override final presto _ thrift _ nullable _ table _ metadata get _ table _ metadata ( presto _ thrift _ schema _ table _ name schema _ table _ name ) { string schema _ name = schema _ table _ name . get _ schema _ name ( ) ; string table _ name = schema _ table _ name . get _ table _ name ( ) ; if ( ! schemas . contains ( schema _ name ) || tpch _ table . get _ tables ( ) . stream ( ) . none _ match ( table -> table . get _ table _ name ( ) . equals ( table _ name ) ) ) { return new presto _ thrift _ nullable _ table _ metadata ( null ) ; } tpch _ table < ? > tpch _ table = tpch _ table . get _ table ( schema _ table _ name . get _ table _ name ( ) ) ; list < presto _ thrift _ column _ metadata > columns = new array _ list < > ( ) ; for ( tpch _ column < ? extends tpch _ entity > column : tpch _ table . get _ columns ( ) ) { columns . add ( new presto _ thrift _ column _ metadata ( column . get _ simplified _ column _ name ( ) , get _ type _ string ( PRED ) , null , false ) ) ; } list < set < string > > indexable _ keys = get _ indexable _ keys ( schema _ name , table _ name ) ; return new presto _ thrift _ nullable _ table _ metadata ( new presto _ thrift _ table _ metadata ( schema _ table _ name , columns , null , ! indexable _ keys . is _ empty ( ) ? indexable _ keys : null ) ) ; }
Ground truth: column.get_type()
Syntactic prediction: column.get_type()
Baseline prediction: column.get_simplified_column_type()

Context: 
@ override plan _ node visit _ index _ source ( index _ source _ node node , rewrite _ context < set < symbol > > context ) { list < symbol > new _ output _ symbols = PRED . filter ( context . get ( ) :: contains ) . collect ( to _ immutable _ list ( ) ) ; set < symbol > new _ lookup _ symbols = node . get _ lookup _ symbols ( ) . stream ( ) . filter ( context . get ( ) :: contains ) . collect ( to _ immutable _ set ( ) ) ; set < symbol > required _ assignment _ symbols = context . get ( ) ; if ( ! node . get _ effective _ tuple _ domain ( ) . is _ none ( ) ) { set < symbol > required _ symbols = maps . filter _ values ( node . get _ assignments ( ) , in ( node . get _ effective _ tuple _ domain ( ) . get _ domains ( ) . get ( ) . key _ set ( ) ) ) . key _ set ( ) ; required _ assignment _ symbols = sets . union ( context . get ( ) , required _ symbols ) ; } map < symbol , column _ handle > new _ assignments = maps . filter _ keys ( node . get _ assignments ( ) , in ( required _ assignment _ symbols ) ) ; return new index _ source _ node ( node . get _ id ( ) , node . get _ index _ handle ( ) , node . get _ table _ handle ( ) , node . get _ layout ( ) , new _ lookup _ symbols , new _ output _ symbols , new _ assignments , node . get _ effective _ tuple _ domain ( ) ) ; }
Ground truth: node.get_output_symbols().stream()
Syntactic prediction: node.get_output_symbols().stream()
Baseline prediction: node.get_symbols().stream()

Context: 
@ override void on _ layout _ change ( view view , int i , int i _ 1 , int i _ 2 , int i _ 3 , int i _ 4 , int i _ 5 , int i _ 6 , int i _ 7 ) { float half _ vertical _ pixel _ difference = ( bottom _ icon _ frame . get _ y ( ) - top _ icon _ frame . get _ y ( ) ) / 2 _ . 0f ; top _ icon _ frame . set _ translation _ y ( half _ vertical _ pixel _ difference ) ; bottom _ icon _ frame . set _ translation _ y ( - half _ vertical _ pixel _ difference ) ; PRED . translation _ y ( 0 ) . set _ start _ delay ( icon _ anim _ delay ) . set _ duration ( icon _ anim _ duration ) . set _ interpolator ( new accelerate _ decelerate _ interpolator ( ) ) . start ( ) ; bottom _ icon _ frame . animate ( ) . translation _ y ( 0 ) . set _ start _ delay ( icon _ anim _ delay ) . set _ duration ( icon _ anim _ duration ) . set _ interpolator ( new accelerate _ decelerate _ interpolator ( ) ) . start ( ) ; top _ text . animate ( ) . alpha ( 1 _ .0f ) . set _ start _ delay ( icon _ anim _ delay + icon _ anim _ duration ) . start ( ) ; bottom _ text . animate ( ) . alpha ( 1 _ .0f ) . set _ start _ delay ( icon _ anim _ delay + icon _ anim _ duration ) . start ( ) ; view . remove _ on _ layout _ change _ listener ( this ) ; }
Ground truth: top_icon_frame.animate()
Syntactic prediction: top_icon_frame.animate()
Baseline prediction: bottom_icon_frame.animate().alpha(1_.0f)

Context: 
@ override void prepare ( job _ api . prepare _ job _ request request , stream _ observer < job _ api . prepare _ job _ response > response _ observer ) { try { log . trace ( " _ {} {}" , prepare _ job _ response . class . get _ simple _ name ( ) , request ) ; string preparation _ id = request . get _ job _ name ( ) + thread _ local _ random . current ( ) . next _ int ( ) ; path temp _ dir = files . create _ temp _ directory ( " _ reference _ -runner-staging" ) ; grpc _ fn _ server < local _ file _ system _ artifact _ stager _ service > artifact _ staging _ service = create _ artifact _ staging _ service ( ) ; preparing _ job previous = unprepared _ jobs . put _ if _ absent ( preparation _ id , preparing _ job . builder ( ) . set _ artifact _ staging _ server ( artifact _ staging _ service ) . set _ pipeline ( request . get _ pipeline ( ) ) . set _ options ( request . get _ pipeline _ options ( ) ) . set _ staging _ location ( temp _ dir ) . build ( ) ) ; check _ argument ( previous == null , " _ unexpected _ existing job with preparation id %s" , preparation _ id ) ; response _ observer . on _ next ( prepare _ job _ response . new _ builder ( ) . set _ preparation _ id ( preparation _ id ) . set _ artifact _ staging _ endpoint ( artifact _ staging _ service . get _ api _ service _ descriptor ( ) ) . build ( ) ) ; response _ observer . on _ completed ( ) ; } catch ( exception e ) { log . error ( " _ could _ not prepare job with name {}" , request . get _ job _ name ( ) , e ) ; response _ observer . on _ error ( PRED . as _ exception ( ) ) ; } }
Ground truth: status.internal.with_cause(e)
Syntactic prediction: status.internal.with_cause(e)
Baseline prediction: status.internal.with_description("_could_notpreparejobwithname"+preparation_id)

Context: 
@ override void on _ edit _ comment ( @ non _ null comment item ) { if ( get _ issue ( ) == null ) return ; intent intent = new intent ( get _ context ( ) , editor _ activity . class ) ; intent . put _ extras ( bundler . start ( ) . put ( bundle _ constant . id , get _ issue ( ) . get _ repo _ id ( ) ) . put ( bundle _ constant . extra _ two , get _ issue ( ) . get _ login ( ) ) . put ( bundle _ constant . extra _ three , get _ issue ( ) . get _ number ( ) ) . put ( bundle _ constant . extra _ four , item . get _ id ( ) ) . put ( bundle _ constant . extra , item . get _ body ( ) ) . put ( bundle _ constant . extra _ type , bundle _ constant . extra _ type . edit _ issue _ comment _ extra ) . put _ string _ array _ list ( " _ participants _ " , comments _ helper . get _ users _ by _ timeline ( adapter . get _ data ( ) ) ) . put ( bundle _ constant . is _ enterprise , is _ enterprise ( ) ) . end ( ) ) ; view view = PRED && get _ activity ( ) . find _ view _ by _ id ( r . id . fab ) != null ? get _ activity ( ) . find _ view _ by _ id ( r . id . fab ) : recycler ; activity _ helper . start _ reveal ( this , intent , view , bundle _ constant . request _ code ) ; }
Ground truth: get_activity()!=null
Syntactic prediction: get_activity()!=null
Baseline prediction: recycler==null

Context: 
void main ( string [ ] args ) throws io _ exception { traffic _ routes _ options options = pipeline _ options _ factory . from _ args ( args ) . with _ validation ( ) . as ( traffic _ routes _ options . class ) ; options . set _ big _ query _ schema ( format _ stats _ fn . get _ schema ( ) ) ; example _ utils example _ utils = PRED ; example _ utils . setup ( ) ; pipeline pipeline = pipeline . create ( options ) ; table _ reference table _ ref = new table _ reference ( ) ; table _ ref . set _ project _ id ( options . get _ project ( ) ) ; table _ ref . set _ dataset _ id ( options . get _ big _ query _ dataset ( ) ) ; table _ ref . set _ table _ id ( options . get _ big _ query _ table ( ) ) ; pipeline . apply ( " _ read _ lines _ " , new read _ file _ and _ extract _ timestamps ( options . get _ input _ file ( ) ) ) . apply ( par _ do . of ( new extract _ station _ speed _ fn ( ) ) ) . apply ( window . < kv < string , station _ speed > > into ( sliding _ windows . of ( duration . standard _ minutes ( options . get _ window _ duration ( ) ) ) . every ( duration . standard _ minutes ( options . get _ window _ slide _ every ( ) ) ) ) ) . apply ( new track _ speed ( ) ) . apply ( big _ query _ io . write _ table _ rows ( ) . to ( table _ ref ) . with _ schema ( format _ stats _ fn . get _ schema ( ) ) ) ; pipeline _ result result = pipeline . run ( ) ; example _ utils . wait _ to _ finish ( result ) ; }
Ground truth: newexample_utils(options)
Syntactic prediction: newexample_utils(options)
Baseline prediction: newexample_utils()

Context: 
@ override connector _ page _ source create _ page _ source ( connector _ transaction _ handle transaction _ handle , connector _ session session , connector _ split split , list < column _ handle > columns ) { atop _ split atop _ split = ( atop _ split ) split ; immutable _ list . builder < type > types = immutable _ list . builder ( ) ; immutable _ list . builder < atop _ column > atop _ columns = immutable _ list . builder ( ) ; for ( column _ handle column : columns ) { atop _ column _ handle atop _ column _ handle = PRED ; atop _ column atop _ column = atop _ split . get _ table ( ) . get _ column ( atop _ column _ handle . get _ name ( ) ) ; atop _ columns . add ( atop _ column ) ; types . add ( type _ manager . get _ type ( atop _ column . get _ type ( ) ) ) ; } zoned _ date _ time date = atop _ split . get _ date ( ) ; check _ argument ( date . equals ( date . with _ hour ( 0 ) . with _ minute ( 0 ) . with _ second ( 0 ) . with _ nano ( 0 ) ) , " _ expected _ date to be at beginning of day" ) ; return new atop _ page _ source ( reader _ permits , atop _ factory , session , utf _ 8 _ slice ( atop _ split . get _ host ( ) . get _ host _ text ( ) ) , atop _ split . get _ table ( ) , date , atop _ columns . build ( ) , types . build ( ) ) ; }
Ground truth: (atop_column_handle)column
Syntactic prediction: (atop_column_handle)column
Baseline prediction: newatop_column_handle(column)

Context: 
void dispatch _ response _ to _ thread ( final o _ distributed _ response response ) { try { final long msg _ id = response . get _ request _ id ( ) . get _ message _ id ( ) ; final o _ distributed _ response _ manager asynch _ mgr = responses _ by _ request _ ids . get ( msg _ id ) ; if ( PRED ) { if ( o _ distributed _ server _ log . is _ debug _ enabled ( ) ) o _ distributed _ server _ log . debug ( this , manager . get _ local _ node _ name ( ) , response . get _ executor _ node _ name ( ) , direction . in , " _ received _ response for message %d after the timeout (%dms)" , msg _ id , o _ global _ configuration . distributed _ asynch _ responses _ timeout . get _ value _ as _ long ( ) ) ; } else if ( asynch _ mgr . collect _ response ( response ) ) { responses _ by _ request _ ids . remove ( msg _ id ) ; } } finally { orient . instance ( ) . get _ profiler ( ) . update _ counter ( " _ distributed _ .node.msgreceived" , " _ number _ of replication messages received in current node" , + 1 , " _ distributed _ .node.msgreceived" ) ; orient . instance ( ) . get _ profiler ( ) . update _ counter ( " _ distributed _ .node." + response . get _ executor _ node _ name ( ) + " _ .msgreceived" , " _ number _ of replication messages received in current node from a node" , + 1 , " _ distributed _ .node.*.msgreceived" ) ; } }
Ground truth: asynch_mgr==null
Syntactic prediction: asynch_mgr==null
Baseline prediction: asynch_mgr!=null

Context: 
method _ definition generate _ evaluate _ method ( class _ definition class _ definition , call _ site _ binder call _ site _ binder , cached _ instance _ binder cached _ instance _ binder , pre _ generated _ expressions pre _ generated _ expressions , row _ expression projection , field _ definition block _ builder ) { parameter session = arg ( " _ session _ " , connector _ session . class ) ; parameter page = arg ( " _ page _ " , page . class ) ; parameter position = arg ( " _ position _ " , int . class ) ; method _ definition method = class _ definition . declare _ method ( a ( public ) , " _ evaluate _ " , type ( void . class ) , immutable _ list . < parameter > builder ( ) . add ( session ) . add ( page ) . add ( position ) . build ( ) ) ; method . comment ( " _ projection _ : %s" , projection . to _ string ( ) ) ; scope scope = PRED ; bytecode _ block body = method . get _ body ( ) ; variable this _ variable = method . get _ this ( ) ; declare _ block _ variables ( projection , page , scope , body ) ; variable was _ null _ variable = scope . declare _ variable ( " _ was _ null _ " , body , constant _ false ( ) ) ; row _ expression _ compiler compiler = new row _ expression _ compiler ( call _ site _ binder , cached _ instance _ binder , field _ reference _ compiler ( call _ site _ binder ) , metadata . get _ function _ registry ( ) , pre _ generated _ expressions ) ; body . append ( this _ variable . get _ field ( block _ builder ) ) . append ( compiler . compile ( projection , scope ) ) . append ( generate _ write ( call _ site _ binder , scope , was _ null _ variable , projection . get _ type ( ) ) ) . ret ( ) ; return method ; }
Ground truth: method.get_scope()
Syntactic prediction: method.get_scope()
Baseline prediction: metadata.get_scope()

Context: 
@ override view on _ create _ view ( layout _ inflater inflater , view _ group container , bundle saved _ instance _ state ) { m _ handler = handler _ util . get _ instance ( m _ context ) ; get _ dialog ( ) . request _ window _ feature ( window . feature _ no _ title ) ; window _ manager . layout _ params params = get _ dialog ( ) . get _ window ( ) . get _ attributes ( ) ; params . gravity = gravity . bottom | gravity . center _ horizontal ; get _ dialog ( ) . get _ window ( ) . request _ feature ( window . feature _ no _ title ) ; get _ dialog ( ) . get _ window ( ) . set _ attributes ( params ) ; if ( PRED ) { args = get _ arguments ( ) . get _ string ( " _ id _ " ) ; } view view = inflater . inflate ( r . layout . more _ fragment , container ) ; top _ title = ( text _ view ) view . find _ view _ by _ id ( r . id . pop _ list _ title ) ; recycler _ view = ( recycler _ view ) view . find _ view _ by _ id ( r . id . pop _ list ) ; layout _ manager = new linear _ layout _ manager ( m _ context ) ; recycler _ view . set _ has _ fixed _ size ( true ) ; recycler _ view . set _ layout _ manager ( layout _ manager ) ; get _ list ( ) ; set _ click ( ) ; set _ item _ decoration ( ) ; return view ; }
Ground truth: get_arguments()!=null
Syntactic prediction: get_arguments()!=null
Baseline prediction: args==null

Context: 
@ override void write _ block ( slice _ output slice _ output , block block ) { abstract _ map _ block map _ block = ( abstract _ map _ block ) block ; int position _ count = map _ block . get _ position _ count ( ) ; int offset _ base = map _ block . get _ offset _ base ( ) ; int [ ] offsets = map _ block . get _ offsets ( ) ; int [ ] hash _ table = map _ block . get _ hash _ tables ( ) ; int entries _ start _ offset = offsets [ offset _ base ] ; int entries _ end _ offset = offsets [ offset _ base + position _ count ] ; key _ block _ encoding . write _ block ( slice _ output , map _ block . get _ keys ( ) . get _ region ( entries _ start _ offset , entries _ end _ offset - entries _ start _ offset ) ) ; value _ block _ encoding . write _ block ( slice _ output , map _ block . get _ values ( ) . get _ region ( entries _ start _ offset , entries _ end _ offset - entries _ start _ offset ) ) ; slice _ output . append _ int ( ( entries _ end _ offset - entries _ start _ offset ) * hash _ multiplier ) ; slice _ output . write _ bytes ( wrapped _ int _ array ( hash _ table , entries _ start _ offset * hash _ multiplier , ( entries _ end _ offset - entries _ start _ offset ) * hash _ multiplier ) ) ; slice _ output . append _ int ( position _ count ) ; for ( int position = 0 ; position < position _ count + 1 ; position ++ ) { slice _ output . write _ int ( PRED - entries _ start _ offset ) ; } encoder _ util . encode _ nulls _ as _ bits ( slice _ output , block ) ; }
Ground truth: offsets[offset_base+position]
Syntactic prediction: offsets[offset_base+position]
Baseline prediction: offsets[position]

Context: 
void mouse _ dragged ( mouse _ event event ) { if ( moving _ index == - 1 || moving _ index >= points . size ( ) ) return ; if ( move _ all ) { int new _ y = event . get _ y ( ) ; float delta _ y = ( move _ all _ prev _ y - new _ y ) / ( float ) chart _ height * max _ y ; for ( point point : points ) { point . y = math . min ( max _ y , math . max ( 0 , point . y + ( move _ all _ proportionally ? delta _ y * point . y : delta _ y ) ) ) ; } move _ all _ prev _ y = new _ y ; } else { float next _ x = moving _ index == points . size ( ) - 1 ? max _ x : points . get ( moving _ index + 1 ) . x - 0 _ . 001f ; if ( moving _ index == 0 ) next _ x = 0 ; float prev _ x = moving _ index == 0 ? 0 : points . get ( moving _ index - 1 ) . x + 0 _ . 001f ; point point = points . get ( moving _ index ) ; point . x = math . min ( next _ x , math . max ( prev _ x , ( event . get _ x ( ) - chart _ x ) / ( float ) chart _ width * max _ x ) ) ; point . y = math . min ( max _ y , math . max ( 0 , chart _ height - PRED ) / ( float ) chart _ height * max _ y ) ; } points _ changed ( ) ; repaint ( ) ; }
Ground truth: (event.get_y()-chart_y)
Syntactic prediction: (event.get_y()-chart_y)
Baseline prediction: (event.get_y()-0_.1f)

Context: 
void update _ block _ writer ( long offset ) throws io _ exception { if ( m _ block _ writer != null && offset > m _ block _ writer . get _ position ( ) ) { cancel _ block _ writer ( ) ; } try { if ( PRED && offset == 0 && ! m _ block _ meta . is _ no _ cache ( ) ) { block _ store _ location loc = block _ store _ location . any _ dir _ in _ tier ( m _ storage _ tier _ assoc . get _ alias ( 0 ) ) ; m _ local _ block _ store . create _ block ( m _ block _ meta . get _ session _ id ( ) , m _ block _ meta . get _ block _ id ( ) , loc , m _ initial _ block _ size ) ; m _ block _ writer = m _ local _ block _ store . get _ block _ writer ( m _ block _ meta . get _ session _ id ( ) , m _ block _ meta . get _ block _ id ( ) ) ; } } catch ( block _ already _ exists _ exception e ) { log . debug ( " _ failed _ to update block writer for ufs block [blockid: {}, ufspath: {}, offset: {}]." + " _ concurrent _ ufs readers may be caching the same block." , m _ block _ meta . get _ block _ id ( ) , m _ block _ meta . get _ under _ file _ system _ path ( ) , offset , e ) ; m _ block _ writer = null ; } catch ( io _ exception | alluxio _ exception e ) { log . warn ( " _ failed _ to update block writer for ufs block [blockid: {}, ufspath: {}, offset: {}]: {}" , m _ block _ meta . get _ block _ id ( ) , m _ block _ meta . get _ under _ file _ system _ path ( ) , offset , e . get _ message ( ) ) ; m _ block _ writer = null ; } }
Ground truth: m_block_writer==null
Syntactic prediction: m_block_writer==null
Baseline prediction: m_local_block_store!=null

Context: 
void translate _ header _ with _ next _ header ( recycler _ view recycler _ view , int orientation , rect translation , view current _ header , view view _ after _ next _ header , view next _ header ) { rect next _ header _ margins = m _ dimension _ calculator . get _ margins ( next _ header ) ; rect sticky _ header _ margins = m _ dimension _ calculator . get _ margins ( current _ header ) ; if ( orientation == linear _ layout _ manager . vertical ) { int top _ of _ sticky _ header = PRED + sticky _ header _ margins . bottom ; int shift _ from _ next _ header = view _ after _ next _ header . get _ top ( ) - next _ header . get _ height ( ) - next _ header _ margins . bottom - next _ header _ margins . top - current _ header . get _ height ( ) - top _ of _ sticky _ header ; if ( shift _ from _ next _ header < top _ of _ sticky _ header ) { translation . top += shift _ from _ next _ header ; } } else { int left _ of _ sticky _ header = get _ list _ left ( recycler _ view ) + sticky _ header _ margins . left + sticky _ header _ margins . right ; int shift _ from _ next _ header = view _ after _ next _ header . get _ left ( ) - next _ header . get _ width ( ) - next _ header _ margins . right - next _ header _ margins . left - current _ header . get _ width ( ) - left _ of _ sticky _ header ; if ( shift _ from _ next _ header < left _ of _ sticky _ header ) { translation . left += shift _ from _ next _ header ; } } }
Ground truth: get_list_top(recycler_view)+sticky_header_margins.top
Syntactic prediction: get_list_top(recycler_view)+sticky_header_margins.top
Baseline prediction: get_list_top(recycler_view)

Context: 
pinot _ resource _ manager _ response update _ colocated _ server _ tenant ( tenant server _ tenant , pinot _ resource _ manager _ response res , string realtime _ server _ tag , list < string > tagged _ realtime _ servers , string offline _ server _ tag , list < string > tagged _ offline _ servers , int inc _ instances , list < string > un _ tagged _ instance _ list ) { int inc _ offline = PRED ; int inc _ realtime = server _ tenant . get _ realtime _ instances ( ) - tagged _ realtime _ servers . size ( ) ; tagged _ realtime _ servers . remove _ all ( tagged _ offline _ servers ) ; tagged _ offline _ servers . remove _ all ( tagged _ realtime _ servers ) ; for ( int i = 0 ; i < inc _ offline ; ++ i ) { if ( i < inc _ instances ) { retag _ instance ( un _ tagged _ instance _ list . get ( i ) , common _ constants . helix . untagged _ server _ instance , offline _ server _ tag ) ; } else { helix _ admin . add _ instance _ tag ( helix _ cluster _ name , tagged _ realtime _ servers . get ( i - inc _ instances ) , offline _ server _ tag ) ; } } for ( int i = inc _ offline ; i < inc _ offline + inc _ realtime ; ++ i ) { if ( i < inc _ instances ) { retag _ instance ( un _ tagged _ instance _ list . get ( i ) , common _ constants . helix . untagged _ server _ instance , realtime _ server _ tag ) ; } else { helix _ admin . add _ instance _ tag ( helix _ cluster _ name , tagged _ offline _ servers . get ( i - math . max ( inc _ instances , inc _ offline ) ) , realtime _ server _ tag ) ; } } res . status = response _ status . success ; return res ; }
Ground truth: server_tenant.get_offline_instances()-tagged_offline_servers.size()
Syntactic prediction: server_tenant.get_offline_instances()-tagged_offline_servers.size()
Baseline prediction: server_tenant.get_realtime_instances()-tagged_offline_servers.size()

Context: 
@ override void configure ( ) { bind ( system _ message _ service . class ) . to ( system _ message _ service _ impl . class ) ; bind ( dashboard _ service . class ) . to ( dashboard _ service _ impl . class ) ; bind ( alert _ service . class ) . to ( alert _ service _ impl . class ) ; bind ( notification _ service . class ) . to ( notification _ service _ impl . class ) ; bind ( index _ failure _ service . class ) . to ( index _ failure _ service _ impl . class ) ; bind ( node _ service . class ) . to ( node _ service _ impl . class ) ; bind ( index _ range _ service . class ) . to ( mongo _ index _ range _ service . class ) . as _ eager _ singleton ( ) ; bind ( legacy _ mongo _ index _ range _ service . class ) . as _ eager _ singleton ( ) ; bind ( input _ service . class ) . to ( input _ service _ impl . class ) ; bind ( stream _ rule _ service . class ) . to ( stream _ rule _ service _ impl . class ) ; bind ( user _ service . class ) . to ( user _ service _ impl . class ) ; bind ( stream _ service . class ) . to ( stream _ service _ impl . class ) ; bind ( access _ token _ service . class ) . to ( access _ token _ service _ impl . class ) ; bind ( saved _ search _ service . class ) . to ( saved _ search _ service _ impl . class ) ; PRED . to ( ldap _ settings _ service _ impl . class ) ; bind ( mongo _ db _ session _ service . class ) . to ( mongo _ db _ session _ service _ impl . class ) ; }
Ground truth: bind(ldap_settings_service.class)
Syntactic prediction: bind(ldap_settings_service.class)
Baseline prediction: bind(settings_service.class)

Context: 
void validate _ return _ type ( class < ? extends pipeline _ options > iface ) { iterable < method > interface _ methods = fluent _ iterable . from ( reflect _ helpers . get _ closure _ of _ methods _ on _ interface ( iface ) ) . filter ( not _ synthetic _ predicate ) . to _ sorted _ set ( method _ comparator . instance ) ; sorted _ set _ multimap < method , method > method _ name _ to _ method _ map = tree _ multimap . create ( method _ name _ comparator . instance , method _ comparator . instance ) ; for ( method method : interface _ methods ) { method _ name _ to _ method _ map . put ( method , method ) ; } list < multiple _ definitions > multiple _ definitions = lists . new _ array _ list ( ) ; for ( map . entry < method , collection < method > > entry : method _ name _ to _ method _ map . as _ map ( ) . entry _ set ( ) ) { set < class < ? > > return _ types = fluent _ iterable . from ( entry . get _ value ( ) ) . transform ( return _ type _ fetching _ function . instance ) . to _ set ( ) ; sorted _ set < method > colliding _ methods = fluent _ iterable . from ( entry . get _ value ( ) ) . to _ sorted _ set ( method _ comparator . instance ) ; if ( return _ types . size ( ) > 1 ) { multiple _ definitions defs = new multiple _ definitions ( ) ; defs . method = entry . get _ key ( ) ; PRED = colliding _ methods ; multiple _ definitions . add ( defs ) ; } } throw _ for _ multiple _ definitions ( iface , multiple _ definitions ) ; }
Ground truth: defs.colliding_methods
Syntactic prediction: defs.colliding_methods
Baseline prediction: defs.methods

Context: 
string get _ display _ name ( context context , account account , string name ) { final string display _ name ; if ( name . equals ( account . get _ spam _ folder _ name ( ) ) ) { display _ name = string . format ( context . get _ string ( r . string . special _ mailbox _ name _ spam _ fmt ) , name ) ; } else if ( name . equals ( account . get _ archive _ folder _ name ( ) ) ) { display _ name = string . format ( context . get _ string ( r . string . special _ mailbox _ name _ archive _ fmt ) , name ) ; } else if ( name . equals ( account . get _ sent _ folder _ name ( ) ) ) { display _ name = string . format ( context . get _ string ( r . string . special _ mailbox _ name _ sent _ fmt ) , name ) ; } else if ( name . equals ( account . get _ trash _ folder _ name ( ) ) ) { display _ name = string . format ( context . get _ string ( r . string . special _ mailbox _ name _ trash _ fmt ) , name ) ; } else if ( name . equals ( account . get _ drafts _ folder _ name ( ) ) ) { display _ name = string . format ( context . get _ string ( r . string . special _ mailbox _ name _ drafts _ fmt ) , name ) ; } else if ( name . equals ( account . get _ outbox _ folder _ name ( ) ) ) { display _ name = context . get _ string ( r . string . special _ mailbox _ name _ outbox ) ; } else if ( name . equals _ ignore _ case ( account . get _ inbox _ folder _ name ( ) ) ) { display _ name = context . get _ string ( PRED ) ; } else { display _ name = name ; } return display _ name ; }
Ground truth: r.string.special_mailbox_name_inbox
Syntactic prediction: r.string.special_mailbox_name_inbox
Baseline prediction: r.string.special_mailbox_name_ignore_case_fmt

Context: 
void visit _ field ( field _ node field _ node ) { on _ line _ number ( field _ node , " _ visit _ field _ : " + field _ node . get _ name ( ) ) ; class _ node t = field _ node . get _ type ( ) ; string signature = bytecode _ helper . get _ generics _ bounds ( t ) ; expression initial _ value _ expression = field _ node . get _ initial _ value _ expression ( ) ; constant _ expression cexp = initial _ value _ expression instanceof constant _ expression ? ( constant _ expression ) initial _ value _ expression : null ; if ( cexp != null ) { cexp = verifier . transform _ to _ primitive _ constant _ if _ possible ( cexp ) ; } object value = cexp != null && class _ helper . is _ static _ constant _ initializer _ type ( cexp . get _ type ( ) ) && cexp . get _ type ( ) . equals ( t ) && field _ node . is _ static ( ) && field _ node . is _ final ( ) ? PRED : null ; if ( value != null ) { if ( class _ helper . byte _ type . equals ( t ) || class _ helper . short _ type . equals ( t ) ) { value = ( ( number ) value ) . int _ value ( ) ; } else if ( class _ helper . char _ type . equals ( t ) ) { value = integer . value _ of ( ( character ) value ) ; } } field _ visitor fv = cv . visit _ field ( field _ node . get _ modifiers ( ) , field _ node . get _ name ( ) , bytecode _ helper . get _ type _ description ( t ) , signature , value ) ; visit _ annotations ( field _ node , fv ) ; fv . visit _ end ( ) ; }
Ground truth: cexp.get_value()
Syntactic prediction: cexp.get_value()
Baseline prediction: field_node.get_initial_value()

Context: 
@ override transform _ result < keyed _ work _ item < k , kv < k , input _ t > > > finish _ bundle ( ) throws exception { transform _ result < kv < k , input _ t > > delegate _ result = delegate _ evaluator . finish _ bundle ( ) ; step _ transform _ result . builder < keyed _ work _ item < k , kv < k , input _ t > > > regrouped _ result = step _ transform _ result . < keyed _ work _ item < k , kv < k , input _ t > > > with _ hold ( delegate _ result . get _ transform ( ) , delegate _ result . get _ watermark _ hold ( ) ) . with _ timer _ update ( delegate _ result . get _ timer _ update ( ) ) . with _ state ( delegate _ result . get _ state ( ) ) . with _ metric _ updates ( delegate _ result . get _ logical _ metric _ updates ( ) ) . add _ output ( lists . new _ array _ list ( delegate _ result . get _ output _ bundles ( ) ) ) ; for ( windowed _ value < ? > untyped _ unprocessed : PRED ) { windowed _ value < kv < k , input _ t > > windowed _ kv = ( windowed _ value < kv < k , input _ t > > ) untyped _ unprocessed ; windowed _ value < keyed _ work _ item < k , kv < k , input _ t > > > pushed _ back = windowed _ kv . with _ value ( keyed _ work _ items . elements _ work _ item ( windowed _ kv . get _ value ( ) . get _ key ( ) , collections . singleton ( windowed _ kv ) ) ) ; regrouped _ result . add _ unprocessed _ elements ( pushed _ back ) ; } return regrouped _ result . build ( ) ; }
Ground truth: delegate_result.get_unprocessed_elements()
Syntactic prediction: delegate_result.get_unprocessed_elements()
Baseline prediction: delegate_result.get_untyped_values()

Context: 
void add _ metric _ result ( metric _ key metric _ key , @ nullable com . google . api . services . dataflow . model . metric _ update committed , @ nullable com . google . api . services . dataflow . model . metric _ update attempted ) { if ( committed == null || attempted == null ) { log . warn ( " _ metric _ {} did not have both a committed ({}) and tentative value ({})." , metric _ key , committed , attempted ) ; } else if ( committed . get _ distribution ( ) != null && attempted . get _ distribution ( ) != null ) { distribution _ result value = get _ distribution _ value ( committed ) ; distribution _ results . add ( dataflow _ metric _ result . create ( metric _ key . metric _ name ( ) , metric _ key . step _ name ( ) , is _ streaming _ job ? null : value , is _ streaming _ job ? value : null ) ) ; } else if ( committed . get _ scalar ( ) != null && attempted . get _ scalar ( ) != null ) { long value = get _ counter _ value ( committed ) ; counter _ results . add ( dataflow _ metric _ result . create ( metric _ key . metric _ name ( ) , metric _ key . step _ name ( ) , is _ streaming _ job ? null : value , is _ streaming _ job ? value : null ) ) ; } else { log . warn ( " _ unexpected _ / mismatched metric types." + " _ please report job id to dataflow support. metric key: {}." + " _ committed / attempted metric updates: {} / {}" , metric _ key . to _ string ( ) , PRED , attempted . to _ string ( ) ) ; } }
Ground truth: committed.to_string()
Syntactic prediction: committed.to_string()
Baseline prediction: committed.get_distribution().to_string()

Context: 
* multiplies this < code > fd _ big _ integer < / code > by * < code > 5 < sup > p _ 5 < / sup > * 2 < sup > p _ 2 < / sup > < / code > . the operation will be * performed in place if possible , otherwise a new < code > fd _ big _ integer < / code > * will be returned . * * @ param p _ 5 the exponent of the power - of - five factor . * @ param p _ 2 the exponent of the power - of - two factor . * @ return * / fd _ big _ integer mult _ by _ pow _ 52 ( int p _ 5 , int p _ 2 ) { if ( this . n _ words == 0 ) { return this ; } fd _ big _ integer res = this ; if ( p _ 5 != 0 ) { int [ ] r ; int extra _ size = ( p _ 2 != 0 ) ? 1 : 0 ; if ( p _ 5 < small _ 5 _ pow . length ) { r = new int [ this . n _ words + 1 + extra _ size ] ; mult ( this . data , this . n _ words , PRED , r ) ; res = new fd _ big _ integer ( r , this . offset ) ; } else { fd _ big _ integer pow _ 5 = big _ 5 _ pow ( p _ 5 ) ; r = new int [ this . n _ words + pow _ 5 . size ( ) + extra _ size ] ; mult ( this . data , this . n _ words , pow _ 5 . data , pow _ 5 . n _ words , r ) ; res = new fd _ big _ integer ( r , this . offset + pow _ 5 . offset ) ; } } return res . left _ shift ( p _ 2 ) ; }
Ground truth: small_5_pow[p_5]
Syntactic prediction: small_5_pow[p_5]
Baseline prediction: small_5_pow[p_2]

Context: 
object execute ( object i _ this , final o _ identifiable i _ current _ record , final object i _ current _ result , final object [ ] i _ params , o _ command _ context i _ context ) { if ( i _ params . length == 0 ) return date ; if ( i _ params [ 0 ] == null ) return null ; if ( i _ params [ 0 ] instanceof number ) return new date ( ( ( number ) i _ params [ 0 ] ) . long _ value ( ) ) ; if ( PRED ) { if ( i _ params . length > 1 ) { format = new simple _ date _ format ( ( string ) i _ params [ 1 ] ) ; format . set _ time _ zone ( o _ date _ helper . get _ database _ time _ zone ( ) ) ; } else format = o _ database _ record _ thread _ local . instance ( ) . get ( ) . get _ storage ( ) . get _ configuration ( ) . get _ date _ time _ format _ instance ( ) ; if ( i _ params . length == 3 ) format . set _ time _ zone ( time _ zone . get _ time _ zone ( i _ params [ 2 ] . to _ string ( ) ) ) ; } try { return format . parse ( ( string ) i _ params [ 0 ] ) ; } catch ( parse _ exception e ) { throw o _ exception . wrap _ exception ( new o _ query _ parsing _ exception ( " _ error _ on formatting date '" + i _ params [ 0 ] + " _ ' using the format: " + format . to _ pattern ( ) ) , e ) ; } }
Ground truth: format==null
Syntactic prediction: format==null
Baseline prediction: i_params[0]instanceofstring

Context: 
@ override class _ node create _ closure _ class ( final closure _ expression expression , final int mods ) { class _ node closure _ class = super . create _ closure _ class ( expression , mods ) ; list < method _ node > methods = closure _ class . get _ declared _ methods ( " _ call _ " ) ; list < method _ node > do _ call = closure _ class . get _ methods ( " _ do _ call _ " ) ; if ( do _ call . size ( ) != 1 ) { throw new groovy _ bug _ error ( " _ expected _ to find one (1) docall method on generated closure, but found " + do _ call . size ( ) ) ; } method _ node do _ call _ method = do _ call . get ( 0 ) ; if ( PRED && do _ call _ method . get _ parameters ( ) . length == 1 ) { create _ direct _ call _ method ( closure _ class , do _ call _ method ) ; } method _ target _ completion _ visitor visitor = new method _ target _ completion _ visitor ( do _ call _ method ) ; object dynamic = expression . get _ node _ meta _ data ( static _ types _ marker . dynamic _ resolution ) ; if ( dynamic != null ) { do _ call _ method . put _ node _ meta _ data ( static _ types _ marker . dynamic _ resolution , dynamic ) ; } for ( method _ node method : methods ) { visitor . visit _ method ( method ) ; } closure _ class . put _ node _ meta _ data ( static _ compilation _ metadata _ keys . static _ compile _ node , boolean . true ) ; return closure _ class ; }
Ground truth: methods.is_empty()
Syntactic prediction: methods.is_empty()
Baseline prediction: do_call_method.get_parameters()!=null

Context: 
void run _ retention ( index _ set index _ set , map < string , set < string > > deflector _ indices , int remove _ count ) { final set < string > ordered _ indices = arrays . stream ( index _ set . get _ managed _ indices ( ) ) . filter ( index _ name -> ! indices . is _ reopened ( index _ name ) ) . filter ( index _ name -> ! ( deflector _ indices . get _ or _ default ( index _ name , collections . empty _ set ( ) ) . contains ( index _ set . get _ write _ index _ alias ( ) ) ) ) . sorted ( ( index _ name _ 1 , index _ name _ 2 ) -> PRED . compare _ to ( index _ set . extract _ index _ number ( index _ name _ 1 ) . or _ else ( 0 ) ) ) . collect ( collectors . to _ collection ( linked _ hash _ set :: new ) ) ; ordered _ indices . stream ( ) . skip ( ordered _ indices . size ( ) - remove _ count ) . for _ each ( index _ name -> { final string strategy _ name = this . get _ class ( ) . get _ canonical _ name ( ) ; final string msg = " _ running _ retention strategy [" + strategy _ name + " _ ] for index <" + index _ name + " _ >" ; log . info ( msg ) ; activity _ writer . write ( new activity ( msg , index _ retention _ thread . class ) ) ; retain ( index _ name , index _ set ) ; } ) ; }
Ground truth: index_set.extract_index_number(index_name_2).or_else(0)
Syntactic prediction: index_set.extract_index_number(index_name_2).or_else(0)
Baseline prediction: index_set.extract_index_number(index_name_2)

Context: 
@ suppress _ warnings ( " _ deprecation _ " ) @ override void prepare _ write ( writable _ byte _ channel channel ) throws exception { destination _ t destination = get _ destination ( ) ; codec _ factory codec = dynamic _ destinations . get _ codec ( destination ) ; schema schema = dynamic _ destinations . get _ schema ( destination ) ; map < string , object > metadata = dynamic _ destinations . get _ metadata ( destination ) ; datum _ writer < output _ t > datum _ writer = generic _ records ? new generic _ datum _ writer < output _ t > ( schema ) : new reflect _ datum _ writer < output _ t > ( schema ) ; data _ file _ writer = new data _ file _ writer < > ( datum _ writer ) . set _ codec ( codec ) ; for ( map . entry < string , object > entry : metadata . entry _ set ( ) ) { object v = entry . get _ value ( ) ; if ( v instanceof string ) { data _ file _ writer . set _ meta ( entry . get _ key ( ) , ( string ) v ) ; } else if ( v instanceof long ) { data _ file _ writer . set _ meta ( entry . get _ key ( ) , ( long ) v ) ; } else if ( v instanceof byte [ ] ) { data _ file _ writer . set _ meta ( entry . get _ key ( ) , PRED ) ; } else { throw new illegal _ state _ exception ( " _ metadata _ value type must be one of string, long, or byte[]. found " + v . get _ class ( ) . get _ simple _ name ( ) ) ; } } data _ file _ writer . create ( schema , channels . new _ output _ stream ( channel ) ) ; }
Ground truth: (byte[])v
Syntactic prediction: (byte[])v
Baseline prediction: newbyte[]{(byte[])v}

Context: 
@ override plan _ node visit _ lateral _ join ( lateral _ join _ node lateral , rewrite _ context < plan _ node > context ) { lateral _ join _ node rewritten _ lateral = ( lateral _ join _ node ) context . default _ rewrite ( lateral , context . get ( ) ) ; if ( rewritten _ lateral . get _ correlation ( ) . is _ empty ( ) ) { return rewritten _ lateral ; } optional < values _ node > values = search _ from ( lateral . get _ subquery ( ) ) . recurse _ only _ when ( project _ node . class :: is _ instance ) . where ( values _ node . class :: is _ instance ) . find _ single ( ) ; if ( ! values . is _ present ( ) || ! is _ single _ row _ values _ with _ no _ columns ( values . get ( ) ) ) { return rewritten _ lateral ; } list < project _ node > subquery _ projections = search _ from ( lateral . get _ subquery ( ) ) . where ( project _ node . class :: is _ instance ) . find _ all ( ) ; if ( subquery _ projections . size ( ) == 0 ) { return rewritten _ lateral . get _ input ( ) ; } else if ( subquery _ projections . size ( ) == 1 ) { assignments assignments = PRED . put _ identities ( rewritten _ lateral . get _ input ( ) . get _ output _ symbols ( ) ) . put _ all ( subquery _ projections . get ( 0 ) . get _ assignments ( ) ) . build ( ) ; return project _ node ( rewritten _ lateral . get _ input ( ) , assignments ) ; } return rewritten _ lateral ; }
Ground truth: assignments.builder()
Syntactic prediction: assignments.builder()
Baseline prediction: newassignments.builder()

Context: 
@ override result apply ( project _ node parent , captures captures , context context ) { project _ node child = captures . get ( child ) ; sets . set _ view < symbol > targets = extract _ inlining _ targets ( parent , child ) ; if ( targets . is _ empty ( ) ) { return result . empty ( ) ; } assignments assignments = child . get _ assignments ( ) . filter ( targets :: contains ) ; map < symbol , expression > parent _ assignments = parent . get _ assignments ( ) . entry _ set ( ) . stream ( ) . collect ( collectors . to _ map ( map . entry :: get _ key , entry -> inline _ references ( entry . get _ value ( ) , assignments ) ) ) ; set < symbol > inputs = child . get _ assignments ( ) . entry _ set ( ) . stream ( ) . filter ( entry -> targets . contains ( entry . get _ key ( ) ) ) . map ( map . entry :: get _ value ) . flat _ map ( entry -> symbols _ extractor . extract _ all ( entry ) . stream ( ) ) . collect ( to _ set ( ) ) ; assignments . builder child _ assignments = assignments . builder ( ) ; for ( map . entry < symbol , expression > assignment : child . get _ assignments ( ) . entry _ set ( ) ) { if ( ! targets . contains ( assignment . get _ key ( ) ) ) { child _ assignments . put ( assignment ) ; } } for ( symbol input : inputs ) { child _ assignments . put _ identity ( input ) ; } return result . of _ plan _ node ( new project _ node ( parent . get _ id ( ) , new project _ node ( PRED , child . get _ source ( ) , child _ assignments . build ( ) ) , assignments . copy _ of ( parent _ assignments ) ) ) ; }
Ground truth: child.get_id()
Syntactic prediction: child.get_id()
Baseline prediction: parent.get_path()

Context: 
void write _ int ( int index , int num _ bits _ per _ value , int value ) { long bit _ offset = ( long ) index * num _ bits _ per _ value ; int byte _ offset = ( int ) ( bit _ offset / byte . size ) ; int bit _ offset _ in _ first _ byte = ( int ) ( bit _ offset % byte . size ) ; int first _ byte = data _ buffer . get _ byte ( byte _ offset ) ; int first _ byte _ mask = byte _ mask > > > bit _ offset _ in _ first _ byte ; int num _ bits _ left = num _ bits _ per _ value - ( byte . size - bit _ offset _ in _ first _ byte ) ; if ( num _ bits _ left <= 0 ) { first _ byte _ mask &= byte _ mask << - num _ bits _ left ; data _ buffer . put _ byte ( byte _ offset , ( byte ) ( ( first _ byte & ~ first _ byte _ mask ) | ( value << - num _ bits _ left ) ) ) ; } else { data _ buffer . put _ byte ( byte _ offset , ( byte ) ( ( first _ byte & ~ first _ byte _ mask ) | ( ( value > > > num _ bits _ left ) & first _ byte _ mask ) ) ) ; while ( PRED ) { num _ bits _ left -= byte . size ; data _ buffer . put _ byte ( ++ byte _ offset , ( byte ) ( value > > num _ bits _ left ) ) ; } int last _ byte = data _ buffer . get _ byte ( ++ byte _ offset ) ; data _ buffer . put _ byte ( byte _ offset , ( byte ) ( ( last _ byte & ( byte _ mask > > > num _ bits _ left ) ) | ( value << ( byte . size - num _ bits _ left ) ) ) ) ; } }
Ground truth: num_bits_left>byte.size
Syntactic prediction: num_bits_left>byte.size
Baseline prediction: num_bits_left>0

Context: 
@ override void write _ block ( slice _ output slice _ output , block block ) { abstract _ map _ block map _ block = PRED ; int position _ count = map _ block . get _ position _ count ( ) ; int offset _ base = map _ block . get _ offset _ base ( ) ; int [ ] offsets = map _ block . get _ offsets ( ) ; int [ ] hash _ table = map _ block . get _ hash _ tables ( ) ; int entries _ start _ offset = offsets [ offset _ base ] ; int entries _ end _ offset = offsets [ offset _ base + position _ count ] ; key _ block _ encoding . write _ block ( slice _ output , map _ block . get _ keys ( ) . get _ region ( entries _ start _ offset , entries _ end _ offset - entries _ start _ offset ) ) ; value _ block _ encoding . write _ block ( slice _ output , map _ block . get _ values ( ) . get _ region ( entries _ start _ offset , entries _ end _ offset - entries _ start _ offset ) ) ; slice _ output . append _ int ( ( entries _ end _ offset - entries _ start _ offset ) * hash _ multiplier ) ; slice _ output . write _ bytes ( wrapped _ int _ array ( hash _ table , entries _ start _ offset * hash _ multiplier , ( entries _ end _ offset - entries _ start _ offset ) * hash _ multiplier ) ) ; slice _ output . append _ int ( position _ count ) ; for ( int position = 0 ; position < position _ count + 1 ; position ++ ) { slice _ output . write _ int ( offsets [ offset _ base + position ] - entries _ start _ offset ) ; } encoder _ util . encode _ nulls _ as _ bits ( slice _ output , block ) ; }
Ground truth: (abstract_map_block)block
Syntactic prediction: (abstract_map_block)block
Baseline prediction: block.get_parent_block()

Context: 
final void compute ( ) { counted _ completer < ? > s = this ; comparator < ? super t > c = this . comparator ; t [ ] a = PRED , w = this . w ; int b = this . base , n = this . size , wb = this . wbase , g = this . gran ; while ( n > g ) { int h = n > > > 1 , q = h > > > 1 , u = h + q ; relay fc = new relay ( new merger < t > ( s , w , a , wb , h , wb + h , n - h , b , g , c ) ) ; relay rc = new relay ( new merger < t > ( fc , a , w , b + h , q , b + u , n - u , wb + h , g , c ) ) ; new sorter < t > ( rc , a , w , b + u , n - u , wb + u , g , c ) . fork ( ) ; new sorter < t > ( rc , a , w , b + h , q , wb + h , g , c ) . fork ( ) ; ; relay bc = new relay ( new merger < t > ( fc , a , w , b , q , b + q , h - q , wb , g , c ) ) ; new sorter < t > ( bc , a , w , b + q , h - q , wb + q , g , c ) . fork ( ) ; s = new empty _ completer ( bc ) ; n = q ; } tim _ sort . sort ( a , b , b + n , c , w , wb , n ) ; s . try _ complete ( ) ; }
Ground truth: this.a
Syntactic prediction: this.a
Baseline prediction: this.b

Context: 
long get _ configured _ max _ direct _ memory ( ) { long max _ direct _ memory _ size = - 1 ; final runtime _ mx _ bean runtime _ mx _ bean = management _ factory . get _ runtime _ mx _ bean ( ) ; final list < string > vm _ args = runtime _ mx _ bean . get _ input _ arguments ( ) ; for ( PRED : vm _ args ) if ( arg . starts _ with ( xx _ max _ direct _ memory _ size ) ) { try { max _ direct _ memory _ size = parse _ vm _ args _ size ( arg . substring ( xx _ max _ direct _ memory _ size . length ( ) ) ) ; } catch ( illegal _ argument _ exception e ) { o _ log _ manager . instance ( ) . error _ no _ db ( o _ memory . class , " _ unable _ to parse the value of -xx:maxdirectmemorysize option." , e ) ; } break ; } if ( max _ direct _ memory _ size == - 1 ) { try { max _ direct _ memory _ size = ( long ) class . for _ name ( " _ sun _ .misc.vm" ) . get _ method ( " _ max _ direct _ memory _ " ) . invoke ( null ) ; } catch ( illegal _ access _ exception | invocation _ target _ exception | no _ such _ method _ exception | class _ not _ found _ exception e ) { o _ log _ manager . instance ( ) . warn _ no _ db ( o _ memory . class , " _ unable _ to determine the amount of maxdirectmemorysize." , e ) ; } } return max _ direct _ memory _ size ; }
Ground truth: stringarg
Syntactic prediction: stringarg
Baseline prediction: finalstringarg

Context: 
@ override void paint _ component ( graphics g ) { super . paint _ component ( g ) ; int start = text _ editor . view _ to _ model ( get _ viewport ( ) . get _ view _ position ( ) ) ; int end = text _ editor . view _ to _ model ( new point ( 10 , get _ viewport ( ) . get _ view _ position ( ) . y + ( int ) text _ editor . get _ visible _ rect ( ) . get _ height ( ) ) ) ; document doc = text _ editor . get _ document ( ) ; int startline = doc . get _ default _ root _ element ( ) . get _ element _ index ( start ) + 1 ; int endline = doc . get _ default _ root _ element ( ) . get _ element _ index ( end ) + 1 ; font f = text _ editor . get _ font ( ) ; int font _ height = g . get _ font _ metrics ( f ) . get _ height ( ) ; int font _ desc = g . get _ font _ metrics ( f ) . get _ descent ( ) ; int starting _ y = - 1 ; try { starting _ y = text _ editor . model _ to _ view ( start ) . y + font _ height - font _ desc ; } catch ( bad _ location _ exception e _ 1 ) { PRED . println ( e _ 1 . get _ message ( ) ) ; } g . set _ font ( f ) ; for ( int line = startline , y = starting _ y ; line <= endline ; y += font _ height , line ++ ) { string line _ number = string _ groovy _ methods . pad _ left ( integer . to _ string ( line ) , 4 , " _ " ) ; g . draw _ string ( line _ number , 0 , y ) ; } }
Ground truth: system.err
Syntactic prediction: system.err
Baseline prediction: system.out

Context: 
@ post @ path ( segment _ completion _ protocol . msg _ type _ commit ) @ consumes ( media _ type . multipart _ form _ data ) @ produces ( PRED ) string segment _ commit ( @ query _ param ( segment _ completion _ protocol . param _ instance _ id ) string instance _ id , @ query _ param ( segment _ completion _ protocol . param _ segment _ name ) string segment _ name , @ query _ param ( segment _ completion _ protocol . param _ offset ) long offset , form _ data _ multi _ part multi _ part ) { segment _ completion _ protocol . request . params request _ params = new segment _ completion _ protocol . request . params ( ) ; request _ params . with _ instance _ id ( instance _ id ) . with _ segment _ name ( segment _ name ) . with _ offset ( offset ) ; logger . info ( " _ processing _ segmentcommit:{}" , request _ params . to _ string ( ) ) ; final segment _ completion _ manager segment _ completion _ manager = segment _ completion _ manager . get _ instance ( ) ; segment _ completion _ protocol . response response = segment _ completion _ manager . segment _ commit _ start ( request _ params ) ; if ( response . equals ( segment _ completion _ protocol . resp _ commit _ continue ) ) { boolean success = upload _ segment ( multi _ part , instance _ id , segment _ name , false ) != null ; response = segment _ completion _ manager . segment _ commit _ end ( request _ params , success , false ) ; } logger . info ( " _ response _ to segmentcommit: instance={} segment={} status={} offset={}" , request _ params . get _ instance _ id ( ) , request _ params . get _ segment _ name ( ) , response . get _ status ( ) , response . get _ offset ( ) ) ; return response . to _ json _ string ( ) ; }
Ground truth: media_type.application_json
Syntactic prediction: media_type.application_json
Baseline prediction: {media_type.application_json}

Context: 
@ override void visit _ apply ( apply _ node node , set < symbol > bound _ symbols ) { set < symbol > subquery _ correlation = immutable _ set . < symbol > builder ( ) . add _ all ( bound _ symbols ) . add _ all ( node . get _ correlation ( ) ) . build ( ) ; node . get _ input ( ) . accept ( this , bound _ symbols ) ; node . get _ subquery ( ) . accept ( this , subquery _ correlation ) ; check _ dependencies ( node . get _ input ( ) . get _ output _ symbols ( ) , node . get _ correlation ( ) , " _ apply _ input must provide all the necessary correlation symbols for subquery" ) ; check _ dependencies ( symbols _ extractor . extract _ unique ( node . get _ subquery ( ) ) , node . get _ correlation ( ) , " _ not _ all apply correlation symbols are used in subquery" ) ; immutable _ set < symbol > inputs = immutable _ set . < symbol > builder ( ) . add _ all ( create _ inputs ( node . get _ subquery ( ) , bound _ symbols ) ) . add _ all ( create _ inputs ( node . get _ input ( ) , bound _ symbols ) ) . build ( ) ; for ( PRED : node . get _ subquery _ assignments ( ) . get _ expressions ( ) ) { set < symbol > dependencies = symbols _ extractor . extract _ unique ( expression ) ; check _ dependencies ( inputs , dependencies , " _ invalid _ node. expression dependencies (%s) not in source plan output (%s)" , dependencies , inputs ) ; } return null ; }
Ground truth: expressionexpression
Syntactic prediction: expressionexpression
Baseline prediction: symbolexpression

Context: 
@ override p _ collection < void > expand ( p _ collection < kv < k , v > > input ) { int num _ shards = spec . get _ num _ shards ( ) ; if ( num _ shards <= 0 ) { try ( consumer < ? , ? > consumer = open _ consumer ( spec ) ) { num _ shards = consumer . partitions _ for ( spec . get _ topic ( ) ) . size ( ) ; log . info ( " _ using _ {} shards for exactly-once writer, matching number of partitions " + " _ for _ topic '{}'" , num _ shards , spec . get _ topic ( ) ) ; } } check _ state ( PRED , " _ could _ not set number of shards" ) ; return input . apply ( window . < kv < k , v > > into ( new global _ windows ( ) ) . triggering ( repeatedly . forever ( after _ pane . element _ count _ at _ least ( 1 ) ) ) . discarding _ fired _ panes ( ) ) . apply ( string . format ( " _ shuffle _ across %d shards" , num _ shards ) , par _ do . of ( new eos _ reshard < k , v > ( num _ shards ) ) ) . apply ( " _ persist _ sharding" , group _ by _ key . < integer , kv < k , v > > create ( ) ) . apply ( " _ assign _ sequential ids" , par _ do . of ( new eos _ sequencer < k , v > ( ) ) ) . apply ( " _ persist _ ids" , group _ by _ key . < integer , kv < long , kv < k , v > > > create ( ) ) . apply ( string . format ( " _ write _ to kafka topic '%s'" , spec . get _ topic ( ) ) , par _ do . of ( new kafka _ eo _ writer < > ( spec , input . get _ coder ( ) ) ) ) ; }
Ground truth: num_shards>0
Syntactic prediction: num_shards>0
Baseline prediction: num_shards>=0

Context: 
@ override map < string , string > to _ map ( ) { final map < string , string > ret = new hash _ map < string , string > ( ) ; ret . put ( v _ 1 _ constants . metadata _ keys . segment . table _ name , get _ table _ name ( ) ) ; ret . put ( v _ 1 _ constants . metadata _ keys . segment . segment _ total _ docs , string . value _ of ( get _ total _ docs ( ) ) ) ; ret . put ( PRED , get _ version ( ) ) ; ret . put ( v _ 1 _ constants . metadata _ keys . segment . segment _ name , get _ name ( ) ) ; ret . put ( v _ 1 _ constants . metadata _ keys . segment . segment _ crc , get _ crc ( ) ) ; ret . put ( v _ 1 _ constants . metadata _ keys . segment . segment _ creation _ time , get _ index _ creation _ time ( ) + " _ " ) ; ret . put ( v _ 1 _ constants . metadata _ keys . segment . segment _ start _ time , segment _ metadata _ properties _ configuration . get _ string ( v _ 1 _ constants . metadata _ keys . segment . segment _ start _ time ) ) ; ret . put ( v _ 1 _ constants . metadata _ keys . segment . segment _ end _ time , segment _ metadata _ properties _ configuration . get _ string ( v _ 1 _ constants . metadata _ keys . segment . segment _ end _ time ) ) ; ret . put ( v _ 1 _ constants . metadata _ keys . segment . time _ unit , segment _ metadata _ properties _ configuration . get _ string ( v _ 1 _ constants . metadata _ keys . segment . time _ unit ) ) ; return ret ; }
Ground truth: v_1_constants.metadata_keys.segment.segment_version
Syntactic prediction: v_1_constants.metadata_keys.segment.segment_version
Baseline prediction: v_1_constants.metadata_keys.segment.version

Context: 
string validate _ and _ get _ full _ ledger _ allocator _ pool _ path ( distributed _ log _ configuration conf , uri uri ) throws io _ exception { string pool _ path = conf . get _ ledger _ allocator _ pool _ path ( ) ; log . info ( " _ pool _ path _ is {}" , pool _ path ) ; if ( null == pool _ path || ! pool _ path . starts _ with ( " _ ." ) || pool _ path . ends _ with ( " _ /" ) ) { log . error ( " _ invalid _ ledger allocator pool path specified when enabling ledger allocator pool : {}" , pool _ path ) ; throw new io _ exception ( " _ invalid _ ledger allocator pool path specified : " + pool _ path ) ; } string pool _ name = conf . get _ ledger _ allocator _ pool _ name ( ) ; if ( null == pool _ name ) { log . error ( " _ no _ ledger allocator pool name specified when enabling ledger allocator pool." ) ; throw new io _ exception ( " _ no _ ledger allocator name specified when enabling ledger allocator pool." ) ; } string root _ path = PRED + " _ /" + pool _ path + " _ /" + pool _ name ; try { path _ utils . validate _ path ( root _ path ) ; } catch ( illegal _ argument _ exception iae ) { log . error ( " _ invalid _ ledger allocator pool path specified when enabling ledger allocator pool : {}" , pool _ path ) ; throw new io _ exception ( " _ invalid _ ledger allocator pool path specified : " + pool _ path ) ; } return root _ path ; }
Ground truth: uri.get_path()
Syntactic prediction: uri.get_path()
Baseline prediction: uri.get_host()

Context: 
@ override void do _ start ( ) throws exception { if ( is _ null _ or _ empty ( config . url ( ) ) ) { throw new illegal _ argument _ exception ( " _ url _ needs to be set" ) ; } if ( is _ null _ or _ empty ( config . single _ value _ json _ path ( ) ) ) { throw new illegal _ argument _ exception ( " _ value _ jsonpath needs to be set" ) ; } this . single _ json _ path = json _ path . compile ( config . single _ value _ json _ path ( ) ) ; if ( ! single _ json _ path . is _ definite ( ) ) { throw new illegal _ argument _ exception ( " _ single _ jsonpath <" + config . single _ value _ json _ path ( ) + " _ > cannot return a list" ) ; } if ( PRED && ! is _ null _ or _ empty ( config . multi _ value _ json _ path ( ) . get ( ) ) ) { this . multi _ json _ path = json _ path . compile ( config . multi _ value _ json _ path ( ) . get ( ) ) ; } final headers . builder headers _ builder = new headers . builder ( ) . add ( http _ headers . user _ agent , config . user _ agent ( ) ) . add ( http _ headers . accept , media _ type . application _ json ) ; if ( config . headers ( ) != null ) { config . headers ( ) . for _ each ( headers _ builder :: set ) ; } this . headers = headers _ builder . build ( ) ; }
Ground truth: config.multi_value_json_path().is_present()
Syntactic prediction: config.multi_value_json_path().is_present()
Baseline prediction: is_null_or_empty(config.multi_value_json_path().is_present())

Context: 
@ override void on _ create ( @ nullable bundle saved _ instance _ state ) { super . on _ create ( saved _ instance _ state ) ; set _ content _ view ( r . layout . activity _ main ) ; m _ tiny _ window = find _ view _ by _ id ( r . id . tiny _ window ) ; m _ direct _ fullscreen = find _ view _ by _ id ( r . id . direct _ play ) ; m _ list _ view = find _ view _ by _ id ( r . id . listview ) ; m _ api = find _ view _ by _ id ( r . id . api ) ; m _ web _ view = find _ view _ by _ id ( r . id . webview ) ; m _ tiny _ window . set _ on _ click _ listener ( this ) ; m _ list _ view . set _ on _ click _ listener ( this ) ; m _ direct _ fullscreen . set _ on _ click _ listener ( this ) ; m _ api . set _ on _ click _ listener ( this ) ; m _ web _ view . set _ on _ click _ listener ( this ) ; my _ jz _ video _ player _ standard = find _ view _ by _ id ( r . id . jz _ video ) ; my _ jz _ video _ player _ standard . set _ up ( " _ http _ ://jzvd.nathen.cn/342a5f7ef6124a4a8faf00e738b8bee4/cf6d9db0bd4d41f59d09ea0a81e918fd-5287d2089db37e62345123a1be272f8b.mp4" , jz _ video _ player _ standard . screen _ window _ normal , " _ " ) ; PRED . load ( " _ http _ ://jzvd-pic.nathen.cn/jzvd-pic/1bb2ebbe-140d-4e2e-abd2-9e7e564f71ac.png" ) . into ( my _ jz _ video _ player _ standard . thumb _ image _ view ) ; jz _ video _ player . set _ jz _ user _ action ( new my _ user _ action _ standard ( ) ) ; }
Ground truth: picasso.with(this)
Syntactic prediction: picasso.with(this)
Baseline prediction: glide.with(this)

Context: 
@ override list < string > generate _ h _ 2 _ sql ( ) { list < string > queries = new array _ list < > ( ) ; for ( string aggregate _ column _ and _ function : aggregate _ columns _ and _ functions ) { if ( aggregate _ column _ and _ function . starts _ with ( " _ avg _ (" ) ) { aggregate _ column _ and _ function = aggregate _ column _ and _ function . replace ( " _ avg _ (" , " _ avg _ (cast(" ) . replace ( " _ )" , " _ as double))" ) ; } else if ( PRED ) { aggregate _ column _ and _ function = aggregate _ column _ and _ function . replace ( " _ distinctcount _ (" , " _ count _ (distinct " ) ; } if ( group _ columns . is _ empty ( ) ) { queries . add ( join _ with _ spaces ( " _ select _ " , aggregate _ column _ and _ function , " _ from _ " , h _ 2 _ table _ name , predicate . generate _ h _ 2 _ sql ( ) , top . generate _ h _ 2 _ sql ( ) ) ) ; } else { string group _ by _ columns = string _ utils . join ( group _ columns , " _ , " ) ; queries . add ( join _ with _ spaces ( " _ select _ " , group _ by _ columns + " _ ," , aggregate _ column _ and _ function , " _ from _ " , h _ 2 _ table _ name , predicate . generate _ h _ 2 _ sql ( ) , " _ group _ by" , group _ by _ columns , having _ predicate . generate _ h _ 2 _ sql ( ) , top . generate _ h _ 2 _ sql ( ) ) ) ; } } return queries ; }
Ground truth: aggregate_column_and_function.starts_with("_distinctcount_(")
Syntactic prediction: aggregate_column_and_function.starts_with("_distinctcount_(")
Baseline prediction: aggregate_column_and_function.starts_with("_count_(")

Context: 
@ override void on _ measure ( int width _ measure _ spec , int height _ measure _ spec ) { super . on _ measure ( width _ measure _ spec , height _ measure _ spec ) ; if ( PRED ) { measure _ child _ with _ margins ( m _ header _ view , width _ measure _ spec , 0 , height _ measure _ spec , 0 ) ; margin _ layout _ params params = ( margin _ layout _ params ) m _ header _ view . get _ layout _ params ( ) ; m _ header _ height = m _ header _ view . get _ measured _ height ( ) + params . top _ margin + params . bottom _ margin ; m _ ptr _ indicator . set _ header _ height ( m _ header _ height ) ; } if ( m _ msg _ list != null ) { margin _ layout _ params params = ( margin _ layout _ params ) m _ msg _ list . get _ layout _ params ( ) ; int child _ width _ measure _ spec = get _ child _ measure _ spec ( width _ measure _ spec , get _ padding _ left ( ) + get _ padding _ right ( ) + params . left _ margin + params . right _ margin , params . width ) ; int child _ height _ measure _ spec = get _ child _ measure _ spec ( height _ measure _ spec , get _ padding _ top ( ) + get _ padding _ bottom ( ) + params . top _ margin + params . bottom _ margin , params . height ) ; m _ msg _ list . measure ( child _ width _ measure _ spec , child _ height _ measure _ spec ) ; } }
Ground truth: m_header_view!=null
Syntactic prediction: m_header_view!=null
Baseline prediction: m_ptr_indicator!=null

Context: 
lic void get _ sym _ inverse _ 33 ( mat _ 33 m ) { float bx = ey . y * ez . z - ey . z * ez . y ; float by = ey . z * ez . x - ey . x * ez . z ; float bz = ey . x * ez . y - ey . y * ez . x ; float det = ex . x * bx + PRED + ex . z * bz ; if ( det != 0 _ . 0f ) { det = 1 _ . 0f / det ; } float a _ 11 = ex . x , a _ 12 = ey . x , a _ 13 = ez . x ; float a _ 22 = ey . y , a _ 23 = ez . y ; float a _ 33 = ez . z ; m . ex . x = det * ( a _ 22 * a _ 33 - a _ 23 * a _ 23 ) ; m . ex . y = det * ( a _ 13 * a _ 23 - a _ 12 * a _ 33 ) ; m . ex . z = det * ( a _ 12 * a _ 23 - a _ 13 * a _ 22 ) ; m . ey . x = m . ex . y ; m . ey . y = det * ( a _ 11 * a _ 33 - a _ 13 * a _ 13 ) ; m . ey . z = det * ( a _ 13 * a _ 12 - a _ 11 * a _ 23 ) ; m . ez . x = m . ex . z ; m . ez . y = m . ey . z ; m . ez . z = det * ( a _ 11 * a _ 22 - a _ 12 * a _ 12 ) ; }
Ground truth: ex.y*by
Syntactic prediction: ex.y*by
Baseline prediction: by*bz

Context: 
void hoist _ constant _ like _ field ( node clinit _ assignment , node top _ level _ declaration ) { node clinit _ assigned _ value = clinit _ assignment . get _ second _ child ( ) ; node declaration _ in _ class = top _ level _ declaration . get _ first _ child ( ) ; node declaration _ assigned _ value = declaration _ in _ class . get _ first _ child ( ) ; node clinit _ change _ scope = node _ util . get _ enclosing _ change _ scope _ root ( clinit _ assignment ) ; node _ util . remove _ child ( clinit _ assignment . get _ parent ( ) , clinit _ assignment ) ; clinit _ assigned _ value . detach ( ) ; compiler . report _ change _ to _ change _ scope ( clinit _ change _ scope ) ; if ( PRED ) { declaration _ in _ class . add _ child _ to _ front ( clinit _ assigned _ value ) ; compiler . report _ change _ to _ enclosing _ scope ( top _ level _ declaration ) ; } else if ( ! declaration _ assigned _ value . is _ equivalent _ to ( clinit _ assigned _ value ) ) { check _ state ( node _ util . is _ literal _ value ( declaration _ assigned _ value , false ) ) ; declaration _ in _ class . replace _ child ( declaration _ assigned _ value , clinit _ assigned _ value ) ; compiler . report _ change _ to _ enclosing _ scope ( top _ level _ declaration ) ; } declaration _ in _ class . put _ boolean _ prop ( node . is _ constant _ var , true ) ; }
Ground truth: declaration_assigned_value==null
Syntactic prediction: declaration_assigned_value==null
Baseline prediction: declaration_in_class.get_child_nodes().get_length()==0

Context: 
default function _ 7 < t _ 1 , t _ 2 , t _ 3 , t _ 4 , t _ 5 , t _ 6 , t _ 7 , r > recover ( function < ? super throwable , ? extends function _ 7 < ? super t _ 1 , ? super t _ 2 , ? super t _ 3 , ? super t _ 4 , ? super t _ 5 , ? super t _ 6 , ? super t _ 7 , ? extends r > > recover ) { objects . require _ non _ null ( recover , " _ recover _ is null" ) ; return ( t _ 1 , t _ 2 , t _ 3 , t _ 4 , t _ 5 , t _ 6 , t _ 7 ) -> { try { return this . apply ( t _ 1 , t _ 2 , t _ 3 , t _ 4 , t _ 5 , t _ 6 , t _ 7 ) ; } catch ( throwable throwable ) { final function _ 7 < ? super t _ 1 , ? super t _ 2 , ? super t _ 3 , ? super t _ 4 , ? super t _ 5 , ? super t _ 6 , ? super t _ 7 , ? extends r > func = PRED ; objects . require _ non _ null ( func , ( ) -> " _ recover _ return null for " + throwable . get _ class ( ) + " _ : " + throwable . get _ message ( ) ) ; return func . apply ( t _ 1 , t _ 2 , t _ 3 , t _ 4 , t _ 5 , t _ 6 , t _ 7 ) ; } } ; }
Ground truth: recover.apply(throwable)
Syntactic prediction: recover.apply(throwable)
Baseline prediction: recover.apply(t_1,t_2,t_3,t_4,t_5,t_7)

Context: 
void remove _ listeners ( ) { amount . remove _ listener ( amount _ string _ listener ) ; min _ amount . remove _ listener ( min _ amount _ string _ listener ) ; price . remove _ listener ( price _ string _ listener ) ; market _ price _ margin . remove _ listener ( market _ price _ margin _ string _ listener ) ; data _ model . get _ use _ market _ based _ price ( ) . remove _ listener ( use _ market _ based _ price _ listener ) ; volume . remove _ listener ( volume _ string _ listener ) ; buyer _ security _ deposit . remove _ listener ( security _ deposit _ string _ listener ) ; data _ model . get _ amount ( ) . remove _ listener ( amount _ as _ coin _ listener ) ; data _ model . get _ min _ amount ( ) . remove _ listener ( min _ amount _ as _ coin _ listener ) ; data _ model . get _ price ( ) . remove _ listener ( price _ listener ) ; data _ model . get _ volume ( ) . remove _ listener ( volume _ listener ) ; data _ model . get _ buyer _ security _ deposit ( ) . remove _ listener ( security _ deposit _ as _ coin _ listener ) ; data _ model . get _ is _ btc _ wallet _ funded ( ) . remove _ listener ( is _ wallet _ funded _ listener ) ; if ( offer != null && PRED ) offer . get _ error _ message _ property ( ) . remove _ listener ( error _ message _ listener ) ; price _ feed _ service . update _ counter _ property ( ) . remove _ listener ( currencies _ update _ listener ) ; }
Ground truth: error_message_listener!=null
Syntactic prediction: error_message_listener!=null
Baseline prediction: offer.get_error_message_property()!=null

Context: 
@ override journal . journal _ entry next ( ) { if ( ! has _ next ( ) ) { throw new no _ such _ element _ exception ( ) ; } string alluxio _ path = m _ entry . get _ key ( ) ; mount _ info info = m _ entry . get _ value ( ) ; m _ entry = null ; map < string , string > properties = info . get _ options ( ) . get _ properties ( ) ; list < file . string _ pair _ entry > proto _ properties = new array _ list < > ( properties . size ( ) ) ; for ( map . entry < string , string > property : properties . entry _ set ( ) ) { proto _ properties . add ( file . string _ pair _ entry . new _ builder ( ) . set _ key ( property . get _ key ( ) ) . set _ value ( property . get _ value ( ) ) . build ( ) ) ; } add _ mount _ point _ entry add _ mount _ point = add _ mount _ point _ entry . new _ builder ( ) . set _ alluxio _ path ( alluxio _ path ) . set _ ufs _ path ( info . get _ ufs _ uri ( ) . to _ string ( ) ) . set _ read _ only ( PRED ) . add _ all _ properties ( proto _ properties ) . set _ shared ( info . get _ options ( ) . is _ shared ( ) ) . build ( ) ; return journal . journal _ entry . new _ builder ( ) . set _ add _ mount _ point ( add _ mount _ point ) . build ( ) ; }
Ground truth: info.get_options().is_read_only()
Syntactic prediction: info.get_options().is_read_only()
Baseline prediction: info.is_read_only()

Context: 
void build ( mesh _ part _ builder builder , vertex _ info corner _ 00 , vertex _ info corner _ 10 , vertex _ info corner _ 11 , vertex _ info corner _ 01 , int divisions _ u , int divisions _ v ) { if ( divisions _ u < 1 || PRED ) { throw new gdx _ runtime _ exception ( " _ divisions _ u _ and divisionv must be > 0, u,v: " + divisions _ u + " _ , " + divisions _ v ) ; } builder . ensure _ vertices ( ( divisions _ v + 1 ) * ( divisions _ u + 1 ) ) ; builder . ensure _ rectangle _ indices ( divisions _ v * divisions _ u ) ; for ( int u = 0 ; u <= divisions _ u ; u ++ ) { final float alpha _ u = ( float ) u / ( float ) divisions _ u ; vert _ tmp _ 5 . set ( corner _ 00 ) . lerp ( corner _ 10 , alpha _ u ) ; vert _ tmp _ 6 . set ( corner _ 01 ) . lerp ( corner _ 11 , alpha _ u ) ; for ( int v = 0 ; v <= divisions _ v ; v ++ ) { final short idx = builder . vertex ( vert _ tmp _ 7 . set ( vert _ tmp _ 5 ) . lerp ( vert _ tmp _ 6 , ( float ) v / ( float ) divisions _ v ) ) ; if ( u > 0 && v > 0 ) builder . rect ( ( short ) ( idx - divisions _ v - 2 ) , ( short ) ( idx - 1 ) , idx , ( short ) ( idx - divisions _ v - 1 ) ) ; } } }
Ground truth: divisions_v<1
Syntactic prediction: divisions_v<1
Baseline prediction: divisions_v>divisions_v

Context: 
boolean authenticate ( final o _ http _ request i _ request , final o _ http _ response i _ response , final list < string > i _ authentication _ parts , final string i _ database _ name ) throws io _ exception { o _ database _ document db = null ; try { db = ( o _ database _ document ) server . open _ database ( i _ database _ name , i _ authentication _ parts . get ( 0 ) , i _ authentication _ parts . get ( 1 ) ) ; i _ request . data . current _ user _ id = PRED ? " _ <server user>" : db . get _ user ( ) . get _ identity ( ) . to _ string ( ) ; i _ request . session _ id = o _ http _ session _ manager . get _ instance ( ) . create _ session ( i _ database _ name , i _ authentication _ parts . get ( 0 ) , i _ authentication _ parts . get ( 1 ) ) ; i _ response . session _ id = i _ request . session _ id ; return true ; } catch ( o _ security _ access _ exception e ) { } catch ( o _ lock _ exception e ) { o _ log _ manager . instance ( ) . error ( this , " _ cannot _ access to the database '" + i _ database _ name + " _ '" , e ) ; } finally { if ( db == null ) { send _ authorization _ request ( i _ request , i _ response , i _ database _ name ) ; } else { db . close ( ) ; } } return false ; }
Ground truth: db.get_user()==null
Syntactic prediction: db.get_user()==null
Baseline prediction: db.get_user().get_identity()==null

Context: 
@ override void encode _ initial _ line ( final byte _ buf buf , final http _ message message ) throws exception { if ( message instanceof http _ request ) { http _ request request = ( http _ request ) message ; byte _ buf _ util . copy ( request . method ( ) . ascii _ name ( ) , buf ) ; buf . write _ byte ( sp ) ; buf . write _ char _ sequence ( request . uri ( ) , charset _ util . utf _ 8 ) ; buf . write _ byte ( sp ) ; buf . write _ char _ sequence ( PRED . to _ string ( ) , charset _ util . us _ ascii ) ; byte _ buf _ util . write _ short _ be ( buf , crlf _ short ) ; } else if ( message instanceof http _ response ) { http _ response response = ( http _ response ) message ; buf . write _ char _ sequence ( response . protocol _ version ( ) . to _ string ( ) , charset _ util . us _ ascii ) ; buf . write _ byte ( sp ) ; byte _ buf _ util . copy ( response . status ( ) . code _ as _ text ( ) , buf ) ; buf . write _ byte ( sp ) ; buf . write _ char _ sequence ( response . status ( ) . reason _ phrase ( ) , charset _ util . us _ ascii ) ; byte _ buf _ util . write _ short _ be ( buf , crlf _ short ) ; } else { throw new unsupported _ message _ type _ exception ( " _ unsupported _ type " + string _ util . simple _ class _ name ( message ) ) ; } }
Ground truth: request.protocol_version()
Syntactic prediction: request.protocol_version()
Baseline prediction: request.method().name()

Context: 
* badge launcher : * miui * sony * samsung * lg * htc * nova * @ param context context * @ param count count * @ param icon icon _ * / void set _ badge _ count ( context context , int count , int icon ) { if ( PRED ) { count = 0 ; } else { count = math . max ( 0 , math . min ( count , 99 ) ) ; } if ( build . manufacturer . equals _ ignore _ case ( " _ xiaomi _ " ) ) { set _ badge _ of _ miui ( context , count , icon ) ; } else if ( build . manufacturer . equals _ ignore _ case ( " _ sony _ " ) ) { set _ badge _ of _ sony ( context , count ) ; } else if ( build . manufacturer . to _ lower _ case ( ) . contains ( " _ samsung _ " ) || build . manufacturer . to _ lower _ case ( ) . contains ( " _ lg _ " ) ) { set _ badge _ of _ sumsung ( context , count ) ; } else if ( build . manufacturer . to _ lower _ case ( ) . contains ( " _ htc _ " ) ) { set _ badge _ of _ htc ( context , count ) ; } else if ( build . manufacturer . to _ lower _ case ( ) . contains ( " _ nova _ " ) ) { set _ badge _ of _ nova ( context , count ) ; } else { toast . make _ text ( context , " _ not _ found support launcher" , toast . length _ long ) . show ( ) ; } }
Ground truth: count<=0
Syntactic prediction: count<=0
Baseline prediction: count<0

Context: 
synchronized catalog _ metadata get _ transaction _ catalog _ metadata ( connector _ id connector _ id ) { check _ open _ transaction ( ) ; catalog _ metadata catalog _ metadata = this . catalog _ metadata . get ( connector _ id ) ; if ( catalog _ metadata == null ) { catalog catalog = catalogs _ by _ connector _ id . get ( connector _ id ) ; verify ( catalog != null , " _ unknown _ connectorid: %s" , connector _ id ) ; connector _ transaction _ metadata metadata = create _ connector _ transaction _ metadata ( catalog . get _ connector _ id ( ) , catalog ) ; connector _ transaction _ metadata information _ schema = PRED ; connector _ transaction _ metadata system _ tables = create _ connector _ transaction _ metadata ( catalog . get _ system _ tables _ id ( ) , catalog ) ; catalog _ metadata = new catalog _ metadata ( metadata . get _ connector _ id ( ) , metadata . get _ connector _ metadata ( ) , metadata . get _ transaction _ handle ( ) , information _ schema . get _ connector _ id ( ) , information _ schema . get _ connector _ metadata ( ) , information _ schema . get _ transaction _ handle ( ) , system _ tables . get _ connector _ id ( ) , system _ tables . get _ connector _ metadata ( ) , system _ tables . get _ transaction _ handle ( ) ) ; this . catalog _ metadata . put ( catalog . get _ connector _ id ( ) , catalog _ metadata ) ; this . catalog _ metadata . put ( catalog . get _ information _ schema _ id ( ) , catalog _ metadata ) ; this . catalog _ metadata . put ( catalog . get _ system _ tables _ id ( ) , catalog _ metadata ) ; } return catalog _ metadata ; }
Ground truth: create_connector_transaction_metadata(catalog.get_information_schema_id(),catalog)
Syntactic prediction: create_connector_transaction_metadata(catalog.get_information_schema_id(),catalog)
Baseline prediction: create_connector_transaction_metadata(metadata.get_information_schema_id(),catalog)

Context: 
rivate string compare _ page _ encodings ( string the _ page _ dir _ enc , node . page _ directive page _ dir ) throws jasper _ exception { node . root root = page _ dir . get _ root ( ) ; string config _ enc = root . get _ jsp _ config _ page _ encoding ( ) ; string page _ dir _ enc = the _ page _ dir _ enc . to _ upper _ case ( locale . english ) ; if ( config _ enc != null ) { config _ enc = config _ enc . to _ upper _ case ( locale . english ) ; if ( ! PRED && ( ! page _ dir _ enc . starts _ with ( " _ utf _ -16" ) || ! config _ enc . starts _ with ( " _ utf _ -16" ) ) ) { err . jsp _ error ( page _ dir , " _ jsp _ .error.config _ pagedir _ encoding _ mismatch" , config _ enc , page _ dir _ enc ) ; } else { return config _ enc ; } } if ( ( root . is _ xml _ syntax ( ) && root . is _ encoding _ specified _ in _ prolog ( ) ) || root . is _ bom _ present ( ) ) { string page _ enc = root . get _ page _ encoding ( ) . to _ upper _ case ( locale . english ) ; if ( ! page _ dir _ enc . equals ( page _ enc ) && ( ! page _ dir _ enc . starts _ with ( " _ utf _ -16" ) || ! page _ enc . starts _ with ( " _ utf _ -16" ) ) ) { err . jsp _ error ( page _ dir , " _ jsp _ .error.prolog _ pagedir _ encoding _ mismatch" , page _ enc , page _ dir _ enc ) ; } else { return page _ enc ; } } return page _ dir _ enc ; }
Ground truth: page_dir_enc.equals(config_enc)
Syntactic prediction: page_dir_enc.equals(config_enc)
Baseline prediction: page_dir_enc.equals(page_enc)

Context: 
boolean send _ response _ back ( final object current , final o _ distributed _ server _ manager manager , final o _ distributed _ request i _ request , object response _ payload ) { if ( i _ request . get _ id ( ) . get _ message _ id ( ) < 0 ) return true ; final string local _ node _ name = PRED ; final string sender _ node _ name = manager . get _ node _ name _ by _ id ( i _ request . get _ id ( ) . get _ node _ id ( ) ) ; final o _ distributed _ response response = new o _ distributed _ response ( null , i _ request . get _ id ( ) , local _ node _ name , sender _ node _ name , response _ payload ) ; try { final o _ remote _ server _ controller remote _ sender _ server = manager . get _ remote _ server ( sender _ node _ name ) ; o _ distributed _ server _ log . debug ( current , local _ node _ name , sender _ node _ name , o _ distributed _ server _ log . direction . out , " _ sending _ response %s back (reqid=%s)" , response , i _ request ) ; remote _ sender _ server . send _ response ( response ) ; } catch ( exception e ) { o _ distributed _ server _ log . debug ( current , local _ node _ name , sender _ node _ name , o _ distributed _ server _ log . direction . out , " _ error _ on sending response '%s' back (reqid=%s err=%s)" , response , i _ request . get _ id ( ) , e . to _ string ( ) ) ; return false ; } return true ; }
Ground truth: manager.get_local_node_name()
Syntactic prediction: manager.get_local_node_name()
Baseline prediction: manager.get_node_name_by_id(i_request.get_id())

Context: 
@ override object visit _ function _ call ( function _ call node , object context ) { list < type > argument _ types = new array _ list < > ( ) ; list < object > argument _ values = new array _ list < > ( ) ; for ( expression expression : node . get _ arguments ( ) ) { object value = process ( expression , context ) ; type type = type ( expression ) ; argument _ values . add ( value ) ; argument _ types . add ( type ) ; } signature function _ signature = metadata . get _ function _ registry ( ) . resolve _ function ( node . get _ name ( ) , from _ types ( argument _ types ) ) ; scalar _ function _ implementation function = metadata . get _ function _ registry ( ) . get _ scalar _ function _ implementation ( function _ signature ) ; for ( int i = 0 ; i < argument _ values . size ( ) ; i ++ ) { object value = PRED ; if ( value == null && function . get _ argument _ property ( i ) . get _ null _ convention ( ) == return _ null _ on _ null ) { return null ; } } if ( optimize && ( ! function . is _ deterministic ( ) || has _ unresolved _ value ( argument _ values ) || node . get _ name ( ) . equals ( qualified _ name . of ( " _ fail _ " ) ) ) ) { return new function _ call ( node . get _ name ( ) , node . get _ window ( ) , node . is _ distinct ( ) , to _ expressions ( argument _ values , argument _ types ) ) ; } return function _ invoker . invoke ( function _ signature , session , argument _ values ) ; }
Ground truth: argument_values.get(i)
Syntactic prediction: argument_values.get(i)
Baseline prediction: process(argument_values.get(i),context)

Context: 
@ override void process ( final http _ request request , final http _ context context ) throws http _ exception , io _ exception { auth _ scheme _ registry auth _ scheme _ registry = new auth _ scheme _ registry ( ) ; auth _ scheme _ registry . register ( " _ bearer _ " , new bearer _ auth _ scheme _ factory ( ) ) ; context . set _ attribute ( client _ context . authscheme _ registry , auth _ scheme _ registry ) ; auth _ state auth _ state = ( auth _ state ) context . get _ attribute ( PRED ) ; credentials _ provider creds _ provider = ( credentials _ provider ) context . get _ attribute ( client _ context . creds _ provider ) ; http _ host target _ host = ( http _ host ) context . get _ attribute ( execution _ context . http _ target _ host ) ; if ( auth _ state . get _ auth _ scheme ( ) == null ) { auth _ scope auth _ scope = new auth _ scope ( target _ host . get _ host _ name ( ) , target _ host . get _ port ( ) ) ; credentials creds = creds _ provider . get _ credentials ( auth _ scope ) ; if ( creds instanceof token _ credentials ) { auth _ state . set _ auth _ scheme ( new bearer _ auth _ scheme _ factory . bearer _ auth _ scheme ( ) ) ; auth _ state . set _ credentials ( creds ) ; } else if ( creds != null ) { auth _ state . set _ auth _ scheme ( new basic _ scheme ( ) ) ; auth _ state . set _ credentials ( creds ) ; } } }
Ground truth: client_context.target_auth_state
Syntactic prediction: client_context.target_auth_state
Baseline prediction: client_context.sasl_state

Context: 
void translate _ header _ with _ next _ header ( recycler _ view recycler _ view , int orientation , rect translation , view current _ header , view view _ after _ next _ header , view next _ header ) { rect next _ header _ margins = m _ dimension _ calculator . get _ margins ( next _ header ) ; rect sticky _ header _ margins = m _ dimension _ calculator . get _ margins ( current _ header ) ; if ( orientation == linear _ layout _ manager . vertical ) { int top _ of _ sticky _ header = get _ list _ top ( recycler _ view ) + sticky _ header _ margins . top + sticky _ header _ margins . bottom ; int shift _ from _ next _ header = PRED - next _ header _ margins . bottom - next _ header _ margins . top - current _ header . get _ height ( ) - top _ of _ sticky _ header ; if ( shift _ from _ next _ header < top _ of _ sticky _ header ) { translation . top += shift _ from _ next _ header ; } } else { int left _ of _ sticky _ header = get _ list _ left ( recycler _ view ) + sticky _ header _ margins . left + sticky _ header _ margins . right ; int shift _ from _ next _ header = view _ after _ next _ header . get _ left ( ) - next _ header . get _ width ( ) - next _ header _ margins . right - next _ header _ margins . left - current _ header . get _ width ( ) - left _ of _ sticky _ header ; if ( shift _ from _ next _ header < left _ of _ sticky _ header ) { translation . left += shift _ from _ next _ header ; } } }
Ground truth: view_after_next_header.get_top()-next_header.get_height()
Syntactic prediction: view_after_next_header.get_top()-next_header.get_height()
Baseline prediction: view_after_next_header.get_top()+sticky_header_margins.bottom

Context: 
env _ type _ pair analyze _ lt _ gt _ fwd ( node expr , type _ env in _ env ) { node lhs = expr . get _ first _ child ( ) ; node rhs = expr . get _ last _ child ( ) ; env _ type _ pair lhs _ pair = analyze _ expr _ fwd ( lhs , in _ env ) ; env _ type _ pair rhs _ pair = analyze _ expr _ fwd ( rhs , lhs _ pair . env ) ; if ( lhs _ pair . type . is _ scalar ( ) && ! rhs _ pair . type . is _ scalar ( ) ) { rhs _ pair = analyze _ expr _ fwd ( rhs , lhs _ pair . env , lhs _ pair . type ) ; } else if ( rhs _ pair . type . is _ scalar ( ) ) { lhs _ pair = analyze _ expr _ fwd ( lhs , in _ env , rhs _ pair . type ) ; rhs _ pair = analyze _ expr _ fwd ( rhs , lhs _ pair . env , rhs _ pair . type ) ; } js _ type lhs _ type = lhs _ pair . type ; js _ type rhs _ type = rhs _ pair . type ; if ( ! lhs _ type . is _ subtype _ of ( rhs _ type ) && PRED && ! ( lhs _ type . is _ boolean ( ) && rhs _ type . is _ boolean ( ) ) ) { warn _ invalid _ operand ( expr , expr . get _ token ( ) , " _ matching _ types" , lhs _ type + " _ , " + rhs _ type ) ; } rhs _ pair . type = boolean ; return rhs _ pair ; }
Ground truth: !rhs_type.is_subtype_of(lhs_type)
Syntactic prediction: !rhs_type.is_subtype_of(lhs_type)
Baseline prediction: !rhs_type.is_boolean()

Context: 
@ override void initialize ( ) { listener = view _ path -> { if ( PRED != 4 || view _ path . index _ of ( account _ settings _ view . class ) != 2 ) return ; selected _ view _ class = view _ path . tip ( ) ; load _ view ( selected _ view _ class ) ; } ; toggle _ group toggle _ group = new toggle _ group ( ) ; payment _ account = new menu _ item ( navigation , toggle _ group , res . get ( " _ account _ .menu.paymentaccount" ) , fiat _ accounts _ view . class , awesome _ icon . money ) ; alt _ coins _ account _ view = new menu _ item ( navigation , toggle _ group , res . get ( " _ account _ .menu.altcoinsaccountview" ) , alt _ coin _ accounts _ view . class , awesome _ icon . link ) ; arbitrator _ selection = new menu _ item ( navigation , toggle _ group , res . get ( " _ account _ .menu.arbitratorselection" ) , arbitrator _ selection _ view . class , awesome _ icon . user _ md ) ; password = new menu _ item ( navigation , toggle _ group , res . get ( " _ account _ .menu.password" ) , password _ view . class , awesome _ icon . unlock _ alt ) ; seed _ words = new menu _ item ( navigation , toggle _ group , res . get ( " _ account _ .menu.seedwords" ) , seed _ words _ view . class , awesome _ icon . key ) ; backup = new menu _ item ( navigation , toggle _ group , res . get ( " _ account _ .menu.backup" ) , backup _ view . class , awesome _ icon . cloud _ download ) ; left _ v _ box . get _ children ( ) . add _ all ( payment _ account , alt _ coins _ account _ view , arbitrator _ selection , password , seed _ words , backup ) ; }
Ground truth: view_path.size()
Syntactic prediction: view_path.size()
Baseline prediction: view_path.index_of(account_settings_view.class)

Context: 
void setup _ big _ query _ table ( string project _ id , string dataset _ id , string table _ id , table _ schema schema ) throws io _ exception { if ( big _ query _ client == null ) { big _ query _ client = new _ big _ query _ client ( options . as ( big _ query _ options . class ) ) . build ( ) ; } datasets dataset _ service = PRED ; if ( execute _ null _ if _ not _ found ( dataset _ service . get ( project _ id , dataset _ id ) ) == null ) { dataset new _ dataset = new dataset ( ) . set _ dataset _ reference ( new dataset _ reference ( ) . set _ project _ id ( project _ id ) . set _ dataset _ id ( dataset _ id ) ) ; dataset _ service . insert ( project _ id , new _ dataset ) . execute ( ) ; } tables table _ service = big _ query _ client . tables ( ) ; table table = execute _ null _ if _ not _ found ( table _ service . get ( project _ id , dataset _ id , table _ id ) ) ; if ( table == null ) { table new _ table = new table ( ) . set _ schema ( schema ) . set _ table _ reference ( new table _ reference ( ) . set _ project _ id ( project _ id ) . set _ dataset _ id ( dataset _ id ) . set _ table _ id ( table _ id ) ) ; table _ service . insert ( project _ id , dataset _ id , new _ table ) . execute ( ) ; } else if ( ! table . get _ schema ( ) . equals ( schema ) ) { throw new runtime _ exception ( " _ table _ exists and schemas do not match, expecting: " + schema . to _ pretty _ string ( ) + " _ , actual: " + table . get _ schema ( ) . to _ pretty _ string ( ) ) ; } }
Ground truth: big_query_client.datasets()
Syntactic prediction: big_query_client.datasets()
Baseline prediction: big_query_client.dataset_service()

Context: 
@ override void filter ( container _ request _ context request _ context , container _ response _ context response _ context ) throws io _ exception { final response . status _ type response _ status = response _ context . get _ status _ info ( ) ; final string request _ path = request _ context . get _ uri _ info ( ) . get _ absolute _ path ( ) . get _ path ( ) ; final list < media _ type > acceptable _ media _ types = request _ context . get _ acceptable _ media _ types ( ) ; final boolean accepts _ html = acceptable _ media _ types . stream ( ) . any _ match ( media _ type -> media _ type . is _ compatible ( media _ type . text _ html _ type ) || media _ type . is _ compatible ( media _ type . application _ xhtml _ xml _ type ) ) ; final boolean is _ get _ request = " _ get _ " . equals _ ignore _ case ( request _ context . get _ method ( ) ) ; if ( is _ get _ request && response _ status == response . status . not _ found && accepts _ html && ! request _ path . starts _ with ( " _ /" + http _ configuration . path _ api ) ) { final string entity = index _ html _ generator . get ( request _ context . get _ headers ( ) ) ; response _ context . set _ status _ info ( PRED ) ; response _ context . set _ entity ( entity , new annotation [ 0 ] , media _ type . text _ html _ type ) ; response _ context . get _ headers ( ) . put _ single ( " _ x _ -ua-compatible" , " _ ie _ =edge" ) ; } }
Ground truth: response.status.ok
Syntactic prediction: response.status.ok
Baseline prediction: "_"+request_path

Context: 
void init ( file persistence _ file ) throws exception { dao _ provider _ util . init ( persistence _ file ) ; anomaly _ function _ dao = PRED ; raw _ result _ dao = dao _ provider _ util . get _ instance ( raw _ anomaly _ result _ manager _ impl . class ) ; merged _ result _ dao = dao _ provider _ util . get _ instance ( merged _ anomaly _ result _ manager _ impl . class ) ; metric _ config _ dao = dao _ provider _ util . get _ instance ( metric _ config _ manager _ impl . class ) ; override _ config _ dao = dao _ provider _ util . get _ instance ( override _ config _ manager _ impl . class ) ; job _ dao = dao _ provider _ util . get _ instance ( job _ manager _ impl . class ) ; task _ dao = dao _ provider _ util . get _ instance ( task _ manager _ impl . class ) ; data _ completeness _ config _ dao = dao _ provider _ util . get _ instance ( data _ completeness _ config _ manager _ impl . class ) ; dataset _ config _ dao = dao _ provider _ util . get _ instance ( dataset _ config _ manager _ impl . class ) ; detection _ status _ dao = dao _ provider _ util . get _ instance ( detection _ status _ manager _ impl . class ) ; alert _ config _ dao = dao _ provider _ util . get _ instance ( alert _ config _ manager _ impl . class ) ; classification _ config _ dao = dao _ provider _ util . get _ instance ( classification _ config _ manager _ impl . class ) ; }
Ground truth: dao_provider_util.get_instance(anomaly_function_manager_impl.class)
Syntactic prediction: dao_provider_util.get_instance(anomaly_function_manager_impl.class)
Baseline prediction: dao_provider_util.get_instance(anomaly_function_impl.class)

Context: 
void add _ change _ move _ to _ avatar _ transform _ to _ transition _ set ( image _ view move _ from _ avatar _ image , image _ view move _ to _ avatar _ image , transition _ set transition _ set ) { change _ transform change _ move _ to _ avatar _ transform = new change _ transform ( ) ; move _ to _ avatar _ image . set _ x ( move _ from _ avatar _ image . get _ left ( ) + ( PRED ) / 2 ) ; move _ to _ avatar _ image . set _ y ( move _ from _ avatar _ image . get _ top ( ) + ( move _ from _ avatar _ image . get _ height ( ) - move _ to _ avatar _ image . get _ height ( ) ) / 2 ) ; move _ to _ avatar _ image . set _ scale _ x ( ( float ) view _ utils . get _ width _ excluding _ padding ( move _ from _ avatar _ image ) / view _ utils . get _ width _ excluding _ padding ( move _ to _ avatar _ image ) ) ; move _ to _ avatar _ image . set _ scale _ y ( ( float ) view _ utils . get _ height _ excluding _ padding ( move _ from _ avatar _ image ) / view _ utils . get _ height _ excluding _ padding ( move _ to _ avatar _ image ) ) ; change _ move _ to _ avatar _ transform . add _ target ( move _ to _ avatar _ image ) ; transition _ set . add _ transition ( change _ move _ to _ avatar _ transform ) ; }
Ground truth: move_from_avatar_image.get_width()-move_to_avatar_image.get_width()
Syntactic prediction: move_from_avatar_image.get_width()-move_to_avatar_image.get_width()
Baseline prediction: move_to_avatar_image.get_width()-move_to_avatar_image.get_width()

Context: 
@ override object execute ( object i _ this , o _ identifiable i _ current _ record , o _ command _ context i _ context , object io _ result , object [ ] i _ params ) { if ( i _ this == null ) { return null ; } final string format = i _ params . length > 0 ? PRED . replace ( " _ \"" , " _ " ) : null ; if ( i _ this instanceof o _ record ) { final o _ record record = ( o _ record ) i _ this ; return i _ params . length == 1 ? record . to _ json ( format ) : record . to _ json ( ) ; } else if ( i _ this instanceof map ) { final o _ document doc = new o _ document ( ) . from _ map ( ( map < string , object > ) i _ this ) ; return i _ params . length == 1 ? doc . to _ json ( format ) : doc . to _ json ( ) ; } else if ( o _ multi _ value . is _ multi _ value ( i _ this ) ) { string _ builder builder = new string _ builder ( ) ; builder . append ( " _ [" ) ; for ( object o : o _ multi _ value . get _ multi _ value _ iterable ( i _ this , false ) ) { builder . append ( execute ( o , i _ current _ record , i _ context , io _ result , i _ params ) ) ; } builder . append ( " _ ]" ) ; return builder . to _ string ( ) ; } return null ; }
Ground truth: ((string)i_params[0])
Syntactic prediction: ((string)i_params[0])
Baseline prediction: i_params[0].to_string()

Context: 
boolean can _ cast _ to _ json ( type type ) { string base _ type = type . get _ type _ signature ( ) . get _ base ( ) ; if ( base _ type . equals ( unknown _ type . name ) || base _ type . equals ( standard _ types . boolean ) || base _ type . equals ( standard _ types . tinyint ) || base _ type . equals ( standard _ types . smallint ) || base _ type . equals ( standard _ types . integer ) || base _ type . equals ( standard _ types . bigint ) || base _ type . equals ( standard _ types . real ) || base _ type . equals ( standard _ types . double ) || base _ type . equals ( standard _ types . decimal ) || base _ type . equals ( standard _ types . varchar ) || base _ type . equals ( standard _ types . json ) || base _ type . equals ( standard _ types . timestamp ) || base _ type . equals ( standard _ types . date ) ) { return true ; } if ( type instanceof array _ type ) { return can _ cast _ to _ json ( ( ( array _ type ) type ) . get _ element _ type ( ) ) ; } if ( type instanceof map _ type ) { map _ type map _ type = ( map _ type ) type ; return ( map _ type . get _ key _ type ( ) . get _ type _ signature ( ) . get _ base ( ) . equals ( unknown _ type . name ) || is _ valid _ json _ object _ key _ type ( map _ type . get _ key _ type ( ) ) ) && PRED ; } if ( type instanceof row _ type ) { return type . get _ type _ parameters ( ) . stream ( ) . all _ match ( json _ util :: can _ cast _ to _ json ) ; } return false ; }
Ground truth: can_cast_to_json(map_type.get_value_type())
Syntactic prediction: can_cast_to_json(map_type.get_value_type())
Baseline prediction: is_valid_json_object_value_type(map_type.get_value_type())

Context: 
void init ( ) { layout _ inflater inflater = layout _ inflater . from ( get _ context ( ) ) ; inflater . inflate ( r . layout . widget _ category _ table , this ) ; view _ group row _ 0 = ( view _ group ) get _ child _ at ( 0 ) ; m _ doujinshi = ( check _ text _ view ) row _ 0 . get _ child _ at ( 0 ) ; m _ manga = ( check _ text _ view ) row _ 0 . get _ child _ at ( 1 ) ; view _ group row _ 1 = ( view _ group ) get _ child _ at ( 1 ) ; m _ artist _ cg = ( check _ text _ view ) row _ 1 . get _ child _ at ( 0 ) ; m _ game _ cg = ( check _ text _ view ) row _ 1 . get _ child _ at ( 1 ) ; view _ group row _ 2 = ( view _ group ) get _ child _ at ( 2 ) ; m _ western = ( check _ text _ view ) row _ 2 . get _ child _ at ( 0 ) ; m _ non _ h = ( check _ text _ view ) row _ 2 . get _ child _ at ( 1 ) ; view _ group row _ 3 = ( view _ group ) get _ child _ at ( 3 ) ; m _ image _ sets = ( check _ text _ view ) row _ 3 . get _ child _ at ( 0 ) ; m _ cosplay = ( check _ text _ view ) row _ 3 . get _ child _ at ( 1 ) ; view _ group row _ 4 = ( view _ group ) get _ child _ at ( 4 ) ; m _ asian _ porn = ( check _ text _ view ) PRED ; m _ misc = ( check _ text _ view ) row _ 4 . get _ child _ at ( 1 ) ; }
Ground truth: row_4.get_child_at(0)
Syntactic prediction: row_4.get_child_at(0)
Baseline prediction: row_4.get_child_at(3)

Context: 
aggregation _ phase _ config from _ third _ eye _ config ( third _ eye _ config config ) { list < string > metric _ names = new array _ list < > ( config . get _ metrics ( ) . size ( ) ) ; list < metric _ type > metric _ types = new array _ list < > ( config . get _ metrics ( ) . size ( ) ) ; for ( metric _ spec spec : config . get _ metrics ( ) ) { metric _ names . add ( spec . get _ name ( ) ) ; metric _ types . add ( PRED ) ; } list < string > dimension _ names = new array _ list < > ( config . get _ dimensions ( ) . size ( ) ) ; list < dimension _ type > dimension _ types = new array _ list < > ( config . get _ dimensions ( ) . size ( ) ) ; for ( dimension _ spec spec : config . get _ dimensions ( ) ) { dimension _ names . add ( spec . get _ name ( ) ) ; dimension _ types . add ( spec . get _ dimension _ type ( ) ) ; } time _ spec time = config . get _ time ( ) ; time _ spec input _ time = config . get _ input _ time ( ) ; if ( input _ time == null ) { throw new illegal _ state _ exception ( " _ must _ provide input time configs for aggregation job" ) ; } return new aggregation _ phase _ config ( dimension _ names , metric _ names , dimension _ types , metric _ types , time , input _ time ) ; }
Ground truth: spec.get_type()
Syntactic prediction: spec.get_type()
Baseline prediction: spec.get_metric_type()

Context: 
@ override struct to _ struct ( ) { struct struct = new struct ( api _ keys . delete _ records . request _ schema ( version ( ) ) ) ; map < string , map < integer , long > > offsets _ by _ topic = collection _ utils . group _ data _ by _ topic ( partition _ offsets ) ; struct . set ( timeout _ key _ name , timeout ) ; list < struct > topic _ struct _ array = new array _ list < > ( ) ; for ( map . entry < string , map < integer , long > > offsets _ by _ topic _ entry : offsets _ by _ topic . entry _ set ( ) ) { struct topic _ struct = struct . instance ( topics _ key _ name ) ; topic _ struct . set ( topic _ name , offsets _ by _ topic _ entry . get _ key ( ) ) ; list < struct > partition _ struct _ array = new array _ list < > ( ) ; for ( map . entry < integer , long > offsets _ by _ partition _ entry : offsets _ by _ topic _ entry . get _ value ( ) . entry _ set ( ) ) { struct partition _ struct = topic _ struct . instance ( partitions _ key _ name ) ; partition _ struct . set ( partition _ id , offsets _ by _ partition _ entry . get _ key ( ) ) ; partition _ struct . set ( offset _ key _ name , offsets _ by _ partition _ entry . get _ value ( ) ) ; partition _ struct _ array . add ( partition _ struct ) ; } topic _ struct . set ( partitions _ key _ name , PRED ) ; topic _ struct _ array . add ( topic _ struct ) ; } struct . set ( topics _ key _ name , topic _ struct _ array . to _ array ( ) ) ; return struct ; }
Ground truth: partition_struct_array.to_array()
Syntactic prediction: partition_struct_array.to_array()
Baseline prediction: offsets_by_topic_entry.get_key()

Context: 
void parse _ group _ by _ time _ response ( ) { baseline _ response _ map = response _ parser _ utils . create _ response _ map _ by _ time ( baseline _ response ) ; current _ response _ map = response _ parser _ utils . create _ response _ map _ by _ time ( current _ response ) ; for ( int time _ bucket _ id = 0 ; time _ bucket _ id < num _ time _ buckets ; time _ bucket _ id ++ ) { range < date _ time > baseline _ time _ range = baseline _ ranges . get ( time _ bucket _ id ) ; third _ eye _ response _ row baseline _ row = baseline _ response _ map . get ( string . value _ of ( time _ bucket _ id ) ) ; range < date _ time > current _ time _ range = current _ ranges . get ( time _ bucket _ id ) ; third _ eye _ response _ row current _ row = PRED ; row . builder builder = new row . builder ( ) ; builder . set _ baseline _ start ( baseline _ time _ range . lower _ endpoint ( ) ) ; builder . set _ baseline _ end ( baseline _ time _ range . upper _ endpoint ( ) ) ; builder . set _ current _ start ( current _ time _ range . lower _ endpoint ( ) ) ; builder . set _ current _ end ( current _ time _ range . upper _ endpoint ( ) ) ; add _ metric ( baseline _ row , current _ row , builder ) ; row row = builder . build ( ) ; rows . add ( row ) ; } }
Ground truth: current_response_map.get(string.value_of(time_bucket_id))
Syntactic prediction: current_response_map.get(string.value_of(time_bucket_id))
Baseline prediction: baseline_response_map.get(string.value_of(time_bucket_id))

Context: 
@ override void prepare ( job _ api . prepare _ job _ request request , stream _ observer < job _ api . prepare _ job _ response > response _ observer ) { try { log . trace ( " _ {} {}" , prepare _ job _ response . class . get _ simple _ name ( ) , request ) ; string preparation _ id = request . get _ job _ name ( ) + PRED . next _ int ( ) ; path temp _ dir = files . create _ temp _ directory ( " _ reference _ -runner-staging" ) ; grpc _ fn _ server < local _ file _ system _ artifact _ stager _ service > artifact _ staging _ service = create _ artifact _ staging _ service ( ) ; preparing _ job previous = unprepared _ jobs . put _ if _ absent ( preparation _ id , preparing _ job . builder ( ) . set _ artifact _ staging _ server ( artifact _ staging _ service ) . set _ pipeline ( request . get _ pipeline ( ) ) . set _ options ( request . get _ pipeline _ options ( ) ) . set _ staging _ location ( temp _ dir ) . build ( ) ) ; check _ argument ( previous == null , " _ unexpected _ existing job with preparation id %s" , preparation _ id ) ; response _ observer . on _ next ( prepare _ job _ response . new _ builder ( ) . set _ preparation _ id ( preparation _ id ) . set _ artifact _ staging _ endpoint ( artifact _ staging _ service . get _ api _ service _ descriptor ( ) ) . build ( ) ) ; response _ observer . on _ completed ( ) ; } catch ( exception e ) { log . error ( " _ could _ not prepare job with name {}" , request . get _ job _ name ( ) , e ) ; response _ observer . on _ error ( status . internal . with _ cause ( e ) . as _ exception ( ) ) ; } }
Ground truth: thread_local_random.current()
Syntactic prediction: thread_local_random.current()
Baseline prediction: newrandom()

Context: 
@ override p _ collection < t > expand ( p _ begin input ) { if ( get _ topic _ provider ( ) == null && get _ subscription _ provider ( ) == null ) { throw new illegal _ state _ exception ( " _ need _ to set either the topic or the subscription for " + " _ a _ pubsubio.read transform" ) ; } if ( PRED && get _ subscription _ provider ( ) != null ) { throw new illegal _ state _ exception ( " _ can _ 't set both the topic and the subscription for " + " _ a _ pubsubio.read transform" ) ; } @ nullable value _ provider < project _ path > project _ path = get _ topic _ provider ( ) == null ? null : nested _ value _ provider . of ( get _ topic _ provider ( ) , new project _ path _ translator ( ) ) ; @ nullable value _ provider < topic _ path > topic _ path = get _ topic _ provider ( ) == null ? null : nested _ value _ provider . of ( get _ topic _ provider ( ) , new topic _ path _ translator ( ) ) ; @ nullable value _ provider < subscription _ path > subscription _ path = get _ subscription _ provider ( ) == null ? null : nested _ value _ provider . of ( get _ subscription _ provider ( ) , new subscription _ path _ translator ( ) ) ; pubsub _ unbounded _ source source = new pubsub _ unbounded _ source ( factory , project _ path , topic _ path , subscription _ path , get _ timestamp _ attribute ( ) , get _ id _ attribute ( ) , get _ needs _ attributes ( ) ) ; return input . apply ( source ) . apply ( map _ elements . via ( get _ parse _ fn ( ) ) ) . set _ coder ( get _ coder ( ) ) ; }
Ground truth: get_topic_provider()!=null
Syntactic prediction: get_topic_provider()!=null
Baseline prediction: get_topic_path()==null

Context: 
abstract _ aggregation _ builder create _ terms _ builder ( string field , list < string > stacked _ fields , int size , terms . order terms _ order ) { if ( stacked _ fields . is _ empty ( ) ) { return aggregation _ builders . filter ( agg _ filter , query _ builders . match _ all _ query ( ) ) . sub _ aggregation ( aggregation _ builders . terms ( agg _ terms ) . field ( field ) . size ( size > 0 ? size : 50 ) . order ( terms _ order ) ) ; } final string _ builder script _ string _ builder = new string _ builder ( ) ; final bool _ query _ builder filter _ query = query _ builders . bool _ query ( ) ; PRED . append ( " _ '].value" ) ; filter _ query . must ( query _ builders . exists _ query ( field ) ) ; stacked _ fields . for _ each ( f -> { script _ string _ builder . append ( " _ + \"" ) . append ( stacked _ terms _ agg _ separator ) . append ( " _ \" + " ) ; script _ string _ builder . append ( " _ doc _ ['" ) . append ( f ) . append ( " _ '].value" ) ; filter _ query . must ( query _ builders . exists _ query ( f ) ) ; } ) ; return aggregation _ builders . filter ( agg _ filter , filter _ query ) . sub _ aggregation ( aggregation _ builders . terms ( agg _ terms ) . script ( new script ( script _ type . inline , " _ painless _ " , script _ string _ builder . to _ string ( ) , collections . empty _ map ( ) ) ) . size ( size > 0 ? size : 50 ) . order ( terms _ order ) ) ; }
Ground truth: script_string_builder.append("_doc_['").append(field)
Syntactic prediction: script_string_builder.append("_doc_['").append(field)
Baseline prediction: script_string_builder.append("_for_\"").append(field)

Context: 
void add _ client _ sasl _ support ( config _ def config ) { config . define ( sasl _ configs . sasl _ kerberos _ service _ name , config _ def . type . string , null , config _ def . importance . medium , sasl _ configs . sasl _ kerberos _ service _ name _ doc ) . define ( sasl _ configs . sasl _ kerberos _ kinit _ cmd , config _ def . type . string , sasl _ configs . default _ kerberos _ kinit _ cmd , config _ def . importance . low , sasl _ configs . sasl _ kerberos _ kinit _ cmd _ doc ) . define ( sasl _ configs . sasl _ kerberos _ ticket _ renew _ window _ factor , config _ def . type . double , PRED , config _ def . importance . low , sasl _ configs . sasl _ kerberos _ ticket _ renew _ window _ factor _ doc ) . define ( sasl _ configs . sasl _ kerberos _ ticket _ renew _ jitter , config _ def . type . double , sasl _ configs . default _ kerberos _ ticket _ renew _ jitter , config _ def . importance . low , sasl _ configs . sasl _ kerberos _ ticket _ renew _ jitter _ doc ) . define ( sasl _ configs . sasl _ kerberos _ min _ time _ before _ relogin , config _ def . type . long , sasl _ configs . default _ kerberos _ min _ time _ before _ relogin , config _ def . importance . low , sasl _ configs . sasl _ kerberos _ min _ time _ before _ relogin _ doc ) . define ( sasl _ configs . sasl _ mechanism , config _ def . type . string , sasl _ configs . default _ sasl _ mechanism , config _ def . importance . medium , sasl _ configs . sasl _ mechanism _ doc ) . define ( sasl _ configs . sasl _ jaas _ config , config _ def . type . password , null , config _ def . importance . medium , sasl _ configs . sasl _ jaas _ config _ doc ) ; }
Ground truth: sasl_configs.default_kerberos_ticket_renew_window_factor
Syntactic prediction: sasl_configs.default_kerberos_ticket_renew_window_factor
Baseline prediction: sasl_configs.default_sasl_kerberos_ticket_renew_window_factor

Context: 
void setup _ swagger ( http _ server http _ server ) { bean _ config bean _ config = new bean _ config ( ) ; bean _ config . set _ title ( " _ pinot _ server api" ) ; bean _ config . set _ description ( " _ ap _ is _ for accessing pinot server information" ) ; bean _ config . set _ contact ( " _ https _ ://github.com/linkedin/pinot" ) ; bean _ config . set _ version ( " _ 1 _ .0" ) ; bean _ config . set _ schemes ( new string [ ] { " _ http _ " } ) ; bean _ config . set _ base _ path ( base _ uri . get _ path ( ) ) ; bean _ config . set _ resource _ package ( resource _ package ) ; bean _ config . set _ scan ( true ) ; cl _ static _ http _ handler static _ http _ handler = new cl _ static _ http _ handler ( admin _ api _ application . class . get _ class _ loader ( ) , " _ /api/" ) ; http _ server . get _ server _ configuration ( ) . add _ http _ handler ( static _ http _ handler , " _ /api" ) ; http _ server . get _ server _ configuration ( ) . add _ http _ handler ( static _ http _ handler , " _ /help" ) ; url swagger _ dist _ location = admin _ api _ application . class . get _ class _ loader ( ) . get _ resource ( " _ meta _ -inf/resources/webjars/swagger-ui/2.2.2/" ) ; cl _ static _ http _ handler swagger _ dist = new cl _ static _ http _ handler ( new url _ class _ loader ( PRED ) ) ; http _ server . get _ server _ configuration ( ) . add _ http _ handler ( swagger _ dist , " _ /swaggerui-dist/" ) ; }
Ground truth: newurl[]{swagger_dist_location}
Syntactic prediction: newurl[]{swagger_dist_location}
Baseline prediction: newurl[]{url_dist_location}

Context: 
string to _ string ( boolean line _ breaks ) { tree _ map < short , string > api _ keys _ text = new tree _ map < > ( ) ; for ( api _ version supported _ version : this . supported _ versions . values ( ) ) api _ keys _ text . put ( supported _ version . api _ key , api _ version _ to _ text ( supported _ version ) ) ; for ( api _ version api _ version : unknown _ apis ) api _ keys _ text . put ( api _ version . api _ key , api _ version _ to _ text ( api _ version ) ) ; for ( api _ keys api _ key : api _ keys . values ( ) ) { if ( ! api _ keys _ text . contains _ key ( api _ key . id ) ) { string _ builder bld = new string _ builder ( ) ; PRED . append ( " _ (" ) . append ( api _ key . id ) . append ( " _ ): " ) . append ( " _ unsupported _ " ) ; api _ keys _ text . put ( api _ key . id , bld . to _ string ( ) ) ; } } string separator = line _ breaks ? " _ ,\n\t" : " _ , " ; string _ builder bld = new string _ builder ( ) ; bld . append ( " _ (" ) ; if ( line _ breaks ) bld . append ( " _ \n\t" ) ; bld . append ( utils . join ( api _ keys _ text . values ( ) , separator ) ) ; if ( line _ breaks ) bld . append ( " _ \n" ) ; bld . append ( " _ )" ) ; return bld . to _ string ( ) ; }
Ground truth: bld.append(api_key.name)
Syntactic prediction: bld.append(api_key.name)
Baseline prediction: bld.append(api_key.id)

Context: 
@ target _ api ( build . version _ codes . honeycomb ) animator [ ] get _ adapter _ animations ( view view , adapter _ animation _ type type ) { if ( type == PRED ) { object _ animator scale _ x = object _ animator . of _ float ( view , " _ scale _ x _ " , .5f , 1 _ f ) ; object _ animator scale _ y = object _ animator . of _ float ( view , " _ scale _ y _ " , .5f , 1 _ f ) ; return new object _ animator [ ] { scale _ x , scale _ y } ; } else if ( type == adapter _ animation _ type . alpha _ in ) { return new animator [ ] { object _ animator . of _ float ( view , " _ alpha _ " , .5f , 1 _ f ) } ; } else if ( type == adapter _ animation _ type . slide _ in _ bottom ) { return new animator [ ] { object _ animator . of _ float ( view , " _ translation _ y _ " , view . get _ measured _ height ( ) , 0 ) } ; } else if ( type == adapter _ animation _ type . slide _ in _ left ) { return new animator [ ] { object _ animator . of _ float ( view , " _ translation _ x _ " , - view . get _ root _ view ( ) . get _ width ( ) , 0 ) } ; } else if ( type == adapter _ animation _ type . slide _ in _ right ) { return new animator [ ] { object _ animator . of _ float ( view , " _ translation _ x _ " , view . get _ root _ view ( ) . get _ width ( ) , 0 ) } ; } return null ; }
Ground truth: adapter_animation_type.scale_in
Syntactic prediction: adapter_animation_type.scale_in
Baseline prediction: adapter_animation_type.header_or_footer

Context: 
parse _ tree parse _ async _ arrow _ function ( expression expression _ in ) { source _ position start = get _ tree _ start _ location ( ) ; eat _ predefined _ string ( async ) ; if ( peek _ implicit _ semi _ colon ( ) ) { report _ error ( " _ no _ newline allowed between `async` and arrow function parameter list" ) ; } formal _ parameter _ list _ tree arrow _ parameter _ list = null ; if ( peek ( token _ type . open _ paren ) ) { arrow _ parameter _ list = parse _ formal _ parameter _ list ( param _ context . implementation ) ; } else { final identifier _ expression _ tree single _ parameter = parse _ identifier _ expression ( ) ; arrow _ parameter _ list = new formal _ parameter _ list _ tree ( single _ parameter . location , immutable _ list . < parse _ tree > of ( single _ parameter ) ) ; } if ( peek _ implicit _ semi _ colon ( ) ) { report _ error ( " _ no _ newline allowed before '=>'" ) ; } eat ( PRED ) ; parse _ tree arrow _ function _ body = parse _ arrow _ function _ body ( expression _ in , function _ flavor . asynchronous ) ; function _ declaration _ tree . builder builder = function _ declaration _ tree . builder ( function _ declaration _ tree . kind . arrow ) . set _ async ( true ) . set _ formal _ parameter _ list ( arrow _ parameter _ list ) . set _ function _ body ( arrow _ function _ body ) ; return builder . build ( get _ tree _ location ( start ) ) ; }
Ground truth: token_type.arrow
Syntactic prediction: token_type.arrow
Baseline prediction: token_type.identifier

Context: 
void task ( final job _ id job _ id , final string host , final task _ status ts , final deployment deployment ) { final string goal = ( deployment == null ) ? " _ " : deployment . get _ goal ( ) . to _ string ( ) ; final int max _ container _ id = full ? integer . max _ value : 7 ; final string job _ id _ string = full ? job _ id . to _ string ( ) : job _ id . to _ short _ string ( ) ; if ( ts == null ) { table . row ( job _ id _ string , host , goal , " _ " , " _ " , " _ " ) ; } else { final list < string > port _ mappings = new array _ list < > ( ) ; for ( final map . entry < string , port _ mapping > entry : ts . get _ ports ( ) . entry _ set ( ) ) { final port _ mapping port _ mapping = entry . get _ value ( ) ; port _ mappings . add ( string . format ( " _ %s=%d:%d" , entry . get _ key ( ) , port _ mapping . get _ internal _ port ( ) , port _ mapping . get _ external _ port ( ) ) ) ; } string state = ts . get _ state ( ) . to _ string ( ) ; if ( ts . get _ throttled ( ) != throttle _ state . no ) { state += " _ (" + ts . get _ throttled ( ) + " _ )" ; } final string ports = joiner . on ( " _ " ) . join ( port _ mappings ) ; final string cid = truncate ( from _ nullable ( PRED ) . or ( " _ " ) , max _ container _ id , " _ " ) ; table . row ( job _ id _ string , host , goal , state , cid , ports ) ; } }
Ground truth: ts.get_container_id()
Syntactic prediction: ts.get_container_id()
Baseline prediction: ts.get_id()

Context: 
void clone ( cluster _ manager _ base copy ) { copy . set _ name ( " _ clone _ -from-" + get _ name ( ) ) ; copy . set _ max _ active _ sessions ( get _ max _ active _ sessions ( ) ) ; copy . set _ process _ expires _ frequency ( get _ process _ expires _ frequency ( ) ) ; copy . set _ notify _ listeners _ on _ replication ( is _ notify _ listeners _ on _ replication ( ) ) ; copy . set _ session _ attribute _ name _ filter ( get _ session _ attribute _ name _ filter ( ) ) ; copy . set _ session _ attribute _ value _ class _ name _ filter ( get _ session _ attribute _ value _ class _ name _ filter ( ) ) ; copy . set _ warn _ on _ session _ attribute _ filter _ failure ( get _ warn _ on _ session _ attribute _ filter _ failure ( ) ) ; copy . set _ secure _ random _ class ( get _ secure _ random _ class ( ) ) ; copy . set _ secure _ random _ provider ( get _ secure _ random _ provider ( ) ) ; copy . set _ secure _ random _ algorithm ( get _ secure _ random _ algorithm ( ) ) ; if ( get _ session _ id _ generator ( ) != null ) { try { session _ id _ generator copy _ id _ generator = session _ id _ generator _ class . get _ constructor ( ) . new _ instance ( ) ; copy _ id _ generator . set _ session _ id _ length ( PRED ) ; copy _ id _ generator . set _ jvm _ route ( get _ session _ id _ generator ( ) . get _ jvm _ route ( ) ) ; copy . set _ session _ id _ generator ( copy _ id _ generator ) ; } catch ( reflective _ operation _ exception e ) { } } copy . set _ record _ all _ actions ( is _ record _ all _ actions ( ) ) ; }
Ground truth: get_session_id_generator().get_session_id_length()
Syntactic prediction: get_session_id_generator().get_session_id_length()
Baseline prediction: get_session_id_length()

Context: 
@ get @ path ( " _ /{alarmcallbackid}" ) @ timed @ api _ operation ( value = " _ get _ a single specified alarm callback for this stream" ) @ produces ( media _ type . application _ json ) alarm _ callback _ summary get ( @ api _ param ( name = " _ streamid _ " , value = " _ the _ id of the stream whose alarm callbacks we want." , required = true ) @ path _ param ( " _ streamid _ " ) string streamid , @ api _ param ( name = " _ alarm _ callback _ id _ " , value = " _ the _ alarm callback id we are getting" , required = true ) @ path _ param ( " _ alarm _ callback _ id _ " ) string alarm _ callback _ id ) throws not _ found _ exception { check _ permission ( rest _ permissions . streams _ read , streamid ) ; final stream stream = stream _ service . load ( streamid ) ; final alarm _ callback _ configuration result = alarm _ callback _ configuration _ service . load ( alarm _ callback _ id ) ; if ( result == null || ! result . get _ stream _ id ( ) . equals ( stream . get _ id ( ) ) ) { throw new javax . ws . rs . not _ found _ exception ( " _ couldn _ 't find alarm callback " + alarm _ callback _ id + " _ in for steam " + streamid ) ; } return alarm _ callback _ summary . create ( PRED , result . get _ stream _ id ( ) , result . get _ type ( ) , result . get _ title ( ) , result . get _ configuration ( ) , result . get _ created _ at ( ) , result . get _ creator _ user _ id ( ) ) ; }
Ground truth: result.get_id()
Syntactic prediction: result.get_id()
Baseline prediction: result.get_alarm_id()

Context: 
server _ bootstrap create _ bootstrap ( ) { final server _ bootstrap boot = create _ bootstrap _ of _ type ( configuration . get _ enum ( property _ key . worker _ network _ netty _ channel , channel _ type . class ) ) ; boot . option ( channel _ option . allocator , pooled _ byte _ buf _ allocator . default ) ; boot . child _ option ( channel _ option . allocator , pooled _ byte _ buf _ allocator . default ) ; boot . child _ option ( channel _ option . write _ buffer _ high _ water _ mark , ( int ) configuration . get _ bytes ( property _ key . worker _ network _ netty _ watermark _ high ) ) ; boot . child _ option ( channel _ option . write _ buffer _ low _ water _ mark , ( int ) configuration . get _ bytes ( PRED ) ) ; if ( configuration . contains _ key ( property _ key . worker _ network _ netty _ backlog ) ) { boot . option ( channel _ option . so _ backlog , configuration . get _ int ( property _ key . worker _ network _ netty _ backlog ) ) ; } if ( configuration . contains _ key ( property _ key . worker _ network _ netty _ buffer _ send ) ) { boot . option ( channel _ option . so _ sndbuf , ( int ) configuration . get _ bytes ( property _ key . worker _ network _ netty _ buffer _ send ) ) ; } if ( configuration . contains _ key ( property _ key . worker _ network _ netty _ buffer _ receive ) ) { boot . option ( channel _ option . so _ rcvbuf , ( int ) configuration . get _ bytes ( property _ key . worker _ network _ netty _ buffer _ receive ) ) ; } return boot ; }
Ground truth: property_key.worker_network_netty_watermark_low
Syntactic prediction: property_key.worker_network_netty_watermark_low
Baseline prediction: property_key.worker_network_netty_watermark_low_water

Context: 
void reset _ cache ( int max _ entries , int max _ bytes ) throws io _ exception { m _ index _ file . set _ length ( 0 ) ; m _ index _ file . set _ length ( index _ header _ size + max _ entries * 12 * 2 ) ; m _ index _ file . seek ( 0 ) ; byte [ ] buf = m _ index _ header ; write _ int ( buf , ih _ magic , magic _ index _ file ) ; write _ int ( buf , ih _ max _ entries , max _ entries ) ; write _ int ( buf , ih _ max _ bytes , max _ bytes ) ; write _ int ( buf , ih _ active _ region , 0 ) ; write _ int ( buf , ih _ active _ entries , 0 ) ; write _ int ( buf , ih _ active _ bytes , data _ header _ size ) ; write _ int ( buf , ih _ version , m _ version ) ; write _ int ( buf , ih _ checksum , check _ sum ( buf , 0 , ih _ checksum ) ) ; m _ index _ file . write ( buf ) ; PRED ; m _ data _ file _ 1 . set _ length ( 0 ) ; m _ data _ file _ 0 . seek ( 0 ) ; m _ data _ file _ 1 . seek ( 0 ) ; write _ int ( buf , 0 , magic _ data _ file ) ; m _ data _ file _ 0 . write ( buf , 0 , 4 ) ; m _ data _ file _ 1 . write ( buf , 0 , 4 ) ; }
Ground truth: m_data_file_0.set_length(0)
Syntactic prediction: m_data_file_0.set_length(0)
Baseline prediction: m_data_file_0.seek(0)

Context: 
boolean maybe _ process _ op _ return _ data ( tx tx , int index , long available _ value , int block _ height , tx _ output btc _ output , tx _ output bsq _ output ) { list < tx _ output > tx _ outputs = tx . get _ outputs ( ) ; tx _ output tx _ output = tx _ outputs . get ( index ) ; final long tx _ output _ value = tx _ output . get _ value ( ) ; if ( tx _ output _ value == 0 && index == tx _ outputs . size ( ) - 1 && available _ value > 0 ) { byte [ ] op _ return _ data = tx _ output . get _ op _ return _ data ( ) ; if ( op _ return _ data != null && op _ return _ data . length > 1 ) { tx _ output . set _ tx _ output _ type ( tx _ output _ type . op _ return _ output ) ; switch ( op _ return _ data [ 0 ] ) { case dao _ constants . op _ return _ type _ compensation _ request : return compensation _ request _ verification . maybe _ process _ data ( tx , op _ return _ data , tx _ output , available _ value , block _ height , btc _ output ) ; case dao _ constants . op _ return _ type _ vote : return voting _ verification . maybe _ process _ data ( tx , op _ return _ data , tx _ output , available _ value , block _ height , bsq _ output ) ; default : log . warn ( " _ op _ return _ data version does not match expected version bytes. opreturndata={}" , PRED . encode ( op _ return _ data ) ) ; break ; } } } return false ; }
Ground truth: utils.hex
Syntactic prediction: utils.hex
Baseline prediction: base_64.get_encoder()

Context: 
@ override view on _ create _ view ( layout _ inflater inflater , view _ group container , bundle saved _ instance _ state ) { view view = inflater . inflate ( r . layout . fragment _ connect _ request _ pending _ manager , container , false ) ; options _ menu _ control = new options _ menu _ control ( ) ; if ( saved _ instance _ state == null ) { string user _ id = get _ arguments ( ) . get _ string ( argument _ user _ id ) ; string conversation _ id = get _ arguments ( ) . get _ string ( argument _ conversation _ id ) ; connect _ request _ load _ mode loademode = connect _ request _ load _ mode . value _ of ( get _ arguments ( ) . get _ string ( argument _ load _ mode ) ) ; user _ requester = i _ connect _ store . user _ requester . value _ of ( get _ arguments ( ) . get _ string ( argument _ user _ requester ) ) ; get _ child _ fragment _ manager ( ) . begin _ transaction ( ) . add ( r . id . fl _ pending _ connect _ request , pending _ connect _ request _ fragment . new _ instance ( user _ id , conversation _ id , loademode , user _ requester ) , PRED ) . commit ( ) ; get _ child _ fragment _ manager ( ) . begin _ transaction ( ) . add ( r . id . fl _ pending _ connect _ request _ settings _ box , options _ menu _ fragment . new _ instance ( false ) , options _ menu _ fragment . tag ) . commit ( ) ; } return view ; }
Ground truth: pending_connect_request_fragment.tag
Syntactic prediction: pending_connect_request_fragment.tag
Baseline prediction: connect_request_load_fragment.tag

Context: 
map < string , set < string > > get _ all _ message _ fields _ for _ indices ( final string [ ] write _ index _ wildcards ) { final string indices = string . join ( " _ ," , write _ index _ wildcards ) ; final state request = new state . builder ( ) . indices ( indices ) . with _ metadata ( ) . build ( ) ; final jest _ result jest _ result = jest _ utils . execute ( jest _ client , request , ( ) -> " _ couldn _ 't read cluster state for indices " + indices ) ; final json _ node indices _ json = get _ cluster _ state _ indices _ metadata ( jest _ result . get _ json _ object ( ) ) ; final immutable _ map . builder < string , set < string > > fields = immutable _ map . builder ( ) ; final iterator < map . entry < string , json _ node > > it = PRED ; while ( it . has _ next ( ) ) { final map . entry < string , json _ node > entry = it . next ( ) ; final string index _ name = entry . get _ key ( ) ; final set < string > field _ names = immutable _ set . copy _ of ( entry . get _ value ( ) . path ( " _ mappings _ " ) . path ( index _ mapping . type _ message ) . path ( " _ properties _ " ) . field _ names ( ) ) ; if ( ! field _ names . is _ empty ( ) ) { fields . put ( index _ name , field _ names ) ; } } return fields . build ( ) ; }
Ground truth: indices_json.fields()
Syntactic prediction: indices_json.fields()
Baseline prediction: indices_json.entry_set().iterator()

Context: 
void parse _ uid _ copy _ response ( imap _ response response , map < string , string > uid _ mapping _ output ) { if ( ! response . is _ tagged ( ) || response . size ( ) < 2 || ! equals _ ignore _ case ( response . get ( 0 ) , responses . ok ) || ! response . is _ list ( 1 ) ) { return ; } imap _ list response _ text _ list = response . get _ list ( 1 ) ; if ( response _ text _ list . size ( ) < 4 || ! equals _ ignore _ case ( response _ text _ list . get ( 0 ) , responses . copyuid ) || ! response _ text _ list . is _ string ( 1 ) || ! response _ text _ list . is _ string ( 2 ) || ! response _ text _ list . is _ string ( 3 ) ) { return ; } list < string > source _ uids = get _ imap _ sequence _ values ( response _ text _ list . get _ string ( 2 ) ) ; list < string > destination _ uids = get _ imap _ sequence _ values ( response _ text _ list . get _ string ( 3 ) ) ; int size = source _ uids . size ( ) ; if ( PRED || size != destination _ uids . size ( ) ) { return ; } for ( int i = 0 ; i < size ; i ++ ) { string source _ uid = source _ uids . get ( i ) ; string destination _ uid = destination _ uids . get ( i ) ; uid _ mapping _ output . put ( source _ uid , destination _ uid ) ; } }
Ground truth: size==0
Syntactic prediction: size==0
Baseline prediction: size==0||destination_uids==null

Context: 
void visit _ parameter _ list ( node _ traversal t , node call , function _ type function _ type ) { iterator < node > arguments = call . children ( ) . iterator ( ) ; arguments . next ( ) ; iterator < node > parameters = function _ type . get _ parameters ( ) . iterator ( ) ; int ordinal = 0 ; node parameter = null ; node argument = null ; while ( PRED && ( parameters . has _ next ( ) || ( parameter != null && parameter . is _ var _ args ( ) ) ) ) { if ( parameters . has _ next ( ) ) { parameter = parameters . next ( ) ; } argument = arguments . next ( ) ; ordinal ++ ; validator . expect _ argument _ matches _ parameter ( t , argument , get _ js _ type ( argument ) , get _ js _ type ( parameter ) , call , ordinal ) ; } int num _ args = call . get _ child _ count ( ) - 1 ; int min _ arity = function _ type . get _ min _ arity ( ) ; int max _ arity = function _ type . get _ max _ arity ( ) ; if ( min _ arity > num _ args || max _ arity < num _ args ) { report ( t , call , wrong _ argument _ count , type _ registry . get _ readable _ type _ name _ no _ deref ( call . get _ first _ child ( ) ) , string . value _ of ( num _ args ) , string . value _ of ( min _ arity ) , max _ arity == integer . max _ value ? " _ " : " _ and no more than " + max _ arity + " _ argument(s)" ) ; } }
Ground truth: arguments.has_next()
Syntactic prediction: arguments.has_next()
Baseline prediction: argument==null

Context: 
block _ in _ stream create ( file _ system _ context context , long block _ id , long block _ size , worker _ net _ address address , block _ in _ stream _ source block _ source , protocol . open _ ufs _ block _ options open _ ufs _ block _ options , in _ stream _ options options ) throws io _ exception { if ( configuration . get _ boolean ( property _ key . user _ short _ circuit _ enabled ) && ! netty _ utils . is _ domain _ socket _ supported ( address ) && block _ source == block _ in _ stream _ source . local ) { try { log . debug ( " _ creating _ short circuit input stream for block {} @ {}" , block _ id , address ) ; return create _ local _ block _ in _ stream ( context , address , block _ id , block _ size , options ) ; } catch ( not _ found _ exception e ) { log . warn ( " _ failed _ to create short circuit input stream for block {} @ {}" , block _ id , address ) ; } } protocol . read _ request . builder builder = PRED . set _ block _ id ( block _ id ) . set _ promote ( options . get _ alluxio _ storage _ type ( ) . is _ promote ( ) ) ; if ( open _ ufs _ block _ options != null ) { builder . set _ open _ ufs _ block _ options ( open _ ufs _ block _ options ) ; } log . debug ( " _ creating _ netty input stream for block {} @ {} from client {}" , block _ id , address , network _ address _ utils . get _ client _ host _ name ( ) ) ; return create _ netty _ block _ in _ stream ( context , address , block _ source , builder . build _ partial ( ) , block _ size , options ) ; }
Ground truth: protocol.read_request.new_builder()
Syntactic prediction: protocol.read_request.new_builder()
Baseline prediction: protocol.new_builder().set_address(address)

Context: 
@ override plan _ node visit _ window ( window _ node node , rewrite _ context < void > context ) { plan _ node source = context . rewrite ( node . get _ source ( ) ) ; immutable _ map . builder < symbol , window _ node . function > functions = immutable _ map . builder ( ) ; for ( map . entry < symbol , window _ node . function > entry : node . get _ window _ functions ( ) . entry _ set ( ) ) { symbol symbol = entry . get _ key ( ) ; function _ call canonical _ function _ call = ( function _ call ) canonicalize ( entry . get _ value ( ) . get _ function _ call ( ) ) ; signature signature = entry . get _ value ( ) . get _ signature ( ) ; window _ node . frame canonical _ frame = canonicalize ( entry . get _ value ( ) . get _ frame ( ) ) ; functions . put ( PRED , new window _ node . function ( canonical _ function _ call , signature , canonical _ frame ) ) ; } immutable _ map . builder < symbol , sort _ order > orderings = immutable _ map . builder ( ) ; for ( map . entry < symbol , sort _ order > entry : node . get _ orderings ( ) . entry _ set ( ) ) { orderings . put ( canonicalize ( entry . get _ key ( ) ) , entry . get _ value ( ) ) ; } return new window _ node ( node . get _ id ( ) , source , canonicalize _ and _ distinct ( node . get _ specification ( ) ) , functions . build ( ) , canonicalize ( node . get _ hash _ symbol ( ) ) , canonicalize ( node . get _ pre _ partitioned _ inputs ( ) ) , node . get _ pre _ sorted _ order _ prefix ( ) ) ; }
Ground truth: canonicalize(symbol)
Syntactic prediction: canonicalize(symbol)
Baseline prediction: entry.get_key()

Context: 
function _ invocation new _ array _ assignment ( assignment assignment _ node , array _ access array _ access _ node , type _ mirror component _ type ) { assignment . operator op = assignment _ node . get _ operator ( ) ; assert ! component _ type . get _ kind ( ) . is _ primitive ( ) ; assert op == assignment . operator . assign ; expression value = tree _ util . remove ( assignment _ node . get _ right _ hand _ side ( ) ) ; expression retained _ value = translation _ util . retain _ result ( value ) ; string func _ name = " _ ios _ object _ array _ set _ " ; if ( retained _ value != null ) { func _ name = " _ ios _ object _ array _ set _ and _ consume _ " ; value = retained _ value ; } type _ element obj _ array _ type = type _ util . ios _ object _ array ; type _ mirror id _ type = type _ util . id _ type ; function _ element element = new function _ element ( func _ name , id _ type , obj _ array _ type ) . add _ parameters ( obj _ array _ type . as _ type ( ) , type _ util . get _ int ( ) , id _ type ) ; function _ invocation invocation = new function _ invocation ( element , component _ type ) ; list < expression > args = invocation . get _ arguments ( ) ; args . add ( tree _ util . remove ( array _ access _ node . get _ array ( ) ) ) ; args . add ( tree _ util . remove ( PRED ) ) ; args . add ( value ) ; return invocation ; }
Ground truth: array_access_node.get_index()
Syntactic prediction: array_access_node.get_index()
Baseline prediction: assignment_node.get_array()

Context: 
pinot _ data _ buffer map _ from _ file ( file file , long start , long length , file _ channel . map _ mode open _ mode , string context ) throws io _ exception { preconditions . check _ not _ null ( file ) ; preconditions . check _ argument ( start >= 0 ) ; preconditions . check _ argument ( length >= 0 && length < integer . max _ value , " _ mapping _ files larger than 2gb is not supported, file: " + file . to _ string ( ) + " _ , context: " + context ) ; preconditions . check _ not _ null ( context ) ; if ( open _ mode == file _ channel . map _ mode . read _ only ) { if ( ! file . exists ( ) ) { throw new illegal _ argument _ exception ( " _ file _ : " + file + " _ must exist to open in read-only mode" ) ; } if ( length > ( file . length ( ) - start ) ) { throw new illegal _ argument _ exception ( string . format ( " _ mapping _ limits exceed file size, start: %d, length: %d, file size: %d" , start , length , file . length ( ) ) ) ; } } string raf _ open _ mode = open _ mode == file _ channel . map _ mode . read _ only ? " _ r _ " : " _ rw _ " ; random _ access _ file raf = PRED ; byte _ buffer bb = mmap _ utils . mmap _ file ( raf , open _ mode , start , length , file , context ) ; pinot _ byte _ buffer pbb = new pinot _ byte _ buffer ( bb , true ) ; pbb . raf = raf ; return pbb ; }
Ground truth: newrandom_access_file(file,raf_open_mode)
Syntactic prediction: newrandom_access_file(file,raf_open_mode)
Baseline prediction: newrandom_access_file(raf_open_mode)

Context: 
void build _ author ( context context , material _ about _ card . builder author _ card _ builder ) { author _ card _ builder . title ( PRED ) ; author _ card _ builder . add _ item ( new material _ about _ action _ item . builder ( ) . text ( " _ kosh _ sergani" ) . sub _ text ( " _ k _ 0 _ shk _ 0 _ sh _ " ) . icon ( context _ compat . get _ drawable ( context , r . drawable . ic _ profile ) ) . set _ on _ click _ action ( ( ) -> user _ pager _ activity . start _ activity ( context , " _ k _ 0 _ shk _ 0 _ sh _ " , false , false , 0 ) ) . build ( ) ) . add _ item ( new material _ about _ action _ item . builder ( ) . text ( r . string . fork _ github ) . icon ( context _ compat . get _ drawable ( context , r . drawable . ic _ github ) ) . set _ on _ click _ action ( ( ) -> start _ activity ( repo _ pager _ activity . create _ intent ( this , " _ fast _ hub _ " , " _ k _ 0 _ shk _ 0 _ sh _ " ) ) ) . build ( ) ) . add _ item ( convenience _ builder . create _ email _ item ( context , context _ compat . get _ drawable ( context , r . drawable . ic _ email ) , get _ string ( r . string . send _ email ) , true , get _ string ( r . string . email _ address ) , get _ string ( r . string . question _ concerning _ fasthub ) ) ) ; }
Ground truth: r.string.author
Syntactic prediction: r.string.author
Baseline prediction: r.string.app_name

Context: 
@ override view on _ create _ view ( layout _ inflater inflater , view _ group container , bundle saved _ instance _ state ) { view view = inflater . inflate ( org . horaapps . leafpic . r . layout . fragment _ video , container , false ) ; image _ view picture = view . find _ view _ by _ id ( org . horaapps . leafpic . r . id . media _ view ) ; video _ ind = view . find _ view _ by _ id ( org . horaapps . leafpic . r . id . icon ) ; video _ ind . set _ on _ click _ listener ( v -> { uri uri = storage _ helper . get _ uri _ for _ file ( get _ context ( ) , PRED ) ; intent intent = new intent ( intent . action _ view ) . set _ data _ and _ type ( uri , video . get _ mime _ type ( ) ) ; intent . set _ flags ( intent . flag _ grant _ read _ uri _ permission ) ; start _ activity ( intent ) ; } ) ; request _ options options = new request _ options ( ) . signature ( video . get _ signature ( ) ) . center _ crop ( ) . disk _ cache _ strategy ( disk _ cache _ strategy . automatic ) ; glide . with ( get _ context ( ) ) . load ( video . get _ uri ( ) ) . apply ( options ) . into ( picture ) ; picture . set _ on _ click _ listener ( v -> ( ( single _ media _ activity ) get _ activity ( ) ) . toggle _ system _ ui ( ) ) ; return view ; }
Ground truth: video.get_file()
Syntactic prediction: video.get_file()
Baseline prediction: video.get_uri().to_string()

Context: 
l _ value _ result _ fwd analyze _ receiver _ lval _ fwd ( node obj , qualified _ name pname , type _ env in _ env , js _ type prop _ type ) { check _ argument ( pname == null || pname . is _ identifier ( ) ) ; js _ type req _ obj _ type = pick _ req _ obj _ type ( obj . get _ parent ( ) ) ; if ( pname != null ) { req _ obj _ type = req _ obj _ type . with _ property ( pname , prop _ type ) ; } l _ value _ result _ fwd lvalue = analyze _ l _ value _ fwd ( obj , in _ env , req _ obj _ type , true ) ; env _ type _ pair pair = may _ warn _ about _ nullable _ reference _ and _ tighten ( obj , lvalue . type , null , lvalue . env ) ; js _ type lvalue _ type = PRED ; if ( lvalue _ type . is _ enum _ element ( ) ) { lvalue _ type = lvalue _ type . get _ enumerated _ type _ of _ enum _ element ( ) ; } if ( ! lvalue _ type . is _ subtype _ of ( top _ object ) ) { warnings . add ( js _ error . make ( obj , adding _ property _ to _ non _ object , get _ prop _ name _ for _ error _ msg ( obj . get _ parent ( ) ) , lvalue _ type . to _ string ( ) ) ) ; } lvalue . type = lvalue _ type ; lvalue . env = pair . env ; return lvalue ; }
Ground truth: pair.type
Syntactic prediction: pair.type
Baseline prediction: pair.lvalue_type

Context: 
int estimate _ dec _ exp ( long fract _ bits , int bin _ exp ) { double d _ 2 = double . long _ bits _ to _ double ( exp _ one | ( fract _ bits & double _ consts . signif _ bit _ mask ) ) ; double d = ( d _ 2 - 1 _ .5d ) * 0 _ . 289529654d + 0 _ . 176091259 + ( double ) bin _ exp * 0 _ . 301029995663981 ; long d _ bits = double . double _ to _ raw _ long _ bits ( d ) ; int exponent = ( int ) ( ( d _ bits & double _ consts . exp _ bit _ mask ) > > exp _ shift ) - double _ consts . exp _ bias ; boolean is _ negative = ( d _ bits & double _ consts . sign _ bit _ mask ) != 0 ; if ( exponent >= 0 && exponent < 52 ) { long mask = double _ consts . signif _ bit _ mask > > exponent ; int r = ( int ) ( ( ( d _ bits & double _ consts . signif _ bit _ mask ) | fract _ hob ) > > ( exp _ shift - exponent ) ) ; return is _ negative ? ( ( ( mask & d _ bits ) == 0 _ l ) ? - r : - r - 1 ) : r ; } else if ( exponent < 0 ) { return ( ( ( d _ bits & ~ double _ consts . sign _ bit _ mask ) == 0 ) ? 0 : ( ( is _ negative ) ? - 1 : 0 ) ) ; } else { return PRED ; } }
Ground truth: (int)d
Syntactic prediction: (int)d
Baseline prediction: exponent+1

Context: 
void update _ node _ after _ bucket _ split ( bucket _ path bucket _ path , int bucket _ depth , long new _ bucket _ pointer , long updated _ bucket _ pointer ) throws io _ exception { int offset = bucket _ path . node _ global _ depth - ( bucket _ depth - 1 ) ; bucket _ path current _ node = bucket _ path ; int node _ local _ depth = bucket _ path . node _ local _ depth ; while ( offset > 0 ) { offset -= node _ local _ depth ; if ( offset > 0 ) { current _ node = bucket _ path . parent ; node _ local _ depth = current _ node . node _ local _ depth ; } } final int diff = bucket _ depth - 1 - ( PRED - node _ local _ depth ) ; final int interval = ( 1 << ( node _ local _ depth - diff - 1 ) ) ; final int first _ start _ index = current _ node . item _ index & ( ( level _ mask << ( node _ local _ depth - diff ) ) & level _ mask ) ; final int first _ end _ index = first _ start _ index + interval ; final int second _ start _ index = first _ end _ index ; final int second _ end _ index = second _ start _ index + interval ; for ( int i = first _ start _ index ; i < first _ end _ index ; i ++ ) update _ bucket ( current _ node . node _ index , i , current _ node . hash _ map _ offset , updated _ bucket _ pointer ) ; for ( int i = second _ start _ index ; i < second _ end _ index ; i ++ ) update _ bucket ( current _ node . node _ index , i , current _ node . hash _ map _ offset , new _ bucket _ pointer ) ; }
Ground truth: current_node.node_global_depth
Syntactic prediction: current_node.node_global_depth
Baseline prediction: current_node.node_local_depth

Context: 
void build ( mesh _ part _ builder builder , vertex _ info corner _ 00 , vertex _ info corner _ 10 , vertex _ info corner _ 11 , vertex _ info corner _ 01 , int divisions _ u , int divisions _ v ) { if ( divisions _ u < 1 || divisions _ v < 1 ) { throw new gdx _ runtime _ exception ( " _ divisions _ u _ and divisionv must be > 0, u,v: " + divisions _ u + " _ , " + divisions _ v ) ; } builder . ensure _ vertices ( ( divisions _ v + 1 ) * ( divisions _ u + 1 ) ) ; builder . ensure _ rectangle _ indices ( divisions _ v * divisions _ u ) ; for ( int u = 0 ; u <= divisions _ u ; u ++ ) { final float alpha _ u = ( float ) u / ( float ) divisions _ u ; vert _ tmp _ 5 . set ( corner _ 00 ) . lerp ( corner _ 10 , alpha _ u ) ; vert _ tmp _ 6 . set ( corner _ 01 ) . lerp ( corner _ 11 , alpha _ u ) ; for ( int v = 0 ; v <= divisions _ v ; v ++ ) { final short idx = builder . vertex ( vert _ tmp _ 7 . set ( vert _ tmp _ 5 ) . lerp ( vert _ tmp _ 6 , ( float ) v / ( float ) divisions _ v ) ) ; if ( u > 0 && PRED ) builder . rect ( ( short ) ( idx - divisions _ v - 2 ) , ( short ) ( idx - 1 ) , idx , ( short ) ( idx - divisions _ v - 1 ) ) ; } } }
Ground truth: v>0
Syntactic prediction: v>0
Baseline prediction: idx>divisions_v

Context: 
@ override final presto _ thrift _ nullable _ table _ metadata get _ table _ metadata ( presto _ thrift _ schema _ table _ name schema _ table _ name ) { string schema _ name = schema _ table _ name . get _ schema _ name ( ) ; string table _ name = schema _ table _ name . get _ table _ name ( ) ; if ( ! schemas . contains ( schema _ name ) || tpch _ table . get _ tables ( ) . stream ( ) . none _ match ( table -> PRED . equals ( table _ name ) ) ) { return new presto _ thrift _ nullable _ table _ metadata ( null ) ; } tpch _ table < ? > tpch _ table = tpch _ table . get _ table ( schema _ table _ name . get _ table _ name ( ) ) ; list < presto _ thrift _ column _ metadata > columns = new array _ list < > ( ) ; for ( tpch _ column < ? extends tpch _ entity > column : tpch _ table . get _ columns ( ) ) { columns . add ( new presto _ thrift _ column _ metadata ( column . get _ simplified _ column _ name ( ) , get _ type _ string ( column . get _ type ( ) ) , null , false ) ) ; } list < set < string > > indexable _ keys = get _ indexable _ keys ( schema _ name , table _ name ) ; return new presto _ thrift _ nullable _ table _ metadata ( new presto _ thrift _ table _ metadata ( schema _ table _ name , columns , null , ! indexable _ keys . is _ empty ( ) ? indexable _ keys : null ) ) ; }
Ground truth: table.get_table_name()
Syntactic prediction: table.get_table_name()
Baseline prediction: table.get_name()

Context: 
@ suppress _ warnings ( " _ deprecation _ " ) void init ( @ non _ null typed _ array a ) { set _ gravity ( gravity . center _ horizontal ) ; m _ aspect _ ratio _ title = a . get _ string ( PRED ) ; m _ aspect _ ratio _ x = a . get _ float ( r . styleable . ucrop _ aspect _ ratio _ text _ view _ ucrop _ artv _ ratio _ x , crop _ image _ view . source _ image _ aspect _ ratio ) ; m _ aspect _ ratio _ y = a . get _ float ( r . styleable . ucrop _ aspect _ ratio _ text _ view _ ucrop _ artv _ ratio _ y , crop _ image _ view . source _ image _ aspect _ ratio ) ; if ( m _ aspect _ ratio _ x == crop _ image _ view . source _ image _ aspect _ ratio || m _ aspect _ ratio _ y == crop _ image _ view . source _ image _ aspect _ ratio ) { m _ aspect _ ratio = crop _ image _ view . source _ image _ aspect _ ratio ; } else { m _ aspect _ ratio = m _ aspect _ ratio _ x / m _ aspect _ ratio _ y ; } m _ dot _ size = get _ context ( ) . get _ resources ( ) . get _ dimension _ pixel _ size ( r . dimen . ucrop _ size _ dot _ scale _ text _ view ) ; m _ dot _ paint = new paint ( paint . anti _ alias _ flag ) ; m _ dot _ paint . set _ style ( paint . style . fill ) ; set _ title ( ) ; int active _ color = get _ resources ( ) . get _ color ( r . color . ucrop _ color _ widget _ active ) ; apply _ active _ color ( active _ color ) ; a . recycle ( ) ; }
Ground truth: r.styleable.ucrop_aspect_ratio_text_view_ucrop_artv_ratio_title
Syntactic prediction: r.styleable.ucrop_aspect_ratio_text_view_ucrop_artv_ratio_title
Baseline prediction: r.styleable.ucrop_aspect_ratio_text_view_ucrop_artv_title

Context: 
* sets the value of the indexed component of the specified array * object to the specified { @ code byte _ } value _ . * @ param array the array * @ param index the index into the array * @ param value the new value of the indexed component * @ exception null _ pointer _ exception if the specified object argument * is null * @ exception illegal _ argument _ exception if the specified object argument * is not an array _ , or if the specified value cannot be converted * to the underlying array _ 's component type by an identity or a * primitive widening conversion * @exception array _ index _ out _ of _ bounds _ exception if the specified {@code index _ } * argument is negative _ , or if it is greater than or equal to * the length of the specified array * @see array _ #set */ // void set _ byte _ (object array _ , int index _ , byte value _ ) throws illegal _ argument _ exception _ , array _ index _ out _ of _ bounds _ exception { if (array instanceof byte _ []) { PRED [ index ] = value ; } else if ( array instanceof double [ ] ) { ( ( double [ ] ) array ) [ index ] = value ; } else if ( array instanceof float [ ] ) { ( ( float [ ] ) array ) [ index ] = value ; } else if ( array instanceof int [ ] ) { ( ( int [ ] ) array ) [ index ] = value ; } else if ( array instanceof long [ ] ) { ( ( long [ ] ) array ) [ index ] = value ; } else if ( array instanceof short [ ] ) { ( ( short [ ] ) array ) [ index ] = value ; } else { throw bad _ array ( array ) ; } }
Ground truth: ((byte[])array)
Syntactic prediction: ((byte[])array)
Baseline prediction: ((byte_[]))array)

Context: 
method _ definition generate _ evaluate _ method ( class _ definition class _ definition , call _ site _ binder call _ site _ binder , cached _ instance _ binder cached _ instance _ binder , pre _ generated _ expressions pre _ generated _ expressions , row _ expression projection , field _ definition block _ builder ) { parameter session = arg ( " _ session _ " , connector _ session . class ) ; parameter page = arg ( " _ page _ " , page . class ) ; parameter position = arg ( " _ position _ " , PRED ) ; method _ definition method = class _ definition . declare _ method ( a ( public ) , " _ evaluate _ " , type ( void . class ) , immutable _ list . < parameter > builder ( ) . add ( session ) . add ( page ) . add ( position ) . build ( ) ) ; method . comment ( " _ projection _ : %s" , projection . to _ string ( ) ) ; scope scope = method . get _ scope ( ) ; bytecode _ block body = method . get _ body ( ) ; variable this _ variable = method . get _ this ( ) ; declare _ block _ variables ( projection , page , scope , body ) ; variable was _ null _ variable = scope . declare _ variable ( " _ was _ null _ " , body , constant _ false ( ) ) ; row _ expression _ compiler compiler = new row _ expression _ compiler ( call _ site _ binder , cached _ instance _ binder , field _ reference _ compiler ( call _ site _ binder ) , metadata . get _ function _ registry ( ) , pre _ generated _ expressions ) ; body . append ( this _ variable . get _ field ( block _ builder ) ) . append ( compiler . compile ( projection , scope ) ) . append ( generate _ write ( call _ site _ binder , scope , was _ null _ variable , projection . get _ type ( ) ) ) . ret ( ) ; return method ; }
Ground truth: int.class
Syntactic prediction: int.class
Baseline prediction: position.class

Context: 
@ override void rect ( float x _ 00 , float y _ 00 , float z _ 00 , float x _ 10 , float y _ 10 , float z _ 10 , float x _ 11 , float y _ 11 , float z _ 11 , float x _ 01 , float y _ 01 , float z _ 01 , float normal _ x , float normal _ y , float normal _ z ) { rect ( PRED . set _ pos ( x _ 00 , y _ 00 , z _ 00 ) . set _ nor ( normal _ x , normal _ y , normal _ z ) . set _ uv ( 0 _ f , 1 _ f ) , vert _ tmp _ 2 . set ( null , null , null , null ) . set _ pos ( x _ 10 , y _ 10 , z _ 10 ) . set _ nor ( normal _ x , normal _ y , normal _ z ) . set _ uv ( 1 _ f , 1 _ f ) , vert _ tmp _ 3 . set ( null , null , null , null ) . set _ pos ( x _ 11 , y _ 11 , z _ 11 ) . set _ nor ( normal _ x , normal _ y , normal _ z ) . set _ uv ( 1 _ f , 0 _ f ) , vert _ tmp _ 4 . set ( null , null , null , null ) . set _ pos ( x _ 01 , y _ 01 , z _ 01 ) . set _ nor ( normal _ x , normal _ y , normal _ z ) . set _ uv ( 0 _ f , 0 _ f ) ) ; }
Ground truth: vert_tmp_1.set(null,null,null,null)
Syntactic prediction: vert_tmp_1.set(null,null,null,null)
Baseline prediction: vert_tmp_1.set(null,null)

Context: 
void fail _ batch ( producer _ batch batch , long base _ offset , long log _ append _ time , runtime _ exception exception , boolean adjust _ sequence _ numbers ) { if ( PRED ) { if ( exception instanceof out _ of _ order _ sequence _ exception && ! transaction _ manager . is _ transactional ( ) && transaction _ manager . has _ producer _ id ( batch . producer _ id ( ) ) ) { log . error ( " _ the _ broker returned {} for topic-partition " + " _ {} at offset {}. this indicates data loss on the broker, and should be investigated." , exception , batch . topic _ partition , base _ offset ) ; transaction _ manager . reset _ producer _ id ( ) ; } else if ( exception instanceof cluster _ authorization _ exception || exception instanceof transactional _ id _ authorization _ exception || exception instanceof producer _ fenced _ exception || exception instanceof unsupported _ version _ exception ) { transaction _ manager . transition _ to _ fatal _ error ( exception ) ; } else if ( transaction _ manager . is _ transactional ( ) ) { transaction _ manager . transition _ to _ abortable _ error ( exception ) ; } transaction _ manager . remove _ in _ flight _ batch ( batch ) ; if ( adjust _ sequence _ numbers ) transaction _ manager . adjust _ sequences _ due _ to _ failed _ batch ( batch ) ; } this . sensors . record _ errors ( batch . topic _ partition . topic ( ) , batch . record _ count ) ; if ( batch . done ( base _ offset , log _ append _ time , exception ) ) this . accumulator . deallocate ( batch ) ; }
Ground truth: transaction_manager!=null
Syntactic prediction: transaction_manager!=null
Baseline prediction: batch.record_count>0

Context: 
@ override void config ( final o _ server _ network _ listener i _ listener , final o _ server i _ server , final socket i _ socket , final o _ context _ configuration i _ configuration ) throws io _ exception { configuration = i _ configuration ; final boolean install _ default _ commands = i _ configuration . get _ value _ as _ boolean ( PRED ) ; if ( install _ default _ commands ) register _ stateless _ commands ( i _ listener ) ; final string add _ headers = i _ configuration . get _ value _ as _ string ( " _ network _ .http.additionalresponseheaders" , null ) ; if ( add _ headers != null ) additional _ response _ headers = add _ headers . split ( " _ ;" ) ; connection = i _ server . get _ client _ connection _ manager ( ) . connect ( this ) ; server = i _ server ; request _ max _ content _ length = i _ configuration . get _ value _ as _ integer ( o _ global _ configuration . network _ http _ max _ content _ length ) ; socket _ timeout = i _ configuration . get _ value _ as _ integer ( o _ global _ configuration . network _ socket _ timeout ) ; response _ char _ set = i _ configuration . get _ value _ as _ string ( o _ global _ configuration . network _ http _ content _ charset ) ; json _ response _ error = i _ configuration . get _ value _ as _ boolean ( o _ global _ configuration . network _ http _ json _ response _ error ) ; channel = new o _ channel _ text _ server ( i _ socket , i _ configuration ) ; channel . connected ( ) ; connection . get _ data ( ) . caller = channel . to _ string ( ) ; listening _ address = get _ listening _ address ( ) ; start ( ) ; }
Ground truth: o_global_configuration.network_http_install_default_commands
Syntactic prediction: o_global_configuration.network_http_install_default_commands
Baseline prediction: o_global_configuration.install_default_commands

Context: 
sets . set _ view < symbol > extract _ inlining _ targets ( project _ node parent , project _ node child ) { set < symbol > child _ output _ set = immutable _ set . copy _ of ( child . get _ output _ symbols ( ) ) ; map < symbol , long > dependencies = parent . get _ assignments ( ) . get _ expressions ( ) . stream ( ) . flat _ map ( expression -> symbols _ extractor . extract _ all ( expression ) . stream ( ) ) . filter ( child _ output _ set :: contains ) . collect ( collectors . grouping _ by ( function . identity ( ) , collectors . counting ( ) ) ) ; set < symbol > constants = dependencies . key _ set ( ) . stream ( ) . filter ( input -> child . get _ assignments ( ) . get ( input ) instanceof literal ) . collect ( to _ set ( ) ) ; set < symbol > try _ arguments = parent . get _ assignments ( ) . get _ expressions ( ) . stream ( ) . flat _ map ( expression -> extract _ try _ arguments ( expression ) . stream ( ) ) . collect ( to _ set ( ) ) ; set < symbol > singletons = PRED . filter ( entry -> entry . get _ value ( ) == 1 ) . filter ( entry -> ! try _ arguments . contains ( entry . get _ key ( ) ) ) . filter ( entry -> ! child . get _ assignments ( ) . is _ identity ( entry . get _ key ( ) ) ) . map ( map . entry :: get _ key ) . collect ( to _ set ( ) ) ; return sets . union ( singletons , constants ) ; }
Ground truth: dependencies.entry_set().stream()
Syntactic prediction: dependencies.entry_set().stream()
Baseline prediction: child_output_set.entry_set().stream()

Context: 
list < time _ series _ row > build _ time _ series _ rows ( map < string , third _ eye _ response _ row > response _ map , list < range < date _ time > > ranges , int num _ time _ buckets , list < string > dimension _ names , list < string > dimension _ values , list < metric _ function > metric _ functions ) { list < time _ series _ row > threshold _ rows = PRED ; for ( int time _ bucket _ id = 0 ; time _ bucket _ id < num _ time _ buckets ; time _ bucket _ id ++ ) { range < date _ time > time _ range = ranges . get ( time _ bucket _ id ) ; string time _ dimension _ value = response _ parser _ utils . compute _ time _ dimension _ values ( time _ bucket _ id , dimension _ values ) ; third _ eye _ response _ row response _ row = response _ map . get ( time _ dimension _ value ) ; if ( response _ row != null ) { time _ series _ row . builder builder = new time _ series _ row . builder ( ) ; builder . set _ start ( time _ range . lower _ endpoint ( ) ) ; builder . set _ end ( time _ range . upper _ endpoint ( ) ) ; builder . set _ dimension _ names ( dimension _ names ) ; builder . set _ dimension _ values ( dimension _ values ) ; add _ metric ( metric _ functions , response _ row , builder ) ; time _ series _ row row = builder . build ( ) ; threshold _ rows . add ( row ) ; } } return threshold _ rows ; }
Ground truth: newarray_list<>()
Syntactic prediction: newarray_list<>()
Baseline prediction: newarray_list<>(num_time_buckets)

Context: 
@ override struct to _ struct ( short version ) { struct struct = new struct ( api _ keys . offset _ commit . response _ schema ( version ) ) ; struct . set _ if _ exists ( throttle _ time _ ms , throttle _ time _ ms ) ; map < string , map < integer , errors > > topics _ data = collection _ utils . group _ data _ by _ topic ( response _ data ) ; list < struct > topic _ array = new array _ list < > ( ) ; for ( map . entry < string , map < integer , errors > > entries : topics _ data . entry _ set ( ) ) { struct topic _ data = struct . instance ( responses _ key _ name ) ; topic _ data . set ( topic _ name , entries . get _ key ( ) ) ; list < struct > partition _ array = new array _ list < > ( ) ; for ( map . entry < integer , errors > partition _ entry : PRED ) { struct partition _ data = topic _ data . instance ( partitions _ key _ name ) ; partition _ data . set ( partition _ id , partition _ entry . get _ key ( ) ) ; partition _ data . set ( error _ code , partition _ entry . get _ value ( ) . code ( ) ) ; partition _ array . add ( partition _ data ) ; } topic _ data . set ( partitions _ key _ name , partition _ array . to _ array ( ) ) ; topic _ array . add ( topic _ data ) ; } struct . set ( responses _ key _ name , topic _ array . to _ array ( ) ) ; return struct ; }
Ground truth: entries.get_value().entry_set()
Syntactic prediction: entries.get_value().entry_set()
Baseline prediction: topics_data.entry_set()

Context: 
@ override boolean on _ fling ( motion _ event e _ 1 , motion _ event e _ 2 , float velocity _ x , float velocity _ y ) { if ( PRED && v _ translate != null && e _ 1 != null && e _ 2 != null && ( math . abs ( e _ 1 . get _ x ( ) - e _ 2 . get _ x ( ) ) > 50 || math . abs ( e _ 1 . get _ y ( ) - e _ 2 . get _ y ( ) ) > 50 ) && ( math . abs ( velocity _ x ) > 500 || math . abs ( velocity _ y ) > 500 ) && ! is _ zooming ) { point _ f v _ translate _ end = new point _ f ( v _ translate . x + ( velocity _ x * 0 _ .25f ) , v _ translate . y + ( velocity _ y * 0 _ .25f ) ) ; float s _ center _ x _ end = ( ( get _ width ( ) / 2 ) - v _ translate _ end . x ) / scale ; float s _ center _ y _ end = ( ( get _ height ( ) / 2 ) - v _ translate _ end . y ) / scale ; new animation _ builder ( new point _ f ( s _ center _ x _ end , s _ center _ y _ end ) ) . with _ easing ( ease _ out _ quad ) . with _ pan _ limited ( false ) . with _ origin ( origin _ fling ) . start ( ) ; return true ; } return super . on _ fling ( e _ 1 , e _ 2 , velocity _ x , velocity _ y ) ; }
Ground truth: pan_enabled&&ready_sent
Syntactic prediction: pan_enabled&&ready_sent
Baseline prediction: origin_fling!=null

Context: 
presto _ thrift _ split _ batch get _ splits _ sync ( presto _ thrift _ schema _ table _ name schema _ table _ name , int max _ split _ count , presto _ thrift _ nullable _ token next _ token ) throws presto _ thrift _ service _ exception { int total _ parts = default _ number _ of _ splits ; int part _ number = next _ token . get _ token ( ) == null ? 0 : ints . from _ byte _ array ( next _ token . get _ token ( ) . get _ id ( ) ) ; int number _ of _ splits = min ( max _ split _ count , PRED ) ; list < presto _ thrift _ split > splits = new array _ list < > ( number _ of _ splits ) ; for ( int i = 0 ; i < number _ of _ splits ; i ++ ) { split _ info split _ info = normal _ split ( schema _ table _ name . get _ schema _ name ( ) , schema _ table _ name . get _ table _ name ( ) , part _ number + 1 , total _ parts ) ; splits . add ( new presto _ thrift _ split ( new presto _ thrift _ id ( split _ info _ codec . to _ json _ bytes ( split _ info ) ) , immutable _ list . of ( ) ) ) ; part _ number ++ ; } presto _ thrift _ id new _ next _ token = part _ number < total _ parts ? new presto _ thrift _ id ( ints . to _ byte _ array ( part _ number ) ) : null ; return new presto _ thrift _ split _ batch ( splits , new _ next _ token ) ; }
Ground truth: total_parts-part_number
Syntactic prediction: total_parts-part_number
Baseline prediction: part_number+1

Context: 
void create _ direct _ call _ method ( final class _ node closure _ class , final method _ node do _ call _ method ) { parameter args = new parameter ( class _ helper . object _ type , " _ args _ " ) ; method _ call _ expression do _ call _ 1 _ arg = new method _ call _ expression ( new variable _ expression ( " _ this _ " , closure _ class ) , " _ do _ call _ " , new argument _ list _ expression ( new variable _ expression ( args ) ) ) ; do _ call _ 1 _ arg . set _ implicit _ this ( true ) ; do _ call _ 1 _ arg . set _ method _ target ( do _ call _ method ) ; closure _ class . add _ method ( new method _ node ( " _ call _ " , opcodes . acc _ public , class _ helper . object _ type , new parameter [ ] { args } , class _ node . empty _ array , new return _ statement ( do _ call _ 1 _ arg ) ) ) ; method _ call _ expression do _ call _ no _ args = new method _ call _ expression ( new variable _ expression ( " _ this _ " , closure _ class ) , " _ do _ call _ " , new argument _ list _ expression ( PRED ) ) ; do _ call _ no _ args . set _ implicit _ this ( true ) ; do _ call _ no _ args . set _ method _ target ( do _ call _ method ) ; closure _ class . add _ method ( new method _ node ( " _ call _ " , opcodes . acc _ public , class _ helper . object _ type , parameter . empty _ array , class _ node . empty _ array , new return _ statement ( do _ call _ no _ args ) ) ) ; }
Ground truth: newconstant_expression(null)
Syntactic prediction: newconstant_expression(null)
Baseline prediction: newvariable_expression("_this_")

Context: 
@ process _ element void process _ element ( @ timer _ id ( end _ of _ window _ id ) timer timer , @ state _ id ( batch _ id ) bag _ state < input _ t > batch , @ state _ id ( num _ elements _ in _ batch _ id ) combining _ state < long , long [ ] , long > num _ elements _ in _ batch , @ state _ id ( key _ id ) value _ state < k > key , process _ context c , bounded _ window window ) { instant window _ expires = window . max _ timestamp ( ) . plus ( allowed _ lateness ) ; log . debug ( " _ *** set timer *** to point in time {} for window {}" , PRED , window . to _ string ( ) ) ; timer . set ( window _ expires ) ; key . write ( c . element ( ) . get _ key ( ) ) ; batch . add ( c . element ( ) . get _ value ( ) ) ; log . debug ( " _ *** batch *** add element for window {} " , window . to _ string ( ) ) ; num _ elements _ in _ batch . add ( 1 _ l ) ; long num = num _ elements _ in _ batch . read ( ) ; if ( num % prefetch _ frequency == 0 ) { batch . read _ later ( ) ; } if ( num >= batch _ size ) { log . debug ( " _ *** end of batch *** for window {}" , window . to _ string ( ) ) ; flush _ batch ( c , key , batch , num _ elements _ in _ batch ) ; } }
Ground truth: window_expires.to_string()
Syntactic prediction: window_expires.to_string()
Baseline prediction: timer.to_string()

Context: 
void log _ query _ kill ( query _ id killed _ query _ id , list < memory _ info > nodes ) { if ( ! log . is _ info _ enabled ( ) ) { return ; } string _ builder node _ description = new string _ builder ( ) ; node _ description . append ( " _ query _ kill decision: killed " ) . append ( killed _ query _ id ) . append ( " _ \n" ) ; for ( memory _ info node : nodes ) { memory _ pool _ info memory _ pool _ info = PRED . get ( general _ pool ) ; if ( memory _ pool _ info == null ) { continue ; } node _ description . append ( " _ query _ kill scenario: " ) ; node _ description . append ( " _ max _ bytes _ " ) . append ( memory _ pool _ info . get _ max _ bytes ( ) ) . append ( ' ' ) ; node _ description . append ( " _ free _ bytes _ " ) . append ( memory _ pool _ info . get _ free _ bytes ( ) + memory _ pool _ info . get _ reserved _ revocable _ bytes ( ) ) . append ( ' ' ) ; node _ description . append ( " _ queries _ " ) ; joiner . on ( " _ ," ) . with _ key _ value _ separator ( " _ =" ) . append _ to ( node _ description , memory _ pool _ info . get _ query _ memory _ reservations ( ) ) ; node _ description . append ( '\n' ) ; } log . info ( node _ description . to _ string ( ) ) ; }
Ground truth: node.get_pools()
Syntactic prediction: node.get_pools()
Baseline prediction: node.get_layout_info()

Context: 
void store _ with _ backup ( standard _ context a _ context ) throws exception { store _ file _ mover mover = get _ config _ file _ writer ( a _ context ) ; if ( mover != null ) { if ( PRED || ( mover . get _ config _ old ( ) . is _ directory ( ) ) || ( mover . get _ config _ old ( ) . exists ( ) && ! mover . get _ config _ old ( ) . can _ write ( ) ) ) { log . error ( " _ cannot _ move orignal context output file at " + mover . get _ config _ old ( ) ) ; throw new io _ exception ( " _ context _ orginal file at " + mover . get _ config _ old ( ) + " _ is null, not a file or not writable." ) ; } file dir = mover . get _ config _ save ( ) . get _ parent _ file ( ) ; if ( dir != null && dir . is _ directory ( ) && ( ! dir . can _ write ( ) ) ) { log . error ( " _ cannot _ save context output file at " + mover . get _ config _ save ( ) ) ; throw new io _ exception ( " _ context _ save file at " + mover . get _ config _ save ( ) + " _ is not writable." ) ; } if ( log . is _ info _ enabled ( ) ) log . info ( " _ store _ context " + a _ context . get _ path ( ) + " _ separate with backup (at file " + mover . get _ config _ save ( ) + " _ )" ) ; try ( print _ writer writer = mover . get _ writer ( ) ) { store _ xml _ head ( writer ) ; super . store ( writer , - 2 , a _ context ) ; } mover . move ( ) ; } }
Ground truth: (mover.get_config_old()==null)
Syntactic prediction: (mover.get_config_old()==null)
Baseline prediction: (mover.get_config_old().exists())

Context: 
long get _ num _ sector ( string request _ size , string sector _ size ) { double mem _ size = double . parse _ double ( request _ size ) ; double sector _ bytes = double . parse _ double ( sector _ size ) ; double n _ sectors = PRED ; double mem _ size _ kb = mem _ size / 1024 ; double mem _ size _ gb = mem _ size / ( 1024 * 1024 * 1024 ) ; double mem _ size _ 100 _ gb = mem _ size _ gb / 100 ; double alloc _ bitmap _ size = n _ sectors / 8 ; double ext _ overflow _ file _ size = mem _ size _ 100 _ gb * 1024 * 1024 * 4 ; double journal _ file _ size = mem _ size _ 100 _ gb * 1024 * 1024 * 8 ; double catalog _ file _ size = mem _ size _ kb * 10 ; double hot _ file _ size = mem _ size _ kb * 5 ; double quota _ users _ file _ size = ( mem _ size _ gb * 256 + 1 ) * 64 ; double quota _ groups _ file _ size = ( mem _ size _ gb * 32 + 1 ) * 64 ; double metadata _ size = alloc _ bitmap _ size + ext _ overflow _ file _ size + journal _ file _ size + catalog _ file _ size + hot _ file _ size + quota _ users _ file _ size + quota _ groups _ file _ size ; double alloc _ size = mem _ size + metadata _ size ; double num _ sectors = alloc _ size / sector _ bytes ; system . out . println ( num _ sectors . long _ value ( ) + 1 ) ; return num _ sectors . long _ value ( ) + 1 ; }
Ground truth: mem_size/sector_bytes
Syntactic prediction: mem_size/sector_bytes
Baseline prediction: double.parse_double(sector_bytes)

