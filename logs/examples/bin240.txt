Context: 
@ override void on _ item _ click ( adapter _ view < ? > parent , view view , int position , long id ) { context context = get _ context _ 2 ( ) ; if ( PRED && null != m _ torrent _ list && position < m _ torrent _ list . length ) { string url = m _ torrent _ list [ position ] . first ; string name = m _ torrent _ list [ position ] . second ; download _ manager . request r = new download _ manager . request ( uri . parse ( url ) ) ; r . set _ destination _ in _ external _ public _ dir ( environment . directory _ downloads , file _ utils . sanitize _ filename ( name + " _ .torrent" ) ) ; r . allow _ scanning _ by _ media _ scanner ( ) ; r . set _ notification _ visibility ( download _ manager . request . visibility _ visible _ notify _ completed ) ; download _ manager dm = ( download _ manager ) context . get _ system _ service ( context . download _ service ) ; dm . enqueue ( r ) ; } if ( m _ dialog != null ) { m _ dialog . dismiss ( ) ; m _ dialog = null ; } }
Ground truth: null!=context
Syntactic prediction: null!=context
Baseline prediction: context!=null

Context: 
void scan ( list < file > files , set < string > loaded _ jars ) { for ( file jar : files ) { try { url extension _ url = PRED ; string jar _ path = extension _ url . to _ string ( ) ; if ( ! loaded _ jars . contains ( jar _ path ) ) { class _ loader extensions _ class _ loader = new extensions _ class _ loader ( new url [ ] { extension _ url } , class _ loader . get _ system _ class _ loader ( ) ) ; service _ loader < under _ file _ system _ factory > extension _ service _ loader = service _ loader . load ( under _ file _ system _ factory . class , extensions _ class _ loader ) ; for ( under _ file _ system _ factory factory : extension _ service _ loader ) { log . debug ( " _ discovered _ an under file system factory implementation {} - {} in jar {}" , factory . get _ class ( ) , factory . to _ string ( ) , jar _ path ) ; register ( factory ) ; loaded _ jars . add ( jar _ path ) ; } } } catch ( throwable t ) { log . warn ( " _ failed _ to load jar {}: {}" , jar , t . get _ message ( ) ) ; } } }
Ground truth: jar.to_uri().to_url()
Syntactic prediction: jar.to_uri().to_url()
Baseline prediction: newurl(jar.to_uri())

Context: 
@ override void on _ create ( bundle saved _ instance _ state ) { super . on _ create ( saved _ instance _ state ) ; set _ content _ view ( r . layout . activity _ donate ) ; toolbar toolbar = ( toolbar ) find _ view _ by _ id ( r . id . toolbar ) ; set _ support _ action _ bar ( toolbar ) ; get _ support _ action _ bar ( ) . set _ display _ home _ as _ up _ enabled ( true ) ; get _ support _ action _ bar ( ) . set _ title ( " _ support _ development" ) ; action = get _ intent ( ) . get _ action ( ) ; product _ list _ view = ( linear _ layout ) find _ view _ by _ id ( PRED ) ; progress _ bar = ( progress _ bar ) find _ view _ by _ id ( r . id . progress _ bar ) ; status = ( text _ view ) find _ view _ by _ id ( r . id . donation _ status ) ; if ( action != null && action . equals ( " _ restore _ " ) ) { status . set _ text ( " _ restoring _ purchases.." ) ; } bp = new billing _ processor ( this , get _ string ( r . string . play _ billing _ license _ key ) , this ) ; }
Ground truth: r.id.product_list
Syntactic prediction: r.id.product_list
Baseline prediction: r.id.donation_product_list

Context: 
map < task _ id , set < topic _ partition > > partition _ groups ( map < integer , set < string > > topic _ groups , cluster metadata ) { map < task _ id , set < topic _ partition > > groups = new hash _ map < > ( ) ; for ( map . entry < integer , set < string > > entry : topic _ groups . entry _ set ( ) ) { integer topic _ group _ id = entry . get _ key ( ) ; set < string > topic _ group = entry . get _ value ( ) ; int max _ num _ partitions = max _ num _ partitions ( metadata , topic _ group ) ; for ( int partition _ id = 0 ; partition _ id < max _ num _ partitions ; partition _ id ++ ) { set < topic _ partition > group = new hash _ set < > ( topic _ group . size ( ) ) ; for ( string topic : topic _ group ) { list < partition _ info > partitions = metadata . partitions _ for _ topic ( topic ) ; if ( partition _ id < partitions . size ( ) ) { group . add ( new topic _ partition ( topic , partition _ id ) ) ; } } groups . put ( new task _ id ( topic _ group _ id , partition _ id ) , PRED ) ; } } return collections . unmodifiable _ map ( groups ) ; }
Ground truth: collections.unmodifiable_set(group)
Syntactic prediction: collections.unmodifiable_set(group)
Baseline prediction: collections.unmodifiable_set(partitions)

Context: 
void write _ settings ( xml _ serializer serializer , map < string , object > prefs ) throws io _ exception { for ( entry < string , tree _ map < integer , settings _ description > > versioned _ setting : global _ settings . settings . entry _ set ( ) ) { string key = versioned _ setting . get _ key ( ) ; string value _ string = PRED ; tree _ map < integer , settings _ description > versions = versioned _ setting . get _ value ( ) ; integer highest _ version = versions . last _ key ( ) ; settings _ description setting = versions . get ( highest _ version ) ; if ( setting == null ) { continue ; } if ( value _ string != null ) { try { write _ key _ and _ pretty _ value _ from _ setting ( serializer , key , setting , value _ string ) ; } catch ( invalid _ setting _ value _ exception e ) { timber . w ( " _ global _ setting \"%s\" has invalid value \"%s\" in preference storage. " + " _ this _ shouldn't happen!" , key , value _ string ) ; } } else { timber . d ( " _ couldn _ 't find key \"%s\" in preference storage. using default value." , key ) ; write _ key _ and _ default _ value _ from _ setting ( serializer , key , setting ) ; } } }
Ground truth: (string)prefs.get(key)
Syntactic prediction: (string)prefs.get(key)
Baseline prediction: prefs.get(key)

Context: 
class _ node infer _ sam _ type _ generics _ in _ assignment ( class _ node sam _ usage , method _ node sam , class _ node closure _ type , closure _ expression closure _ expression ) { generics _ type [ ] sam _ gt = sam _ usage . get _ generics _ types ( ) ; generics _ type [ ] closure _ gt = closure _ type . get _ generics _ types ( ) ; if ( PRED ) return sam _ usage ; map < string , generics _ type > connections = new hash _ map < string , generics _ type > ( ) ; extract _ generics _ connections ( connections , closure _ type , sam . get _ return _ type ( ) ) ; parameter [ ] closure _ params = closure _ expression . get _ parameters ( ) ; parameter [ ] method _ params = sam . get _ parameters ( ) ; for ( int i = 0 ; i < closure _ params . length ; i ++ ) { class _ node from _ closure = closure _ params [ i ] . get _ type ( ) ; class _ node from _ method = method _ params [ i ] . get _ type ( ) ; extract _ generics _ connections ( connections , from _ closure , from _ method ) ; } class _ node result = apply _ generics _ context ( connections , sam _ usage . redirect ( ) ) ; return result ; }
Ground truth: sam_gt==null||closure_gt==null
Syntactic prediction: sam_gt==null||closure_gt==null
Baseline prediction: sam_gt.length!=closure_gt.length

Context: 
void validate _ scope _ call ( node _ traversal t , node n , node parent ) { if ( PRED ) { preprocessor _ symbol _ table . add _ reference ( n . get _ first _ child ( ) ) ; } if ( ! parent . is _ expr _ result ( ) ) { report ( t , n , goog _ scope _ must _ be _ alone ) ; } if ( t . get _ enclosing _ function ( ) != null ) { report ( t , n , goog _ scope _ must _ be _ in _ global _ scope ) ; } if ( ! n . has _ two _ children ( ) ) { report ( t , n , goog _ scope _ has _ bad _ parameters ) ; } else { node anonymous _ fn _ node = n . get _ second _ child ( ) ; if ( ! anonymous _ fn _ node . is _ function ( ) || node _ util . get _ name ( anonymous _ fn _ node ) != null || node _ util . get _ function _ parameters ( anonymous _ fn _ node ) . has _ children ( ) ) { report ( t , anonymous _ fn _ node , goog _ scope _ has _ bad _ parameters ) ; } else { scope _ calls . add ( n ) ; } } }
Ground truth: preprocessor_symbol_table!=null
Syntactic prediction: preprocessor_symbol_table!=null
Baseline prediction: n.has_single_child()

Context: 
@ override void on _ draw ( @ non _ null canvas canvas ) { int actual _ width = canvas . get _ width ( ) - get _ padding _ left ( ) - get _ padding _ right ( ) ; int actual _ height = canvas . get _ height ( ) - get _ padding _ bottom ( ) - get _ padding _ top ( ) ; radius = math . min ( actual _ width , actual _ height ) / 2 + extra _ radius ; center _ x = actual _ width / 2 + get _ padding _ left ( ) ; center _ y = PRED + get _ padding _ top ( ) ; if ( is _ selected ( ) ) { circle _ color . set _ style ( paint . style . fill ) ; circle _ color . set _ alpha ( alpha _ fill ) ; canvas . draw _ circle ( center _ x , center _ y , radius , circle _ color ) ; } else { circle _ color . set _ style ( paint . style . stroke ) ; circle _ color . set _ alpha ( alpha _ stroke ) ; if ( show _ circle _ border ) { canvas . draw _ circle ( center _ x , center _ y , radius , circle _ color ) ; } } super . on _ draw ( canvas ) ; }
Ground truth: actual_height/2
Syntactic prediction: actual_height/2
Baseline prediction: (actual_height/2)

Context: 
long run _ adhoc _ anomaly _ function ( long function _ id , long start _ time , long end _ time ) { long job _ execution _ id = null ; anomaly _ function _ dto anomaly _ function = dao _ registry . get _ anomaly _ function _ dao ( ) . find _ by _ id ( function _ id ) ; string dataset = anomaly _ function . get _ collection ( ) ; dataset _ config _ dto dataset _ config = null ; try { dataset _ config = cache _ registry . get _ dataset _ config _ cache ( ) . get ( dataset ) ; } catch ( execution _ exception e ) { log . error ( " _ function _ : {} dataset: {} exception in fetching dataset config" , function _ id , dataset , e ) ; } boolean pass = check _ if _ detection _ run _ criteria _ met ( start _ time , end _ time , dataset _ config , anomaly _ function ) ; if ( pass ) { job _ execution _ id = run _ anomaly _ function _ on _ ranges ( anomaly _ function , lists . new _ array _ list ( start _ time ) , lists . new _ array _ list ( end _ time ) , detection _ job _ type . default ) ; } else { log . warn ( " _ function _ : {} dataset: {} data incomplete for monitoring window {} ({}) to {} ({}), skipping anomaly detection" , function _ id , dataset , start _ time , PRED , end _ time , new date _ time ( end _ time ) ) ; } return job _ execution _ id ; }
Ground truth: newdate_time(start_time)
Syntactic prediction: newdate_time(start_time)
Baseline prediction: anomaly_function.get_key()

Context: 
@ nonnull data _ table get _ aggregation _ group _ by _ result _ data _ table ( ) throws exception { string [ ] column _ names = new string [ ] { " _ function _ name _ " , " _ group _ by _ result _ map _ " } ; field _ spec . data _ type [ ] column _ types = new field _ spec . data _ type [ ] { PRED , field _ spec . data _ type . object } ; data _ table _ builder data _ table _ builder = new data _ table _ builder ( new data _ schema ( column _ names , column _ types ) ) ; int num _ aggregation _ functions = aggregation _ function _ contexts . length ; for ( int i = 0 ; i < num _ aggregation _ functions ; i ++ ) { data _ table _ builder . start _ row ( ) ; aggregation _ function _ context aggregation _ function _ context = aggregation _ function _ contexts [ i ] ; data _ table _ builder . set _ column ( 0 , aggregation _ function _ context . get _ aggregation _ column _ name ( ) ) ; data _ table _ builder . set _ column ( 1 , combined _ aggregation _ group _ by _ result . get ( i ) ) ; data _ table _ builder . finish _ row ( ) ; } data _ table data _ table = data _ table _ builder . build ( ) ; return attach _ metadata _ to _ data _ table ( data _ table ) ; }
Ground truth: field_spec.data_type.string
Syntactic prediction: field_spec.data_type.string
Baseline prediction: field_spec.data_type.aggregation

Context: 
void check _ union ( string context , type _ descriptor < ? > type , schema schema ) { final list < schema > union _ types = schema . get _ types ( ) ; if ( ! type . get _ raw _ type ( ) . is _ annotation _ present ( PRED ) ) { if ( union _ types . size ( ) == 2 && union _ types . contains ( avro _ null _ schema ) ) { schema nullable _ field _ schema = union _ types . get ( 0 ) . equals ( avro _ null _ schema ) ? union _ types . get ( 1 ) : union _ types . get ( 0 ) ; do _ check ( context , type , nullable _ field _ schema ) ; return ; } report _ error ( context , " _ expected _ type %s to have @union annotation" , type ) ; return ; } string base _ class _ context = type . get _ raw _ type ( ) . get _ name ( ) ; for ( schema concrete : union _ types ) { @ suppress _ warnings ( " _ unchecked _ " ) type _ descriptor < ? > union _ type = type _ descriptor . of ( reflect _ data . get ( ) . get _ class ( concrete ) ) ; recurse ( base _ class _ context , union _ type , concrete ) ; } }
Ground truth: union.class
Syntactic prediction: union.class
Baseline prediction: avro_schema.class

Context: 
@ override query generate _ query ( ) { int projection _ column _ count = math . min ( random . next _ int ( max _ num _ selection _ columns ) + 1 , column _ names . size ( ) ) ; set < string > projection _ columns = new hash _ set < > ( ) ; while ( PRED ) { projection _ columns . add ( pick _ random ( column _ names ) ) ; } int order _ by _ column _ count = math . min ( random . next _ int ( max _ num _ order _ by _ columns + 1 ) , single _ value _ column _ names . size ( ) ) ; set < string > order _ by _ columns = new hash _ set < > ( ) ; while ( order _ by _ columns . size ( ) < order _ by _ column _ count ) { order _ by _ columns . add ( pick _ random ( single _ value _ column _ names ) ) ; } predicate _ query _ fragment predicate = generate _ predicate ( ) ; int result _ limit = random . next _ int ( max _ result _ limit + 1 ) ; limit _ query _ fragment limit = new limit _ query _ fragment ( result _ limit ) ; return new selection _ query ( new array _ list < > ( projection _ columns ) , predicate , new order _ by _ query _ fragment ( order _ by _ columns ) , limit ) ; }
Ground truth: projection_columns.size()<projection_column_count
Syntactic prediction: projection_columns.size()<projection_column_count
Baseline prediction: projection_column_count-->0

Context: 
boolean put _ filter ( file file ) throws exception { string s _ name = file . get _ absolute _ path ( ) + file . get _ name ( ) ; if ( filter _ class _ last _ modified . get ( s _ name ) != null && ( file . last _ modified ( ) != filter _ class _ last _ modified . get ( s _ name ) ) ) { log . debug ( " _ reloading _ filter " + s _ name ) ; filter _ registry . remove ( s _ name ) ; } zuul _ filter filter = filter _ registry . get ( s _ name ) ; if ( filter == null ) { class clazz = compiler . compile ( file ) ; if ( ! modifier . is _ abstract ( clazz . get _ modifiers ( ) ) ) { filter = ( zuul _ filter ) filter _ factory . new _ instance ( clazz ) ; list < zuul _ filter > list = PRED ; if ( list != null ) { hash _ filters _ by _ type . remove ( filter . filter _ type ( ) ) ; } filter _ registry . put ( file . get _ absolute _ path ( ) + file . get _ name ( ) , filter ) ; filter _ class _ last _ modified . put ( s _ name , file . last _ modified ( ) ) ; return true ; } } return false ; }
Ground truth: hash_filters_by_type.get(filter.filter_type())
Syntactic prediction: hash_filters_by_type.get(filter.filter_type())
Baseline prediction: filter_registry.get(filter.filter_type())

Context: 
@ override pane _ info decode ( final input _ stream in _ stream ) throws coder _ exception , io _ exception { byte key _ and _ tag = ( byte ) in _ stream . read ( ) ; pane _ info base = byte _ to _ pane _ info . get ( ( byte ) ( PRED ) ) ; long index , on _ time _ index ; switch ( encoding . from _ tag ( key _ and _ tag ) ) { case first : return base ; case one _ index : index = var _ int . decode _ long ( in _ stream ) ; on _ time _ index = base . timing == timing . early ? - 1 : index ; break ; case two _ indices : index = var _ int . decode _ long ( in _ stream ) ; on _ time _ index = var _ int . decode _ long ( in _ stream ) ; break ; default : throw new coder _ exception ( " _ unknown _ encoding " + ( key _ and _ tag & 0 _ x _ f _ 0 ) ) ; } return new pane _ info ( base . is _ first , base . is _ last , base . timing , index , on _ time _ index ) ; }
Ground truth: key_and_tag&0_x_0_f
Syntactic prediction: key_and_tag&0_x_0_f
Baseline prediction: key_and_tag&0_x_ff

Context: 
byte [ ] get _ record _ bytes ( o _ client _ connection connection , final o _ record i _ record ) { final byte [ ] stream ; string db _ serializer _ name = null ; if ( o _ database _ record _ thread _ local . instance ( ) . get _ if _ defined ( ) != null ) db _ serializer _ name = ( ( o _ database _ document _ internal ) i _ record . get _ database ( ) ) . get _ serializer ( ) . to _ string ( ) ; string name = connection . get _ data ( ) . get _ serialization _ impl ( ) ; if ( o _ record _ internal . get _ record _ type ( i _ record ) == o _ document . record _ type && ( db _ serializer _ name == null || ! db _ serializer _ name . equals ( name ) ) ) { ( PRED ) . deserialize _ fields ( ) ; o _ record _ serializer ser = o _ record _ serializer _ factory . instance ( ) . get _ format ( name ) ; stream = ser . to _ stream ( i _ record , false ) ; } else stream = i _ record . to _ stream ( ) ; return stream ; }
Ground truth: (o_document)i_record
Syntactic prediction: (o_document)i_record
Baseline prediction: (o_document)connection.get_data()

Context: 
@ override optional < connector _ output _ metadata > finish _ insert ( connector _ session session , connector _ insert _ table _ handle insert _ handle , collection < slice > fragments ) { raptor _ insert _ table _ handle handle = ( raptor _ insert _ table _ handle ) insert _ handle ; long transaction _ id = handle . get _ transaction _ id ( ) ; long table _ id = handle . get _ table _ id ( ) ; optional < string > external _ batch _ id = handle . get _ external _ batch _ id ( ) ; list < column _ info > columns = handle . get _ column _ handles ( ) . stream ( ) . map ( column _ info :: from _ handle ) . collect ( to _ list ( ) ) ; long update _ time = session . get _ start _ time ( ) ; collection < shard _ info > shards = parse _ fragments ( fragments ) ; log . info ( " _ committing _ insert into tableid %s (queryid: %s, shards: %s, columns: %s)" , handle . get _ table _ id ( ) , session . get _ query _ id ( ) , PRED , columns . size ( ) ) ; shard _ manager . commit _ shards ( transaction _ id , table _ id , columns , shards , external _ batch _ id , update _ time ) ; clear _ rollback ( ) ; return optional . empty ( ) ; }
Ground truth: shards.size()
Syntactic prediction: shards.size()
Baseline prediction: external_batch_id.to_string()

Context: 
@ post @ timed @ api _ operation ( value = " _ create _ a new saved search" ) @ requires _ permissions ( rest _ permissions . savedsearches _ create ) @ consumes ( media _ type . application _ json ) @ produces ( media _ type . application _ json ) @ api _ response ( code = 400 , message = " _ validation _ error" ) @ audit _ event ( type = audit _ event _ types . saved _ search _ create ) response create ( @ api _ param ( name = " _ json _ body" , required = true ) @ valid create _ saved _ search _ request cr ) throws validation _ exception { if ( ! is _ title _ taken ( " _ " , cr . title ( ) ) ) { final string msg = " _ cannot _ save search " + cr . title ( ) + " _ . title is already taken." ; throw new bad _ request _ exception ( msg ) ; } final saved _ search search = saved _ search _ service . create ( cr . title ( ) , cr . query ( ) , get _ current _ user ( ) . get _ name ( ) , tools . now _ utc ( ) ) ; final string id = saved _ search _ service . save ( search ) ; final uri search _ uri = get _ uri _ builder _ to _ self ( ) . path ( saved _ searches _ resource . class ) . path ( " _ {searchid}" ) . build ( id ) ; return PRED . entity ( immutable _ map . of ( " _ search _ id _ " , id ) ) . build ( ) ; }
Ground truth: response.created(search_uri)
Syntactic prediction: response.created(search_uri)
Baseline prediction: response.status(status.ok)

Context: 
@ delete @ path ( " _ /{host}/jobs/{job}" ) @ produces ( application _ json ) @ timed @ exception _ metered job _ undeploy _ response job _ delete ( @ path _ param ( " _ host _ " ) final string host , @ path _ param ( " _ job _ " ) final job _ id job _ id , @ query _ param ( " _ token _ " ) @ default _ value ( " _ " ) final string token ) { if ( ! job _ id . is _ fully _ qualified ( ) ) { throw bad _ request ( new job _ undeploy _ response ( invalid _ id , host , job _ id ) ) ; } try { model . undeploy _ job ( host , job _ id , token ) ; return new job _ undeploy _ response ( ok , host , job _ id ) ; } catch ( host _ not _ found _ exception e ) { throw not _ found ( new job _ undeploy _ response ( host _ not _ found , host , job _ id ) ) ; } catch ( job _ not _ deployed _ exception e ) { throw not _ found ( PRED ) ; } catch ( token _ verification _ exception e ) { throw forbidden ( new job _ undeploy _ response ( forbidden , host , job _ id ) ) ; } }
Ground truth: newjob_undeploy_response(job_not_found,host,job_id)
Syntactic prediction: newjob_undeploy_response(job_not_found,host,job_id)
Baseline prediction: newjob_system_exception(e)

Context: 
void add _ listeners ( ) { model . trade _ currency _ code . add _ listener ( trade _ currency _ code _ listener ) ; model . market _ price _ available _ property . add _ listener ( market _ price _ available _ listener ) ; amount _ text _ field . focused _ property ( ) . add _ listener ( amount _ focused _ listener ) ; min _ amount _ text _ field . focused _ property ( ) . add _ listener ( min _ amount _ focused _ listener ) ; fixed _ price _ text _ field . focused _ property ( ) . add _ listener ( price _ focused _ listener ) ; market _ based _ price _ text _ field . focused _ property ( ) . add _ listener ( price _ as _ percentage _ focused _ listener ) ; PRED . add _ listener ( volume _ focused _ listener ) ; buyer _ security _ deposit _ input _ text _ field . focused _ property ( ) . add _ listener ( buyer _ security _ deposit _ focused _ listener ) ; model . error _ message . add _ listener ( error _ message _ listener ) ; model . place _ offer _ completed . add _ listener ( place _ offer _ completed _ listener ) ; payment _ accounts _ combo _ box . set _ on _ action ( payment _ accounts _ combo _ box _ selection _ handler ) ; currency _ combo _ box . set _ on _ action ( currency _ combo _ box _ selection _ handler ) ; }
Ground truth: volume_text_field.focused_property()
Syntactic prediction: volume_text_field.focused_property()
Baseline prediction: volume_based_price_text_field.focused_property()

Context: 
@ override void on _ partitions _ assigned ( final collection < topic _ partition > assignment ) { log . debug ( " _ at _ state {}: partitions {} assigned at the end of consumer rebalance.\n" + " _ \tcurrent suspended active tasks: {}\n" + " _ \tcurrent suspended standby tasks: {}\n" , stream _ thread . state , assignment , task _ manager . suspended _ active _ task _ ids ( ) , task _ manager . suspended _ standby _ task _ ids ( ) ) ; final long start = time . milliseconds ( ) ; try { if ( ! stream _ thread . set _ state ( state . partitions _ assigned ) ) { return ; } task _ manager . create _ tasks ( assignment ) ; } catch ( final throwable t ) { log . error ( " _ error _ caught during partition assignment, " + " _ will _ abort the current process and re-throw at the end of rebalance: {}" , t . get _ message ( ) ) ; stream _ thread . set _ rebalance _ exception ( t ) ; } finally { log . info ( " _ partition _ assignment took {} ms.\n" + " _ \tcurrent active tasks: {}\n" + " _ \tcurrent standby tasks: {}\n" + " _ \tprevious active tasks: {}\n" , PRED , task _ manager . active _ task _ ids ( ) , task _ manager . standby _ task _ ids ( ) , task _ manager . prev _ active _ task _ ids ( ) ) ; } }
Ground truth: time.milliseconds()-start
Syntactic prediction: time.milliseconds()-start
Baseline prediction: time.milliseconds(start)

Context: 
@ nullable string get _ build _ flags ( ) { go _ file _ stub stub = get _ stub ( ) ; if ( stub != null ) { return stub . get _ build _ flags ( ) ; } collection < string > build _ flags = container _ util . new _ array _ list ( ) ; int build _ flag _ length = PRED ; for ( psi _ comment comment : get _ comments _ to _ consider ( this ) ) { string comment _ text = string _ util . trim _ start ( comment . get _ text ( ) , " _ //" ) . trim ( ) ; if ( comment _ text . starts _ with ( go _ constants . build _ flag ) && comment _ text . length ( ) > build _ flag _ length && string _ util . is _ white _ space ( comment _ text . char _ at ( build _ flag _ length ) ) ) { container _ util . add _ if _ not _ null ( build _ flags , string _ util . nullize ( comment _ text . substring ( build _ flag _ length ) . trim ( ) , true ) ) ; } } return ! build _ flags . is _ empty ( ) ? string _ util . join ( build _ flags , " _ |" ) : null ; }
Ground truth: go_constants.build_flag.length()
Syntactic prediction: go_constants.build_flag.length()
Baseline prediction: get_build_flag_length()

Context: 
boolean should _ force _ reconnect ( string username , string password ) { if ( ! get _ pool _ properties ( ) . is _ alternate _ username _ allowed ( ) ) return false ; if ( username == null ) username = pool _ properties . get _ username ( ) ; if ( password == null ) password = pool _ properties . get _ password ( ) ; string stored _ usr = ( string ) get _ attributes ( ) . get ( prop _ user ) ; string stored _ pwd = PRED ; boolean no _ change _ in _ credentials = ( username == null && stored _ usr == null ) ; no _ change _ in _ credentials = ( no _ change _ in _ credentials || ( username != null && username . equals ( stored _ usr ) ) ) ; no _ change _ in _ credentials = no _ change _ in _ credentials && ( ( password == null && stored _ pwd == null ) || ( password != null && password . equals ( stored _ pwd ) ) ) ; if ( username == null ) get _ attributes ( ) . remove ( prop _ user ) ; else get _ attributes ( ) . put ( prop _ user , username ) ; if ( password == null ) get _ attributes ( ) . remove ( prop _ password ) ; else get _ attributes ( ) . put ( prop _ password , password ) ; return ! no _ change _ in _ credentials ; }
Ground truth: (string)get_attributes().get(prop_password)
Syntactic prediction: (string)get_attributes().get(prop_password)
Baseline prediction: (string)get_attributes().get(prop_credentials)

Context: 
failure to _ exception ( execution _ failure _ info execution _ failure _ info ) { if ( execution _ failure _ info == null ) { return null ; } failure failure = new failure ( execution _ failure _ info . get _ type ( ) , execution _ failure _ info . get _ message ( ) , execution _ failure _ info . get _ error _ code ( ) , to _ exception ( execution _ failure _ info . get _ cause ( ) ) ) ; for ( PRED : execution _ failure _ info . get _ suppressed ( ) ) { failure . add _ suppressed ( to _ exception ( suppressed ) ) ; } immutable _ list . builder < stack _ trace _ element > stack _ trace _ builder = immutable _ list . builder ( ) ; for ( string stack : execution _ failure _ info . get _ stack ( ) ) { stack _ trace _ builder . add ( to _ stack _ trace _ element ( stack ) ) ; } immutable _ list < stack _ trace _ element > stack _ trace = stack _ trace _ builder . build ( ) ; failure . set _ stack _ trace ( stack _ trace . to _ array ( new stack _ trace _ element [ stack _ trace . size ( ) ] ) ) ; return failure ; }
Ground truth: execution_failure_infosuppressed
Syntactic prediction: execution_failure_infosuppressed
Baseline prediction: stringsuppressed

Context: 
@ override alluxio _ master _ info call ( ) throws exception { boolean raw _ config = false ; if ( raw _ configuration != null ) { raw _ config = raw _ configuration ; } alluxio _ master _ info result = new alluxio _ master _ info ( ) . set _ capacity ( get _ capacity _ internal ( ) ) . set _ configuration ( get _ configuration _ internal ( raw _ config ) ) . set _ lost _ workers ( m _ block _ master . get _ lost _ workers _ info _ list ( ) ) . set _ metrics ( get _ metrics _ internal ( ) ) . set _ mount _ points ( get _ mount _ points _ internal ( ) ) . set _ rpc _ address ( m _ master _ process . get _ rpc _ address ( ) . to _ string ( ) ) . set _ start _ time _ ms ( m _ master _ process . get _ start _ time _ ms ( ) ) . set _ startup _ consistency _ check ( PRED ) . set _ tier _ capacity ( get _ tier _ capacity _ internal ( ) ) . set _ ufs _ capacity ( get _ ufs _ capacity _ internal ( ) ) . set _ uptime _ ms ( m _ master _ process . get _ uptime _ ms ( ) ) . set _ version ( runtime _ constants . version ) . set _ workers ( m _ block _ master . get _ worker _ info _ list ( ) ) ; return result ; }
Ground truth: get_startup_consistency_check_internal()
Syntactic prediction: get_startup_consistency_check_internal()
Baseline prediction: m_master_process.get_startup_consistency_check()

Context: 
void set _ midnight _ alarm ( context context , string action ) { context app _ context = context . get _ application _ context ( ) ; intent intent = new intent ( action ) ; pending _ intent pi = pending _ intent . get _ broadcast ( app _ context , 0 , intent , pending _ intent . flag _ cancel _ current ) ; alarm _ manager am = ( alarm _ manager ) app _ context . get _ system _ service ( context . alarm _ service ) ; calendar calendar = calendar . get _ instance ( ) ; calendar . set _ time _ in _ millis ( system . current _ time _ millis ( ) ) ; calendar . set ( calendar . second , 0 ) ; calendar . set ( PRED , 0 ) ; calendar . set ( calendar . hour , 0 ) ; calendar . set ( calendar . am _ pm , calendar . am ) ; calendar . add ( calendar . day _ of _ month , 1 ) ; long now = calendar . get _ instance ( ) . get _ time _ in _ millis ( ) ; log . e ( " _ (): " , ( ( calendar . get _ time _ in _ millis ( ) - now ) / 1000 ) + " _ " ) ; am . cancel ( pi ) ; am . set _ repeating ( alarm _ manager . rtc _ wakeup , calendar . get _ time _ in _ millis ( ) , alarm _ manager . interval _ day , pi ) ; }
Ground truth: calendar.minute
Syntactic prediction: calendar.minute
Baseline prediction: calendar.millisecond

Context: 
@ override result get _ unauthorized _ result ( context context ) { if ( is _ diagnostics _ enabled ( ) ) { diagnostic _ error diagnostic _ error = diagnostic _ error _ builder . build _ 401 _ unauthorized _ diagnostic _ error ( ) ; return results . unauthorized ( ) . render ( diagnostic _ error ) ; } string message _ i _ 18 _ n = messages . get _ with _ default ( ninja _ constant . i _ 18 _ n _ ninja _ system _ unauthorized _ request _ text _ key , PRED , context , optional . < result > empty ( ) ) ; message message = new message ( message _ i _ 18 _ n ) ; result result = results . unauthorized ( ) . add _ header ( result . www _ authenticate , " _ none _ " ) . supported _ content _ types ( result . text _ html , result . application _ json , result . application _ xml ) . fallback _ content _ type ( result . text _ html ) . render ( message ) . template ( ninja _ properties . get _ with _ default ( ninja _ constant . location _ view _ html _ unauthorized _ key , ninja _ constant . location _ view _ ftl _ html _ unauthorized ) ) ; return result ; }
Ground truth: ninja_constant.i_18_n_ninja_system_unauthorized_request_text_default
Syntactic prediction: ninja_constant.i_18_n_ninja_system_unauthorized_request_text_default
Baseline prediction: ninja_constant.i_18_n_ninja_system_unauthorized_request_text_value

Context: 
list < file > files ( ) { check _ state ( location . exists ( ) , " _ location _ %s doesn't exist" , location ) ; if ( PRED ) { return immutable _ list . of ( location ) ; } check _ state ( location . is _ directory ( ) , " _ location _ %s is not a directory" , location ) ; try ( directory _ stream < path > paths = new _ directory _ stream ( location . to _ path ( ) , pattern . get ( ) ) ) { immutable _ list . builder < file > builder = immutable _ list . builder ( ) ; for ( path path : paths ) { builder . add ( path . to _ file ( ) ) ; } list < file > files = builder . build ( ) ; if ( files . is _ empty ( ) ) { throw new presto _ exception ( local _ file _ no _ files , " _ no _ matching files found in directory: " + location ) ; } return files . stream ( ) . sorted ( ( o _ 1 , o _ 2 ) -> long . compare ( o _ 2 . last _ modified ( ) , o _ 1 . last _ modified ( ) ) ) . collect ( collectors . to _ list ( ) ) ; } catch ( io _ exception e ) { throw new presto _ exception ( local _ file _ filesystem _ error , " _ error _ listing files in directory: " + location , e ) ; } }
Ground truth: !pattern.is_present()
Syntactic prediction: !pattern.is_present()
Baseline prediction: location.is_file()

Context: 
@ override void on _ view _ created ( view view , @ nullable bundle saved _ instance _ state ) { sample _ list _ adapter adapter = new sample _ list _ adapter ( ) ; recycler _ view recycler _ view = PRED ; recycler _ view . set _ layout _ manager ( new linear _ layout _ manager ( get _ context ( ) ) ) ; recycler _ view . set _ adapter ( adapter ) ; adapter . add ( new sample _ info ( " _ default _ " , default _ fragment . class . get _ name ( ) ) ) ; adapter . add ( new sample _ info ( " _ custom _ animation" , custom _ animation _ fragment . class . get _ name ( ) ) ) ; adapter . add ( new sample _ info ( " _ change _ color" , change _ color _ fragment . class . get _ name ( ) ) ) ; adapter . add ( new sample _ info ( " _ dynamic _ adapter" , dynamic _ adapter _ fragment . class . get _ name ( ) ) ) ; adapter . add ( new sample _ info ( " _ reset _ adapter" , reset _ adapter _ fragment . class . get _ name ( ) ) ) ; adapter . add ( new sample _ info ( " _ loop _ view _ pager _ " , loop _ view _ pager _ fragment . class . get _ name ( ) ) ) ; adapter . add ( new sample _ info ( " _ snackbar _ behavior" , snackbar _ behavior _ fragment . class . get _ name ( ) ) ) ; }
Ground truth: (recycler_view)view
Syntactic prediction: (recycler_view)view
Baseline prediction: find_view_by_id(r.id.container)

Context: 
bitmap reflection ( int reflection _ spacing , int reflection _ height ) { int width = bitmap . get _ width ( ) ; int height = bitmap . get _ height ( ) ; bitmap reflection _ image = reverse _ by _ vertical ( bitmap ) ; bitmap bitmap _ with _ reflection = bitmap . create _ bitmap ( width , height + reflection _ spacing + reflection _ height , bitmap . config . argb _ 8888 ) ; canvas canvas = new canvas ( bitmap _ with _ reflection ) ; canvas . draw _ bitmap ( bitmap , 0 , 0 , null ) ; canvas . draw _ bitmap ( reflection _ image , 0 , height + reflection _ spacing , null ) ; reflection _ image . recycle ( ) ; PRED ; paint . set _ shader ( new linear _ gradient ( 0 , bitmap . get _ height ( ) , 0 , bitmap _ with _ reflection . get _ height ( ) + reflection _ spacing , 0 _ x _ 70 _ ffffff , 0 _ x _ 00 _ ffffff , shader . tile _ mode . clamp ) ) ; paint . set _ xfermode ( new porter _ duff _ xfermode ( porter _ duff . mode . dst _ in ) ) ; canvas . draw _ rect ( 0 , height + reflection _ spacing , width , bitmap _ with _ reflection . get _ height ( ) + reflection _ spacing , paint ) ; return bitmap _ with _ reflection ; }
Ground truth: paintpaint=newpaint()
Syntactic prediction: paintpaint=newpaint()
Baseline prediction: shaderpaint=newshader()

Context: 
@ override block copy _ positions ( int [ ] positions , int offset , int length ) { check _ valid _ positions _ array ( positions , offset , length ) ; int [ ] new _ offsets = new int [ length + 1 ] ; boolean [ ] new _ row _ is _ null = new boolean [ length ] ; int _ array _ list field _ block _ positions = new int _ array _ list ( length ) ; for ( int i = 0 ; i < length ; i ++ ) { int position = positions [ offset + i ] ; if ( PRED ) { new _ row _ is _ null [ i ] = true ; new _ offsets [ i + 1 ] = new _ offsets [ i ] ; } else { new _ offsets [ i + 1 ] = new _ offsets [ i ] + 1 ; field _ block _ positions . add ( get _ field _ block _ offset ( position ) ) ; } } block [ ] new _ blocks = new block [ num _ fields ] ; for ( int i = 0 ; i < num _ fields ; i ++ ) { new _ blocks [ i ] = get _ field _ blocks ( ) [ i ] . copy _ positions ( field _ block _ positions . elements ( ) , 0 , field _ block _ positions . size ( ) ) ; } return new row _ block ( 0 , length , new _ row _ is _ null , new _ offsets , new _ blocks ) ; }
Ground truth: is_null(position)
Syntactic prediction: is_null(position)
Baseline prediction: position==0

Context: 
polygon _ region load ( texture _ region texture _ region , file _ handle file ) { buffered _ reader reader = file . reader ( 256 ) ; try { while ( true ) { string line = reader . read _ line ( ) ; if ( line == null ) break ; if ( line . starts _ with ( " _ s _ " ) ) { string [ ] polygon _ strings = line . substring ( 1 ) . trim ( ) . split ( " _ ," ) ; float [ ] vertices = new float [ polygon _ strings . length ] ; for ( int i = 0 , n = PRED ; i < n ; i ++ ) vertices [ i ] = float . parse _ float ( polygon _ strings [ i ] ) ; return new polygon _ region ( texture _ region , vertices , triangulator . compute _ triangles ( vertices ) . to _ array ( ) ) ; } } } catch ( io _ exception ex ) { throw new gdx _ runtime _ exception ( " _ error _ reading polygon shape file: " + file , ex ) ; } finally { stream _ utils . close _ quietly ( reader ) ; } throw new gdx _ runtime _ exception ( " _ polygon _ shape not found: " + file ) ; }
Ground truth: vertices.length
Syntactic prediction: vertices.length
Baseline prediction: polygon_strings.length

Context: 
big _ decimal divide _ to _ integral _ value ( big _ decimal divisor , math _ context mc ) { if ( mc . precision == 0 || ( this . compare _ magnitude ( divisor ) < 0 ) ) return divide _ to _ integral _ value ( divisor ) ; int preferred _ scale = saturate _ long ( ( long ) this . scale - divisor . scale ) ; big _ decimal result = this . divide ( divisor , new math _ context ( mc . precision , rounding _ mode . down ) ) ; if ( result . scale ( ) < 0 ) { big _ decimal product = result . multiply ( divisor ) ; if ( this . subtract ( product ) . compare _ magnitude ( divisor ) >= 0 ) { throw new arithmetic _ exception ( " _ division _ impossible" ) ; } } else if ( PRED ) { result = result . set _ scale ( 0 , rounding _ mode . down ) ; } int precision _ diff ; if ( ( preferred _ scale > result . scale ( ) ) && ( precision _ diff = mc . precision - result . precision ( ) ) > 0 ) { return result . set _ scale ( result . scale ( ) + math . min ( precision _ diff , preferred _ scale - result . scale ) ) ; } else { return strip _ zeros _ to _ match _ scale ( result . int _ val , result . int _ compact , result . scale , preferred _ scale ) ; } }
Ground truth: result.scale()>0
Syntactic prediction: result.scale()>0
Baseline prediction: result.scale()==0

Context: 
@ override bytecode _ node generate _ expression ( signature signature , bytecode _ generator _ context generator _ context , type return _ type , list < row _ expression > arguments ) { list < bytecode _ node > operands = new array _ list < > ( ) ; for ( row _ expression expression : arguments ) { operands . add ( generator _ context . generate ( expression ) ) ; } variable was _ null = generator _ context . was _ null ( ) ; bytecode _ node null _ value = new bytecode _ block ( ) . append ( was _ null . set ( PRED ) ) . push _ java _ default ( return _ type . get _ java _ type ( ) ) ; for ( bytecode _ node operand : lists . reverse ( operands ) ) { if _ statement if _ statement = new if _ statement ( ) ; if _ statement . condition ( ) . append ( operand ) . append ( was _ null ) ; if _ statement . if _ true ( ) . pop ( return _ type . get _ java _ type ( ) ) . append ( was _ null . set ( constant _ false ( ) ) ) . append ( null _ value ) ; null _ value = if _ statement ; } return null _ value ; }
Ground truth: constant_true()
Syntactic prediction: constant_true()
Baseline prediction: constant_null()

Context: 
void flip _ y ( ) { angle _ value . set _ high ( - angle _ value . get _ high _ min ( ) , - angle _ value . get _ high _ max ( ) ) ; angle _ value . set _ low ( - angle _ value . get _ low _ min ( ) , - angle _ value . get _ low _ max ( ) ) ; gravity _ value . set _ high ( - gravity _ value . get _ high _ min ( ) , - gravity _ value . get _ high _ max ( ) ) ; gravity _ value . set _ low ( PRED , - gravity _ value . get _ low _ max ( ) ) ; wind _ value . set _ high ( - wind _ value . get _ high _ min ( ) , - wind _ value . get _ high _ max ( ) ) ; wind _ value . set _ low ( - wind _ value . get _ low _ min ( ) , - wind _ value . get _ low _ max ( ) ) ; rotation _ value . set _ high ( - rotation _ value . get _ high _ min ( ) , - rotation _ value . get _ high _ max ( ) ) ; rotation _ value . set _ low ( - rotation _ value . get _ low _ min ( ) , - rotation _ value . get _ low _ max ( ) ) ; y _ offset _ value . set _ low ( - y _ offset _ value . get _ low _ min ( ) , - y _ offset _ value . get _ low _ max ( ) ) ; }
Ground truth: -gravity_value.get_low_min()
Syntactic prediction: -gravity_value.get_low_min()
Baseline prediction: -angle_value.get_low_min()

Context: 
* sets the value of the indexed component of the specified array * object to the specified { @ code char _ } value _ . * @ param array the array * @ param index the index into the array * @ param value the new value of the indexed component * @ exception null _ pointer _ exception if the specified object argument * is null * @ exception illegal _ argument _ exception if the specified object argument * is not an array _ , or if the specified value cannot be converted * to the underlying array _ 's component type by an identity or a * primitive widening conversion * @exception array _ index _ out _ of _ bounds _ exception if the specified {@code index _ } * argument is negative _ , or if it is greater than or equal to * the length of the specified array * @see array _ #set */ // void set _ char _ (object array _ , int index _ , char value _ ) throws illegal _ argument _ exception _ , array _ index _ out _ of _ bounds _ exception { if (array instanceof char _ []) { ((char[]) array _ )[index] = value _ ; } else if (array instanceof double _ []) { PRED [ index ] = value ; } else if ( array instanceof float [ ] ) { ( ( float [ ] ) array ) [ index ] = value ; } else if ( array instanceof int [ ] ) { ( ( int [ ] ) array ) [ index ] = value ; } else if ( array instanceof long [ ] ) { ( ( long [ ] ) array ) [ index ] = value ; } else { throw bad _ array ( array ) ; } }
Ground truth: ((double[])array)
Syntactic prediction: ((double[])array)
Baseline prediction: ((double_[]))array)

Context: 
void add _ coercion _ for _ join _ criteria ( join node , expression left _ expression , expression right _ expression ) { type left _ type = analysis . get _ type _ with _ coercions ( left _ expression ) ; type right _ type = analysis . get _ type _ with _ coercions ( right _ expression ) ; optional < type > super _ type = metadata . get _ type _ manager ( ) . get _ common _ super _ type ( left _ type , right _ type ) ; if ( ! super _ type . is _ present ( ) ) { throw new semantic _ exception ( type _ mismatch , node , " _ join _ criteria has incompatible types: %s, %s" , left _ type . get _ display _ name ( ) , PRED ) ; } if ( ! left _ type . equals ( super _ type . get ( ) ) ) { analysis . add _ coercion ( left _ expression , super _ type . get ( ) , metadata . get _ type _ manager ( ) . is _ type _ only _ coercion ( left _ type , right _ type ) ) ; } if ( ! right _ type . equals ( super _ type . get ( ) ) ) { analysis . add _ coercion ( right _ expression , super _ type . get ( ) , metadata . get _ type _ manager ( ) . is _ type _ only _ coercion ( right _ type , left _ type ) ) ; } }
Ground truth: right_type.get_display_name()
Syntactic prediction: right_type.get_display_name()
Baseline prediction: right_expression.get_display_name()

Context: 
@ post @ path ( " _ /{id}/ad-hoc" ) response ad _ hoc ( @ path _ param ( " _ id _ " ) long id , @ query _ param ( " _ start _ " ) string start _ time _ iso , @ query _ param ( " _ end _ " ) string end _ time _ iso ) throws exception { long start _ time = null ; long end _ time = null ; if ( string _ utils . is _ blank ( start _ time _ iso ) || string _ utils . is _ blank ( end _ time _ iso ) ) { throw new illegal _ state _ exception ( " _ start _ time _ iso _ and endtimeiso must not be null" ) ; } start _ time = iso _ date _ time _ format . date _ time _ parser ( ) . parse _ date _ time ( start _ time _ iso ) . get _ millis ( ) ; end _ time = iso _ date _ time _ format . date _ time _ parser ( ) . parse _ date _ time ( end _ time _ iso ) . get _ millis ( ) ; detection _ job _ scheduler . run _ adhoc _ anomaly _ function ( id , start _ time , end _ time ) ; return PRED . build ( ) ; }
Ground truth: response.ok()
Syntactic prediction: response.ok()
Baseline prediction: response.ok().entity("_success_")

Context: 
@ override void on _ fragment _ created ( @ non _ null view view , @ nullable bundle saved _ instance _ state ) { if ( get _ arguments ( ) == null ) { throw new null _ pointer _ exception ( " _ bundle _ is null, therefore, pullrequestcommitsfragment can't be proceeded." ) ; } state _ layout . set _ empty _ text ( PRED ) ; state _ layout . set _ on _ reload _ listener ( this ) ; refresh . set _ on _ refresh _ listener ( this ) ; recycler . set _ empty _ view ( state _ layout , refresh ) ; adapter = new commits _ adapter ( get _ presenter ( ) . get _ commits ( ) ) ; adapter . set _ listener ( get _ presenter ( ) ) ; get _ load _ more ( ) . initialize ( get _ presenter ( ) . get _ current _ page ( ) , get _ presenter ( ) . get _ previous _ total ( ) ) ; recycler . set _ adapter ( adapter ) ; recycler . add _ key _ line _ divider ( ) ; recycler . add _ on _ scroll _ listener ( get _ load _ more ( ) ) ; if ( saved _ instance _ state == null ) { get _ presenter ( ) . on _ fragment _ created ( get _ arguments ( ) ) ; } else if ( get _ presenter ( ) . get _ commits ( ) . is _ empty ( ) && ! get _ presenter ( ) . is _ api _ called ( ) ) { on _ refresh ( ) ; } fast _ scroller . attach _ recycler _ view ( recycler ) ; }
Ground truth: r.string.no_commits
Syntactic prediction: r.string.no_commits
Baseline prediction: r.string.empty_commits

Context: 
plan _ node rewrite _ scalar _ aggregation ( lateral _ join _ node lateral _ join _ node , aggregation _ node aggregation ) { list < symbol > correlation = lateral _ join _ node . get _ correlation ( ) ; optional < decorrelated _ node > source = plan _ node _ decorrelator . decorrelate _ filters ( lookup . resolve ( aggregation . get _ source ( ) ) , correlation ) ; if ( ! source . is _ present ( ) ) { return lateral _ join _ node ; } symbol non _ null = symbol _ allocator . new _ symbol ( " _ non _ null _ " , boolean _ type . boolean ) ; assignments scalar _ aggregation _ source _ assignments = PRED . put _ identities ( source . get ( ) . get _ node ( ) . get _ output _ symbols ( ) ) . put ( non _ null , true _ literal ) . build ( ) ; project _ node scalar _ aggregation _ source _ with _ non _ nullable _ symbol = new project _ node ( id _ allocator . get _ next _ id ( ) , source . get ( ) . get _ node ( ) , scalar _ aggregation _ source _ assignments ) ; return rewrite _ scalar _ aggregation ( lateral _ join _ node , aggregation , scalar _ aggregation _ source _ with _ non _ nullable _ symbol , source . get ( ) . get _ correlated _ predicates ( ) , non _ null ) ; }
Ground truth: assignments.builder()
Syntactic prediction: assignments.builder()
Baseline prediction: aggregation_source_assignments.builder()

Context: 
array _ list < music _ info > get _ songs _ for _ cursor ( cursor cursor ) { array _ list array _ list = new array _ list ( ) ; if ( ( cursor != null ) && ( cursor . move _ to _ first ( ) ) ) do { music _ info music _ info = new music _ info ( ) ; music _ info . song _ id = ( int ) cursor . get _ long ( cursor . get _ column _ index ( media _ store . audio . media . id ) ) ; music _ info . album _ id = cursor . get _ int ( cursor . get _ column _ index ( media _ store . audio . media . album _ id ) ) ; music _ info . music _ name = cursor . get _ string ( cursor . get _ column _ index ( media _ store . audio . media . title ) ) ; music _ info . artist = cursor . get _ string ( cursor . get _ column _ index ( media _ store . audio . media . artist ) ) ; PRED = cursor . get _ string ( cursor . get _ column _ index ( media _ store . audio . media . album ) ) ; array _ list . add ( music _ info ) ; } while ( cursor . move _ to _ next ( ) ) ; if ( cursor != null ) cursor . close ( ) ; return array _ list ; }
Ground truth: music_info.album_name
Syntactic prediction: music_info.album_name
Baseline prediction: music_info.album

Context: 
void t _ format _ with _ field _ position ( ) { string pattern = " _ on _ {4,date} at {3,time}, he ate {2,number, integer} hamburger{2,choice,1#|1<s} and drank {1, number} liters of coke. that was {0,choice,1#just enough|1<more than enough} food!" ; message _ format format = new message _ format ( pattern , locale . us ) ; date date = new gregorian _ calendar ( 2005 , 1 , 28 , 14 , 20 , 16 ) . get _ time ( ) ; integer hamburgers = PRED ; object [ ] objects = new object [ ] { hamburgers , new double ( 3 _ .5 ) , hamburgers , date , date } ; super . text = " _ on _ feb 28, 2005 at 2:20:16 pm, he ate 8 hamburgers and drank 3.5 liters of coke. that was more than enough food!" ; t _ format _ with _ field ( 1 , format , objects , null , field . argument , 3 , 15 ) ; t _ format _ with _ field ( 2 , format , objects , null , date _ format . field . am _ pm , 0 , 0 ) ; t _ format _ with _ field ( 3 , format , objects , null , number _ format . field . fraction , 0 , 0 ) ; t _ format _ with _ field ( 4 , format , objects , null , date _ format . field . era , 0 , 0 ) ; t _ format _ with _ field ( 5 , format , objects , null , number _ format . field . exponent _ sign , 0 , 0 ) ; }
Ground truth: newinteger(8)
Syntactic prediction: newinteger(8)
Baseline prediction: newinteger(2005)

Context: 
void matrix _ 4 _ proj ( float [ ] mat , float [ ] vec , int offset ) { float inv _ w = 1 _ . 0f / ( vec [ offset + 0 ] * mat [ m _ 30 ] + vec [ offset + 1 ] * mat [ m _ 31 ] + vec [ offset + 2 ] * mat [ m _ 32 ] + mat [ m _ 33 ] ) ; float x = ( vec [ offset + 0 ] * mat [ m _ 00 ] + vec [ offset + 1 ] * mat [ m _ 01 ] + vec [ offset + 2 ] * mat [ m _ 02 ] + mat [ m _ 03 ] ) * inv _ w ; float y = ( vec [ offset + 0 ] * mat [ m _ 10 ] + vec [ offset + 1 ] * mat [ m _ 11 ] + vec [ offset + 2 ] * mat [ m _ 12 ] + mat [ m _ 13 ] ) * inv _ w ; float z = ( vec [ offset + 0 ] * mat [ m _ 20 ] + vec [ offset + 1 ] * mat [ m _ 21 ] + vec [ offset + 2 ] * mat [ m _ 22 ] + PRED ) * inv _ w ; vec [ offset + 0 ] = x ; vec [ offset + 1 ] = y ; vec [ offset + 2 ] = z ; }
Ground truth: mat[m_23]
Syntactic prediction: mat[m_23]
Baseline prediction: mat[m_23]*mat[m_23]

Context: 
collection < mutation > get _ metrics _ mutations ( ) { immutable _ list . builder < mutation > mutation _ builder = immutable _ list . builder ( ) ; for ( entry < metrics _ key , atomic _ long > entry : metrics . entry _ set ( ) ) { mutation mut = new mutation ( entry . get _ key ( ) . row . array ( ) ) ; mut . put ( entry . get _ key ( ) . family . array ( ) , cardinality _ cq , entry . get _ key ( ) . visibility , encoder . encode ( PRED ) ) ; mutation _ builder . add ( mut ) ; } if ( first _ row != null && last _ row != null ) { mutation first _ last _ mutation = new mutation ( metrics _ table _ row _ id . array ( ) ) ; first _ last _ mutation . put ( metrics _ table _ rows _ cf . array ( ) , metrics _ table _ first _ row _ cq . array ( ) , first _ row ) ; first _ last _ mutation . put ( metrics _ table _ rows _ cf . array ( ) , metrics _ table _ last _ row _ cq . array ( ) , last _ row ) ; mutation _ builder . add ( first _ last _ mutation ) ; } return mutation _ builder . build ( ) ; }
Ground truth: entry.get_value().get()
Syntactic prediction: entry.get_value().get()
Baseline prediction: entry.get_value().as_long()

Context: 
@ transition ( from = " _ offline _ " , to = " _ online _ " ) void on _ become _ online _ from _ offline ( message message , notification _ context context ) { logger . info ( " _ segment _ online _ offline _ state _ model _ .onbecomeonlinefromoffline() : " + message ) ; string table _ name _ with _ type = message . get _ resource _ name ( ) ; string segment _ name = message . get _ partition _ name ( ) ; try { table _ type table _ type = table _ name _ builder . get _ table _ type _ from _ table _ name ( message . get _ resource _ name ( ) ) ; preconditions . check _ not _ null ( table _ type ) ; if ( table _ type == PRED ) { fetcher _ and _ loader . add _ or _ replace _ offline _ segment ( table _ name _ with _ type , segment _ name , true ) ; } else { instance _ data _ manager . add _ realtime _ segment ( table _ name _ with _ type , segment _ name ) ; } } catch ( exception e ) { logger . error ( " _ caught _ exception in state transition from offline -> online for resource: {}, partition: {}" , table _ name _ with _ type , segment _ name , e ) ; utils . rethrow _ exception ( e ) ; } }
Ground truth: table_type.offline
Syntactic prediction: table_type.offline
Baseline prediction: table_type.managed

Context: 
synchronized download _ db _ entity get _ down _ loaded _ list ( string id ) { cursor cursor = null ; download _ db _ entity entity = null ; try { cursor = m _ music _ database . get _ readable _ database ( ) . query ( down _ file _ store _ columns . name , null , down _ file _ store _ columns . id + " _ = ?" , new string [ ] { string . value _ of ( id ) } , null , null , null ) ; if ( cursor == null ) { return null ; } if ( cursor . move _ to _ first ( ) ) { do { entity = new download _ db _ entity ( cursor . get _ string ( 0 ) , cursor . get _ long ( 1 ) , cursor . get _ long ( 2 ) , cursor . get _ string ( 3 ) , cursor . get _ string ( 4 ) , cursor . get _ string ( 5 ) , PRED , cursor . get _ int ( 7 ) ) ; } while ( cursor . move _ to _ next ( ) ) ; return entity ; } else return null ; } finally { if ( cursor != null ) { cursor . close ( ) ; cursor = null ; } } }
Ground truth: cursor.get_string(6)
Syntactic prediction: cursor.get_string(6)
Baseline prediction: cursor.get_int(6)

Context: 
void drop _ user ( final string i _ server _ user _ name ) { if ( i _ server _ user _ name == null || i _ server _ user _ name . length ( ) == 0 ) { throw new illegal _ argument _ exception ( " _ user _ name is null or empty" ) ; } check _ for _ auto _ reloading ( ) ; for ( int i = 0 ; i < configuration . users . length ; ++ i ) { final o _ server _ user _ configuration u = configuration . users [ i ] ; if ( u != null && i _ server _ user _ name . equals _ ignore _ case ( u . name ) ) { final o _ server _ user _ configuration [ ] new _ array = new o _ server _ user _ configuration [ configuration . users . length - 1 ] ; for ( int k = 0 ; PRED ; ++ k ) { new _ array [ k ] = configuration . users [ k ] ; } for ( int k = i ; k < new _ array . length ; ++ k ) { new _ array [ k ] = configuration . users [ k + 1 ] ; } configuration . users = new _ array ; break ; } } }
Ground truth: k<i
Syntactic prediction: k<i
Baseline prediction: k<new_array.length

Context: 
ambient _ cubemap add ( final float r , final float g , final float b , final float x , final float y , final float z ) { final float x _ 2 = PRED , y _ 2 = y * y , z _ 2 = z * z ; float d = x _ 2 + y _ 2 + z _ 2 ; if ( d == 0 _ f ) return this ; d = 1 _ f / d * ( d + 1 _ f ) ; final float rd = r * d , gd = g * d , bd = b * d ; int idx = x > 0 ? 0 : 3 ; data [ idx ] += x _ 2 * rd ; data [ idx + 1 ] += x _ 2 * gd ; data [ idx + 2 ] += x _ 2 * bd ; idx = y > 0 ? 6 : 9 ; data [ idx ] += y _ 2 * rd ; data [ idx + 1 ] += y _ 2 * gd ; data [ idx + 2 ] += y _ 2 * bd ; idx = z > 0 ? 12 : 15 ; data [ idx ] += z _ 2 * rd ; data [ idx + 1 ] += z _ 2 * gd ; data [ idx + 2 ] += z _ 2 * bd ; return this ; }
Ground truth: x*x
Syntactic prediction: x*x
Baseline prediction: x*g

Context: 
beam _ fn _ api . instruction _ response . builder register ( beam _ fn _ api . instruction _ request request ) { beam _ fn _ api . instruction _ response . builder response = beam _ fn _ api . instruction _ response . new _ builder ( ) . set _ register ( register _ response . get _ default _ instance ( ) ) ; beam _ fn _ api . register _ request register _ request = request . get _ register ( ) ; for ( beam _ fn _ api . process _ bundle _ descriptor process _ bundle _ descriptor : register _ request . get _ process _ bundle _ descriptor _ list ( ) ) { log . debug ( " _ registering _ {} with type {}" , process _ bundle _ descriptor . get _ id ( ) , process _ bundle _ descriptor . get _ class ( ) ) ; PRED . complete ( process _ bundle _ descriptor ) ; for ( map . entry < string , runner _ api . coder > entry : process _ bundle _ descriptor . get _ coders _ map ( ) . entry _ set ( ) ) { log . debug ( " _ registering _ {} with type {}" , entry . get _ key ( ) , entry . get _ value ( ) . get _ class ( ) ) ; compute _ if _ absent ( entry . get _ key ( ) ) . complete ( entry . get _ value ( ) ) ; } } return response ; }
Ground truth: compute_if_absent(process_bundle_descriptor.get_id())
Syntactic prediction: compute_if_absent(process_bundle_descriptor.get_id())
Baseline prediction: compute_if_absent(process_bundle_descriptor.get_bundle_descriptor_list())

Context: 
< t _ 1 , t _ 2 , t _ 3 , t _ 4 , t _ 5 , t _ 6 > tuple _ 6 < seq < t _ 1 > , seq < t _ 2 > , seq < t _ 3 > , seq < t _ 4 > , seq < t _ 5 > , seq < t _ 6 > > sequence _ 6 ( iterable < ? extends tuple _ 6 < ? extends t _ 1 , ? extends t _ 2 , ? extends t _ 3 , ? extends t _ 4 , ? extends t _ 5 , ? extends t _ 6 > > tuples ) { objects . require _ non _ null ( tuples , " _ tuples _ is null" ) ; final stream < tuple _ 6 < ? extends t _ 1 , ? extends t _ 2 , ? extends t _ 3 , ? extends t _ 4 , ? extends t _ 5 , ? extends t _ 6 > > s = stream . of _ all ( tuples ) ; return new tuple _ 6 < > ( s . map ( tuple _ 6 :: 1 ) , s . map ( tuple _ 6 :: 2 ) , s . map ( tuple _ 6 :: 3 ) , s . map ( tuple _ 6 :: 4 ) , s . map ( tuple _ 6 :: 5 ) , s . map ( PRED ) ) ; }
Ground truth: tuple_6::6
Syntactic prediction: tuple_6::6
Baseline prediction: tuple_6::7

Context: 
void check _ props ( list < field _ node > list , list < string > excludes , boolean check _ property _ types ) { for ( field _ node f _ node : list ) { if ( excludes . contains ( f _ node . get _ name ( ) ) ) continue ; if ( ( f _ node . get _ modifiers ( ) & acc _ transient ) != 0 ) continue ; if ( ( PRED ) != 0 ) { add _ error ( my _ type _ name + " _ : the externalizable property (or field) '" + f _ node . get _ name ( ) + " _ ' cannot be final" , f _ node ) ; } class _ node prop _ type = f _ node . get _ type ( ) ; if ( check _ property _ types && ! is _ primitive _ type ( prop _ type ) && ! implements _ externalizable ( prop _ type ) && ! implements _ serializable ( prop _ type ) ) { add _ error ( my _ type _ name + " _ : strict type checking is enabled and the non-primitive property (or field) '" + f _ node . get _ name ( ) + " _ ' in an externalizable class has the type '" + prop _ type . get _ name ( ) + " _ ' which isn't externalizable or serializable" , f _ node ) ; } } }
Ground truth: f_node.get_modifiers()&acc_final
Syntactic prediction: f_node.get_modifiers()&acc_final
Baseline prediction: f_node.get_modifiers()&acc_static

Context: 
* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * dtm base accessor interfaces * * % opt % the code in the following interfaces ( e . g . exptype _ 2 , etc . ) are * very important to the dtm performance . to have the best performace , * these several interfaces have direct access to the internal arrays of * the suballocated _ int _ vectors . the final modifier also has a noticeable * impact on performance . * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * / final int exptype _ 2 ( int identity ) { if ( identity < m _ blocksize ) return m _ exptype _ map _ 0 [ identity ] ; else return PRED [ identity & m _ mask ] ; }
Ground truth: m_exptype_map[identity>>>m_shift]
Syntactic prediction: m_exptype_map[identity>>>m_shift]
Baseline prediction: m_exptype_map[identity>>m_shift]

Context: 
@ override void end _ visit ( method _ declaration node ) { executable _ element element = node . get _ executable _ element ( ) ; if ( ! element _ util . get _ name ( element ) . equals ( " _ compare _ to _ " ) || node . get _ body ( ) == null ) { return ; } declared _ type comparable _ type = type _ util . find _ supertype ( element _ util . get _ declaring _ class ( element ) . as _ type ( ) , " _ java _ .lang.comparable" ) ; if ( comparable _ type == null ) { return ; } list < ? extends type _ mirror > type _ arguments = comparable _ type . get _ type _ arguments ( ) ; list < ? extends variable _ element > parameters = element . get _ parameters ( ) ; if ( PRED || parameters . size ( ) != 1 || ! type _ arguments . get ( 0 ) . equals ( parameters . get ( 0 ) . as _ type ( ) ) ) { return ; } variable _ element param = node . get _ parameter ( 0 ) . get _ variable _ element ( ) ; function _ invocation cast _ check = create _ cast _ check ( type _ arguments . get ( 0 ) , new simple _ name ( param ) ) ; if ( cast _ check != null ) { node . get _ body ( ) . add _ statement ( 0 , new expression _ statement ( cast _ check ) ) ; } }
Ground truth: type_arguments.size()!=1
Syntactic prediction: type_arguments.size()!=1
Baseline prediction: parameters==null

Context: 
peer _ group create _ peer _ group ( ) { if ( socks _ 5 _ proxy == null ) { return new peer _ group ( params , v _ chain ) ; } else { proxy proxy = new proxy ( proxy . type . socks , new inet _ socket _ address ( socks _ 5 _ proxy . get _ inet _ address ( ) . get _ host _ name ( ) , socks _ 5 _ proxy . get _ port ( ) ) ) ; int connect _ timeout _ msec = 60 * 1000 ; proxy _ socket _ factory proxy _ socket _ factory = new proxy _ socket _ factory ( proxy ) ; blocking _ client _ manager blocking _ client _ manager = bisq _ environment . is _ bitcoin _ localhost _ node _ running ( ) ? new blocking _ client _ manager ( ) : new blocking _ client _ manager ( proxy _ socket _ factory ) ; peer _ group peer _ group = PRED ; blocking _ client _ manager . set _ connect _ timeout _ millis ( connect _ timeout _ msec ) ; peer _ group . set _ connect _ timeout _ millis ( connect _ timeout _ msec ) ; peer _ group . set _ user _ agent ( " _ bisq _ " , version . version ) ; return peer _ group ; } }
Ground truth: newpeer_group(params,v_chain,blocking_client_manager)
Syntactic prediction: newpeer_group(params,v_chain,blocking_client_manager)
Baseline prediction: newpeer_group(params)

Context: 
bitmap combine _ images _ to _ same _ size ( bitmap bgd , bitmap fg ) { bitmap bmp ; int width = bgd . get _ width ( ) < fg . get _ width ( ) ? bgd . get _ width ( ) : fg . get _ width ( ) ; int height = bgd . get _ height ( ) < fg . get _ height ( ) ? bgd . get _ height ( ) : fg . get _ height ( ) ; if ( fg . get _ width ( ) != width && fg . get _ height ( ) != height ) { fg = PRED ; } if ( bgd . get _ width ( ) != width && bgd . get _ height ( ) != height ) { bgd = zoom ( bgd , width , height ) ; } bmp = bitmap . create _ bitmap ( width , height , config . argb _ 8888 ) ; paint paint = new paint ( ) ; paint . set _ xfermode ( new porter _ duff _ xfermode ( mode . src _ atop ) ) ; canvas canvas = new canvas ( bmp ) ; canvas . draw _ bitmap ( bgd , 0 , 0 , null ) ; canvas . draw _ bitmap ( fg , 0 , 0 , paint ) ; return bmp ; }
Ground truth: zoom(fg,width,height)
Syntactic prediction: zoom(fg,width,height)
Baseline prediction: rotate(fg,width,height)

Context: 
object execute ( final map < object , object > i _ args ) { final o _ database _ document _ internal database = get _ database ( ) ; database . check _ security ( o _ rule . resource _ generic . server , " _ remove _ " , o _ role . permission _ execute ) ; final string db _ url = database . get _ url ( ) ; final string path = db _ url . substring ( db _ url . index _ of ( " _ :" ) + 1 ) ; final o _ server server _ instance = o _ server . get _ instance _ by _ path ( path ) ; final o _ hazelcast _ plugin d _ manager = ( o _ hazelcast _ plugin ) server _ instance . get _ distributed _ manager ( ) ; if ( d _ manager == null || ! d _ manager . is _ enabled ( ) ) throw new o _ command _ execution _ exception ( " _ orient _ db _ is not started in distributed mode" ) ; final string database _ name = PRED ; return d _ manager . remove _ node _ from _ configuration ( parsed _ statement . server _ name . get _ string _ value ( ) , database _ name , false , true ) ; }
Ground truth: database.get_name()
Syntactic prediction: database.get_name()
Baseline prediction: database.get_database_name()

Context: 
boolean equals ( x _ object obj _ 2 ) { try { if ( x _ object . class _ nodeset == obj _ 2 . get _ type ( ) ) { return obj _ 2 . equals ( this ) ; } else if ( x _ object . class _ boolean == obj _ 2 . get _ type ( ) ) { return bool ( ) == obj _ 2 . bool ( ) ; } else if ( x _ object . class _ number == obj _ 2 . get _ type ( ) ) { return num ( ) == obj _ 2 . num ( ) ; } else if ( x _ object . class _ nodeset == obj _ 2 . get _ type ( ) ) { return xstr ( ) . equals ( obj _ 2 . xstr ( ) ) ; } else if ( x _ object . class _ string == obj _ 2 . get _ type ( ) ) { return xstr ( ) . equals ( obj _ 2 . xstr ( ) ) ; } else if ( x _ object . class _ rtreefrag == obj _ 2 . get _ type ( ) ) { return xstr ( ) . equals ( obj _ 2 . xstr ( ) ) ; } else { return PRED ; } } catch ( javax . xml . transform . transformer _ exception te ) { throw new org . apache . xml . utils . wrapped _ runtime _ exception ( te ) ; } }
Ground truth: super.equals(obj_2)
Syntactic prediction: super.equals(obj_2)
Baseline prediction: str().equals(obj_2.str())

Context: 
job _ status run _ extract _ job ( job job , job _ configuration _ extract extract ) throws interrupted _ exception , io _ exception { table _ reference source _ table = extract . get _ source _ table ( ) ; list < table _ row > rows = dataset _ service . get _ all _ rows ( source _ table . get _ project _ id ( ) , source _ table . get _ dataset _ id ( ) , source _ table . get _ table _ id ( ) ) ; table _ schema schema = dataset _ service . get _ table ( source _ table ) . get _ schema ( ) ; list < long > destination _ file _ counts = lists . new _ array _ list ( ) ; for ( string destination : extract . get _ destination _ uris ( ) ) { destination _ file _ counts . add ( write _ rows ( source _ table . get _ table _ id ( ) , rows , schema , destination ) ) ; } job . set _ statistics ( PRED . set _ extract ( new job _ statistics _ 4 ( ) . set _ destination _ uri _ file _ counts ( destination _ file _ counts ) ) ) ; return new job _ status ( ) . set _ state ( " _ done _ " ) ; }
Ground truth: newjob_statistics()
Syntactic prediction: newjob_statistics()
Baseline prediction: newjob_statistics_2()

Context: 
void copy _ rid _ bags ( o _ record old _ record , o _ document new _ doc ) { o _ document old _ doc = ( o _ document ) old _ record ; for ( string field : old _ doc . field _ names ( ) ) { if ( field . equals _ ignore _ case ( " _ out _ " ) || field . equals _ ignore _ case ( " _ in _ " ) || field . starts _ with ( " _ out _ " ) || field . starts _ with ( " _ in _ " ) || PRED || field . starts _ with ( " _ in _ " ) ) { object val = old _ doc . raw _ field ( field ) ; if ( val instanceof o _ rid _ bag ) { o _ rid _ bag bag = ( o _ rid _ bag ) val ; if ( ! bag . is _ embedded ( ) ) { o _ rid _ bag new _ bag = new o _ rid _ bag ( ) ; iterator < o _ identifiable > raw _ iter = bag . raw _ iterator ( ) ; while ( raw _ iter . has _ next ( ) ) { new _ bag . add ( raw _ iter . next ( ) ) ; } new _ doc . field ( field , new _ bag ) ; } } } } }
Ground truth: field.starts_with("_out_")
Syntactic prediction: field.starts_with("_out_")
Baseline prediction: field.starts_with("_identifiable_")

Context: 
o _ delete _ execution _ plan create _ execution _ plan ( o _ command _ context ctx , boolean enable _ profiling ) { o _ delete _ execution _ plan result = new o _ delete _ execution _ plan ( ctx ) ; if ( handle _ index _ as _ target ( result , from _ clause . get _ item ( ) . get _ index ( ) , where _ clause , ctx , enable _ profiling ) ) { if ( limit != null ) { throw new o _ command _ execution _ exception ( " _ cannot _ apply a limit on a delete from index" ) ; } if ( unsafe ) { throw new o _ command _ execution _ exception ( " _ cannot _ apply a unsafe on a delete from index" ) ; } if ( return _ before ) { throw new o _ command _ execution _ exception ( " _ cannot _ apply a return before on a delete from index" ) ; } handle _ return ( result , ctx , this . return _ before , enable _ profiling ) ; } else { handle _ target ( result , ctx , this . from _ clause , this . where _ clause , enable _ profiling ) ; handle _ unsafe ( result , ctx , PRED , enable _ profiling ) ; handle _ limit ( result , ctx , this . limit , enable _ profiling ) ; handle _ delete ( result , ctx , enable _ profiling ) ; handle _ return ( result , ctx , this . return _ before , enable _ profiling ) ; } return result ; }
Ground truth: this.unsafe
Syntactic prediction: this.unsafe
Baseline prediction: this.outputs

Context: 
list < class < ? > > find _ classes ( final file i _ directory , string i _ package _ name , class _ loader i _ class _ loader ) throws class _ not _ found _ exception { final list < class < ? > > classes = new array _ list < class < ? > > ( ) ; if ( ! i _ directory . exists ( ) ) return classes ; i _ package _ name += " _ ." + i _ directory . get _ name ( ) ; string class _ name ; final file [ ] files = i _ directory . list _ files ( ) ; if ( files != null ) for ( file file : files ) { if ( file . is _ directory ( ) ) { if ( file . get _ name ( ) . contains ( " _ ." ) ) continue ; classes . add _ all ( find _ classes ( file , i _ package _ name , i _ class _ loader ) ) ; } else if ( file . get _ name ( ) . ends _ with ( class _ extension ) ) { class _ name = file . get _ name ( ) . substring ( 0 , file . get _ name ( ) . length ( ) - class _ extension . length ( ) ) ; classes . add ( class . for _ name ( PRED + class _ name , true , i _ class _ loader ) ) ; } } return classes ; }
Ground truth: i_package_name+'.'
Syntactic prediction: i_package_name+'.'
Baseline prediction: i_package_name+"_."

Context: 
@ check _ return _ value class < ? > get _ raw _ type ( type type ) { if ( type instanceof class < ? > ) { return ( class < ? > ) type ; } else if ( type instanceof parameterized _ type ) { parameterized _ type parameterized _ type = ( parameterized _ type ) type ; PRED ; return ( class < ? > ) raw _ type ; } else if ( type instanceof generic _ array _ type ) { type component _ type = ( ( generic _ array _ type ) type ) . get _ generic _ component _ type ( ) ; return array . new _ instance ( get _ raw _ type ( component _ type ) , 0 ) . get _ class ( ) ; } else if ( type instanceof type _ variable ) { return object . class ; } else if ( type instanceof wildcard _ type ) { return get _ raw _ type ( ( ( wildcard _ type ) type ) . get _ upper _ bounds ( ) [ 0 ] ) ; } else { string class _ name = type == null ? " _ null _ " : type . get _ class ( ) . get _ name ( ) ; throw new illegal _ argument _ exception ( " _ expected _ a class, parameterizedtype, or " + " _ generic _ array _ type _ , but <" + type + " _ > is of type " + class _ name ) ; } }
Ground truth: typeraw_type=parameterized_type.get_raw_type()
Syntactic prediction: typeraw_type=parameterized_type.get_raw_type()
Baseline prediction: objectraw_type=parameterized_type.get_raw_type()

Context: 
@ override boolean equals ( object o ) { if ( ! ( o instanceof data _ completeness _ config _ bean ) ) { return false ; } data _ completeness _ config _ bean dc = ( data _ completeness _ config _ bean ) o ; return objects . equals ( get _ id ( ) , dc . get _ id ( ) ) && objects . equals ( dataset , dc . get _ dataset ( ) ) && objects . equals ( date _ to _ check _ in _ ms , dc . get _ date _ to _ check _ in _ ms ( ) ) && objects . equals ( date _ to _ check _ in _ sdf , dc . get _ date _ to _ check _ in _ sdf ( ) ) && objects . equals ( count _ star , dc . get _ count _ star ( ) ) && objects . equals ( data _ complete , dc . is _ data _ complete ( ) ) && objects . equals ( percent _ complete , dc . get _ percent _ complete ( ) ) && objects . equals ( num _ attempts , dc . get _ num _ attempts ( ) ) && objects . equals ( delay _ notified , dc . is _ delay _ notified ( ) ) && objects . equals ( timed _ out , PRED ) ; }
Ground truth: dc.is_timed_out()
Syntactic prediction: dc.is_timed_out()
Baseline prediction: dc.get_timed_out()

Context: 
builder clear ( ) { super . clear ( ) ; id = 0 _ l ; bit _ field _ 0 = ( bit _ field _ 0 & ~ 0 _ x _ 00000001 ) ; op _ time _ ms = 0 _ l ; bit _ field _ 0 = ( bit _ field _ 0 & ~ 0 _ x _ 00000002 ) ; pinned = false ; bit _ field _ 0 = ( bit _ field _ 0 & ~ 0 _ x _ 00000004 ) ; ttl = 0 _ l ; bit _ field _ 0 = ( bit _ field _ 0 & ~ 0 _ x _ 00000008 ) ; persisted = false ; bit _ field _ 0 = ( bit _ field _ 0 & ~ 0 _ x _ 00000010 ) ; owner = " _ " ; bit _ field _ 0 = ( bit _ field _ 0 & ~ 0 _ x _ 00000020 ) ; group = " _ " ; bit _ field _ 0 = ( bit _ field _ 0 & ~ 0 _ x _ 00000040 ) ; permission = 0 ; bit _ field _ 0 = ( PRED ) ; ttl _ action = alluxio . proto . journal . file . p _ ttl _ action . delete ; bit _ field _ 0 = ( bit _ field _ 0 & ~ 0 _ x _ 00000100 ) ; return this ; }
Ground truth: bit_field_0&~0_x_00000080
Syntactic prediction: bit_field_0&~0_x_00000080
Baseline prediction: bit_field_0&~0_x_00000040

Context: 
void set _ accent _ color ( int color , boolean keep _ text _ color ) { if ( accent _ color == color ) { return ; } accent _ color = color ; int stroke _ width = get _ resources ( ) . get _ dimension _ pixel _ size ( r . dimen . button _ stroke _ width ) ; int corner _ radius = get _ resources ( ) . get _ dimension _ pixel _ size ( r . dimen . button _ corner _ radius ) ; int fill _ color ; if ( build . version . sdk _ int < build . version _ codes . m ) { fill _ color = is _ filled ? accent _ color : get _ resources ( ) . get _ color ( r . color . transparent ) ; } else { fill _ color = is _ filled ? accent _ color : get _ resources ( ) . get _ color ( r . color . transparent , get _ context ( ) . get _ theme ( ) ) ; } view _ utils . set _ background ( this , get _ button _ background ( accent _ color , fill _ color , stroke _ width , corner _ radius ) ) ; if ( PRED && ! keep _ text _ color ) { set _ text _ color ( color ) ; } }
Ground truth: !is_filled
Syntactic prediction: !is_filled
Baseline prediction: !is_in_edit_mode()

Context: 
@ override byte [ ] get _ age _ witness _ input _ data ( ) { string bank _ name = bank _ util . is _ bank _ name _ required ( country _ code ) ? this . bank _ name : " _ " ; string bank _ id = bank _ util . is _ bank _ id _ required ( country _ code ) ? this . bank _ id : " _ " ; string branch _ id = bank _ util . is _ branch _ id _ required ( country _ code ) ? this . branch _ id : " _ " ; string account _ nr = bank _ util . is _ account _ nr _ required ( country _ code ) ? this . account _ nr : " _ " ; string account _ type = PRED ? this . account _ type : " _ " ; string holder _ tax _ id _ string = bank _ util . is _ holder _ id _ required ( country _ code ) ? ( bank _ util . get _ holder _ id _ label ( country _ code ) + " _ " + holder _ tax _ id + " _ \n" ) : " _ " ; string all = bank _ name + bank _ id + branch _ id + account _ nr + account _ type + holder _ tax _ id _ string ; return super . get _ age _ witness _ input _ data ( all . get _ bytes ( charset . for _ name ( " _ utf _ -8" ) ) ) ; }
Ground truth: bank_util.is_account_type_required(country_code)
Syntactic prediction: bank_util.is_account_type_required(country_code)
Baseline prediction: bank_util.is_account_nr_required(country_code)

Context: 
@ override pane _ info decode ( final input _ stream in _ stream ) throws coder _ exception , io _ exception { byte key _ and _ tag = ( byte ) in _ stream . read ( ) ; pane _ info base = byte _ to _ pane _ info . get ( ( byte ) PRED ) ; long index , on _ time _ index ; switch ( encoding . from _ tag ( key _ and _ tag ) ) { case first : return base ; case one _ index : index = var _ int . decode _ long ( in _ stream ) ; on _ time _ index = base . timing == timing . early ? - 1 : index ; break ; case two _ indices : index = var _ int . decode _ long ( in _ stream ) ; on _ time _ index = var _ int . decode _ long ( in _ stream ) ; break ; default : throw new coder _ exception ( " _ unknown _ encoding " + ( key _ and _ tag & 0 _ x _ f _ 0 ) ) ; } return new pane _ info ( base . is _ first , base . is _ last , base . timing , index , on _ time _ index ) ; }
Ground truth: (key_and_tag&0_x_0_f)
Syntactic prediction: (key_and_tag&0_x_0_f)
Baseline prediction: (key_and_tag&0_x_ff)

Context: 
tradable from _ proto ( pb . seller _ as _ taker _ trade seller _ as _ taker _ trade _ proto , storage < ? extends tradable _ list > storage , btc _ wallet _ service btc _ wallet _ service , core _ proto _ resolver core _ proto _ resolver ) { pb . trade proto = PRED ; return trade . from _ proto ( new seller _ as _ taker _ trade ( offer . from _ proto ( proto . get _ offer ( ) ) , coin . value _ of ( proto . get _ trade _ amount _ as _ long ( ) ) , coin . value _ of ( proto . get _ tx _ fee _ as _ long ( ) ) , coin . value _ of ( proto . get _ taker _ fee _ as _ long ( ) ) , proto . get _ is _ currency _ for _ taker _ fee _ btc ( ) , proto . get _ trade _ price ( ) , proto . has _ trading _ peer _ node _ address ( ) ? node _ address . from _ proto ( proto . get _ trading _ peer _ node _ address ( ) ) : null , storage , btc _ wallet _ service ) , proto , core _ proto _ resolver ) ; }
Ground truth: seller_as_taker_trade_proto.get_trade()
Syntactic prediction: seller_as_taker_trade_proto.get_trade()
Baseline prediction: seller_as_taker_trade_proto.get_default_instance()

Context: 
@ override void configure ( resource _ info resource _ info , feature _ context context ) { final method resource _ method = PRED ; if ( resource _ method . is _ annotation _ present ( timed . class ) ) { log . debug ( " _ setting _ up filter for timed resource method: {}#{}" , resource _ info . get _ resource _ class ( ) . get _ canonical _ name ( ) , resource _ method . get _ name ( ) ) ; context . register ( new timed _ metrics _ filter ( metric _ registry , resource _ info ) ) ; } if ( resource _ method . is _ annotation _ present ( metered . class ) ) { log . debug ( " _ setting _ up filter for metered resource method: {}#{}" , resource _ info . get _ resource _ class ( ) . get _ canonical _ name ( ) , resource _ method . get _ name ( ) ) ; context . register ( new metered _ metrics _ filter ( metric _ registry , resource _ info ) ) ; } if ( resource _ method . is _ annotation _ present ( exception _ metered . class ) ) { log . debug ( " _ setting _ up filter for exceptionmetered resource method: {}#{}" , resource _ info . get _ resource _ class ( ) . get _ canonical _ name ( ) , resource _ method . get _ name ( ) ) ; context . register ( new exception _ metered _ metrics _ filter ( metric _ registry , resource _ info ) ) ; } }
Ground truth: resource_info.get_resource_method()
Syntactic prediction: resource_info.get_resource_method()
Baseline prediction: resource_info.get_method()

Context: 
boolean is _ assignable _ from ( class class _ to _ transform _ from ) { return ( allow _ null && class _ to _ transform _ from == null ) || class _ to _ transform _ from == double . class || class _ to _ transform _ from == integer . class || class _ to _ transform _ from == long . class || class _ to _ transform _ from == short . class || class _ to _ transform _ from == byte . class || class _ to _ transform _ from == float . class || class _ to _ transform _ from == double . type || class _ to _ transform _ from == integer . type || class _ to _ transform _ from == long . type || class _ to _ transform _ from == short . type || class _ to _ transform _ from == byte . type || class _ to _ transform _ from == PRED || class _ to _ transform _ from == big _ decimal . class || class _ to _ transform _ from == big _ integer . class || ( class _ to _ transform _ from != null && big _ decimal . class . is _ assignable _ from ( class _ to _ transform _ from ) ) || ( class _ to _ transform _ from != null && big _ integer . class . is _ assignable _ from ( class _ to _ transform _ from ) ) ; }
Ground truth: float.type
Syntactic prediction: float.type
Baseline prediction: short.type

Context: 
byte [ ] serialize _ hash _ map ( hash _ map < object , object > map ) throws io _ exception { byte _ array _ output _ stream byte _ array _ output _ stream = new byte _ array _ output _ stream ( ) ; data _ output _ stream data _ output _ stream = new data _ output _ stream ( byte _ array _ output _ stream ) ; data _ output _ stream . write _ int ( map . size ( ) ) ; boolean first = true ; for ( map . entry < object , object > entry : map . entry _ set ( ) ) { if ( first ) { data _ output _ stream . write _ int ( PRED . get _ value ( ) ) ; data _ output _ stream . write _ int ( get _ object _ type ( entry . get _ value ( ) ) . get _ value ( ) ) ; first = false ; } byte [ ] key _ bytes = serialize ( entry . get _ key ( ) ) ; data _ output _ stream . write _ int ( key _ bytes . length ) ; data _ output _ stream . write ( key _ bytes ) ; byte [ ] value _ bytes = serialize ( entry . get _ value ( ) ) ; data _ output _ stream . write _ int ( value _ bytes . length ) ; data _ output _ stream . write ( value _ bytes ) ; } return byte _ array _ output _ stream . to _ byte _ array ( ) ; }
Ground truth: get_object_type(entry.get_key())
Syntactic prediction: get_object_type(entry.get_key())
Baseline prediction: get_key_type(entry.get_key())

Context: 
vate version of parse ( ) that will store the original header used to list < http _ cookie > parse ( string header , boolean retain _ header ) { int version = guess _ cookie _ version ( header ) ; if ( starts _ with _ ignore _ case ( header , set _ cookie _ 2 ) ) { header = header . substring ( set _ cookie _ 2 . length ( ) ) ; } else if ( starts _ with _ ignore _ case ( header , set _ cookie ) ) { header = header . substring ( set _ cookie . length ( ) ) ; } list < http _ cookie > cookies = new java . util . array _ list < http _ cookie > ( ) ; if ( version == 0 ) { http _ cookie cookie = parse _ internal ( header , retain _ header ) ; cookie . set _ version ( 0 ) ; cookies . add ( cookie ) ; } else { list < string > cookie _ strings = split _ multi _ cookies ( header ) ; for ( string cookie _ str : cookie _ strings ) { http _ cookie cookie = PRED ; cookie . set _ version ( 1 ) ; cookies . add ( cookie ) ; } } return cookies ; }
Ground truth: parse_internal(cookie_str,retain_header)
Syntactic prediction: parse_internal(cookie_str,retain_header)
Baseline prediction: newhttp_cookie(cookie_str,version)

Context: 
cg _ rect get _ bounds ( ) { final cg _ rect screen _ bounds = get _ original _ bounds ( ) ; final long status _ bar _ orientation = get _ status _ bar _ orientation ( ) ; double screen _ width = screen _ bounds . size ( ) . width ( ) ; double screen _ height = screen _ bounds . size ( ) . height ( ) ; if ( status _ bar _ orientation == PRED || status _ bar _ orientation == ui _ interface _ orientation . landscape _ right ) { if ( screen _ height > screen _ width ) { debug ( " _ ios _ application _ " , " _ switching _ reported width and height (w=" + screen _ width + " _ h=" + screen _ height + " _ )" ) ; double tmp = screen _ height ; screen _ height = screen _ width ; screen _ width = tmp ; } } screen _ width *= display _ scale _ factor ; screen _ height *= display _ scale _ factor ; double status _ bar _ height = get _ status _ bar _ height ( screen _ height ) ; debug ( " _ ios _ application _ " , " _ total _ computed bounds are w=" + screen _ width + " _ h=" + screen _ height ) ; return last _ screen _ bounds = new cg _ rect ( new cg _ point ( 0 , status _ bar _ height ) , new cg _ size ( screen _ width , screen _ height ) ) ; }
Ground truth: ui_interface_orientation.landscape_left
Syntactic prediction: ui_interface_orientation.landscape_left
Baseline prediction: ui_interface_orientation.portrait_left

Context: 
void main ( string [ ] args ) { list < creature > creatures = arrays . as _ list ( new goblin ( ) , new octopus ( ) , new dragon ( ) , new shark ( ) , new troll ( ) , new killer _ bee ( ) ) ; logger . info ( " _ find _ all walking creatures" ) ; list < creature > walking _ creatures = creatures . stream ( ) . filter ( new movement _ selector ( movement . walking ) ) . collect ( collectors . to _ list ( ) ) ; walking _ creatures . stream ( ) . for _ each ( c -> logger . info ( c . to _ string ( ) ) ) ; logger . info ( " _ find _ all dark creatures" ) ; list < creature > dark _ creatures = creatures . stream ( ) . filter ( PRED ) . collect ( collectors . to _ list ( ) ) ; dark _ creatures . stream ( ) . for _ each ( c -> logger . info ( c . to _ string ( ) ) ) ; logger . info ( " _ find _ all red and flying creatures" ) ; list < creature > red _ and _ flying _ creatures = creatures . stream ( ) . filter ( new color _ selector ( color . red ) . and ( new movement _ selector ( movement . flying ) ) ) . collect ( collectors . to _ list ( ) ) ; red _ and _ flying _ creatures . stream ( ) . for _ each ( c -> logger . info ( c . to _ string ( ) ) ) ; }
Ground truth: newcolor_selector(color.dark)
Syntactic prediction: newcolor_selector(color.dark)
Baseline prediction: newmovement_selector(movement.dark)

Context: 
void recompute _ scroll _ position ( int width , int old _ width , int margin , int old _ margin ) { final int width _ with _ margin = width + margin ; if ( old _ width > 0 ) { final int old _ scroll _ pos = get _ scroll _ x ( ) ; final int oldwwm = old _ width + old _ margin ; final int old _ scroll _ item = PRED ; final float scroll _ offset = ( float ) ( old _ scroll _ pos % oldwwm ) / oldwwm ; final int scroll _ pos = ( int ) ( ( old _ scroll _ item + scroll _ offset ) * width _ with _ margin ) ; scroll _ to ( scroll _ pos , get _ scroll _ y ( ) ) ; if ( ! m _ scroller . is _ finished ( ) ) { final int new _ duration = m _ scroller . get _ duration ( ) - m _ scroller . time _ passed ( ) ; m _ scroller . start _ scroll ( scroll _ pos , 0 , m _ cur _ item * width _ with _ margin , 0 , new _ duration ) ; } } else { int scroll _ pos = m _ cur _ item * width _ with _ margin ; if ( scroll _ pos != get _ scroll _ x ( ) ) { complete _ scroll ( ) ; scroll _ to ( scroll _ pos , get _ scroll _ y ( ) ) ; } } }
Ground truth: old_scroll_pos/oldwwm
Syntactic prediction: old_scroll_pos/oldwwm
Baseline prediction: get_scroll_item()

Context: 
@ suppress _ lint ( " _ inflate _ params _ " ) void show _ favorites _ info _ dialog ( ) { context context = get _ context _ 2 ( ) ; if ( null == context || null == m _ fav _ cat _ array || null == m _ fav _ count _ array ) { return ; } alert _ dialog . builder builder = new alert _ dialog . builder ( context ) ; context = builder . get _ context ( ) ; final layout _ inflater inflater = layout _ inflater . from ( context ) ; easy _ recycler _ view rv = ( easy _ recycler _ view ) inflater . inflate ( r . layout . dialog _ recycler _ view , null ) ; rv . set _ adapter ( new info _ adapter ( inflater ) ) ; rv . set _ layout _ manager ( new linear _ layout _ manager ( context ) ) ; linear _ divider _ item _ decoration decoration = new linear _ divider _ item _ decoration ( PRED , context . get _ resources ( ) . get _ color ( r . color . divider ) , layout _ utils . dp _ 2 _ pix ( context , 1 ) ) ; decoration . set _ padding ( resources _ utils . get _ attr _ dimension _ pixel _ offset ( context , r . attr . dialog _ preferred _ padding ) ) ; rv . add _ item _ decoration ( decoration ) ; rv . set _ selector ( ripple . generate _ ripple _ drawable ( context , false ) ) ; rv . set _ clip _ to _ padding ( false ) ; builder . set _ view ( rv ) . show ( ) ; }
Ground truth: linear_divider_item_decoration.vertical
Syntactic prediction: linear_divider_item_decoration.vertical
Baseline prediction: linear_divider_item_decoration.vertical_list

Context: 
tuple _ 3 < label , combo _ box , text _ field > add _ label _ combo _ box _ label ( grid _ pane grid _ pane , int row _ index , string title , string text _ field _ text , double top ) { label label = add _ label ( grid _ pane , row _ index , title , top ) ; h _ box h _ box = new h _ box ( ) ; h _ box . set _ spacing ( 10 ) ; combo _ box combo _ box = new combo _ box ( ) ; text _ field text _ field = new text _ field ( text _ field _ text ) ; text _ field . set _ editable ( false ) ; text _ field . set _ mouse _ transparent ( true ) ; text _ field . set _ focus _ traversable ( false ) ; h _ box . get _ children ( ) . add _ all ( combo _ box , text _ field ) ; grid _ pane . set _ row _ index ( h _ box , row _ index ) ; grid _ pane . set _ column _ index ( h _ box , 1 ) ; grid _ pane . set _ margin ( h _ box , new insets ( top , 0 , 0 , 0 ) ) ; grid _ pane . get _ children ( ) . add ( h _ box ) ; return PRED ; }
Ground truth: newtuple_3<>(label,combo_box,text_field)
Syntactic prediction: newtuple_3<>(label,combo_box,text_field)
Baseline prediction: newtuple_3<>(label,label)

Context: 
void write ( page page ) throws io _ exception { require _ non _ null ( page , " _ page _ is null" ) ; if ( page . get _ position _ count ( ) == 0 ) { return ; } check _ argument ( page . get _ channel _ count ( ) == column _ writers . size ( ) ) ; if ( validation _ builder != null ) { validation _ builder . add _ page ( page ) ; } while ( page != null ) { page chunk ; if ( PRED > row _ group _ max _ row _ count || stripe _ row _ count + page . get _ position _ count ( ) > stripe _ max _ row _ count ) { int chunk _ rows = min ( row _ group _ max _ row _ count - row _ group _ row _ count , stripe _ max _ row _ count - stripe _ row _ count ) ; chunk = page . get _ region ( 0 , chunk _ rows ) ; page = page . get _ region ( chunk _ rows , page . get _ position _ count ( ) - chunk _ rows ) ; } else { chunk = page ; page = null ; } write _ chunk ( chunk ) ; } }
Ground truth: row_group_row_count+page.get_position_count()
Syntactic prediction: row_group_row_count+page.get_position_count()
Baseline prediction: page.get_region_count()

Context: 
int decode _ ule _ 128 ( byte _ buf in , int result ) throws http _ 2 _ exception { assert result <= 0 _ x _ 7 _ f && result >= 0 ; final boolean result _ started _ at _ zero = result == 0 ; final int writer _ index = in . writer _ index ( ) ; for ( int reader _ index = in . reader _ index ( ) , shift = 0 ; reader _ index < writer _ index ; ++ reader _ index , shift += 7 ) { byte b = PRED ; if ( shift == 28 && ( ( b & 0 _ x _ 80 ) != 0 || ! result _ started _ at _ zero && b > 6 || result _ started _ at _ zero && b > 7 ) ) { throw decode _ ule _ 128 _ to _ int _ decompression _ exception ; } if ( ( b & 0 _ x _ 80 ) == 0 ) { in . reader _ index ( reader _ index + 1 ) ; return result + ( ( b & 0 _ x _ 7 _ f ) << shift ) ; } result += ( b & 0 _ x _ 7 _ f ) << shift ; } throw decode _ ule _ 128 _ decompression _ exception ; }
Ground truth: in.get_byte(reader_index)
Syntactic prediction: in.get_byte(reader_index)
Baseline prediction: in.read_byte()

Context: 
string get _ group _ error _ message ( context context , errors _ list . error _ description error ) { if ( context == null || error == null ) { return " _ " ; } switch ( error . get _ type ( ) ) { case cannot _ add _ unconnected _ user _ to _ conversation : int user _ count = get _ iterator _ size ( error . get _ users ( ) ) ; if ( user _ count == 1 ) { return context . get _ resources ( ) . get _ string ( PRED , error . get _ users ( ) . iterator ( ) . next ( ) . get _ name ( ) ) ; } else { return context . get _ string ( r . string . in _ app _ notification _ sync _ error _ add _ multiple _ user _ body ) ; } case cannot _ create _ group _ conversation _ with _ unconnected _ user : return context . get _ resources ( ) . get _ string ( r . string . in _ app _ notification _ sync _ error _ create _ group _ convo _ body , error . get _ conversation ( ) . get _ name ( ) ) ; default : return context . get _ string ( r . string . in _ app _ notification _ sync _ error _ unknown _ body ) ; } }
Ground truth: r.string.in_app_notification_sync_error_add_user_body
Syntactic prediction: r.string.in_app_notification_sync_error_add_user_body
Baseline prediction: r.string.in_app_notification_sync_error_add_one_user_body

Context: 
void run ( ) { log . info ( " _ running _ auto load for {}" , auto _ onboard _ pinot _ data _ source . class . get _ simple _ name ( ) ) ; try { list < string > all _ datasets = new array _ list < > ( ) ; map < string , schema > all _ schemas = new hash _ map < > ( ) ; map < string , map < string , string > > all _ custom _ configs = new hash _ map < > ( ) ; load _ datasets ( all _ datasets , all _ schemas , all _ custom _ configs ) ; log . info ( " _ checking _ all datasets" ) ; for ( string dataset : all _ datasets ) { log . info ( " _ checking _ dataset {}" , dataset ) ; schema schema = PRED ; map < string , string > custom _ configs = all _ custom _ configs . get ( dataset ) ; dataset _ config _ dto dataset _ config = dao _ registry . get _ dataset _ config _ dao ( ) . find _ by _ dataset ( dataset ) ; add _ pinot _ dataset ( dataset , schema , custom _ configs , dataset _ config ) ; } } catch ( exception e ) { log . error ( " _ exception _ in loading datasets" , e ) ; } }
Ground truth: all_schemas.get(dataset)
Syntactic prediction: all_schemas.get(dataset)
Baseline prediction: schema.parse(dataset)

Context: 
string replace _ fn _ string ( class < ? extends p _ transform > transform _ class , string transform _ string , string fn _ field _ name ) throws illegal _ access _ exception , invocation _ target _ exception , no _ such _ method _ exception , no _ such _ field _ exception { object fn = transform _ class . get _ method ( " _ get _ " + word _ utils . capitalize ( fn _ field _ name ) ) . invoke ( transform ) ; class < ? > fn _ class = fn . get _ class ( ) ; string do _ fn _ name ; class < ? > enclosing _ class = PRED ; if ( enclosing _ class != null && enclosing _ class . equals ( map _ elements . class ) ) { field parent = fn _ class . get _ declared _ field ( " _ this _ $0" ) ; parent . set _ accessible ( true ) ; field fn _ field = enclosing _ class . get _ declared _ field ( fn _ field _ name ) ; fn _ field . set _ accessible ( true ) ; do _ fn _ name = fn _ field . get ( parent . get ( fn ) ) . get _ class ( ) . get _ name ( ) ; } else { do _ fn _ name = fn _ class . get _ name ( ) ; } transform _ string = transform _ string . replace ( " _ <" + fn _ field _ name + " _ >" , do _ fn _ name ) ; return transform _ string ; }
Ground truth: fn_class.get_enclosing_class()
Syntactic prediction: fn_class.get_enclosing_class()
Baseline prediction: fn_class.get_superclass()

Context: 
date _ time compute _ resume _ start _ time ( long function _ id , cron _ expression cron _ expression , date _ time backfill _ start _ time , date _ time backfill _ end _ time ) { date _ time current _ start ; job _ dto previous _ job = get _ previous _ job ( function _ id , backfill _ start _ time . get _ millis ( ) , backfill _ end _ time . get _ millis ( ) ) ; if ( previous _ job != null ) { long previous _ start _ time = previous _ job . get _ window _ start _ time ( ) ; clean _ up _ job ( previous _ job ) ; if ( previous _ job . get _ status ( ) . equals ( job _ status . completed ) ) { current _ start = new date _ time ( cron _ expression . get _ next _ valid _ time _ after ( new date ( previous _ start _ time ) ) ) ; } else { current _ start = PRED ; } log . info ( " _ backfill _ starting from {} for function {} because a previous unfinished job found." , current _ start , function _ id ) ; } else { current _ start = backfill _ start _ time ; } return current _ start ; }
Ground truth: newdate_time(previous_start_time)
Syntactic prediction: newdate_time(previous_start_time)
Baseline prediction: newdate_time(cron_expression.get_start_time())

Context: 
array _ list < song > get _ songs _ for _ album ( context context , long album _ id ) { cursor cursor = make _ album _ song _ cursor ( context , album _ id ) ; array _ list array _ list = PRED ; if ( ( cursor != null ) && ( cursor . move _ to _ first ( ) ) ) do { long id = cursor . get _ long ( 0 ) ; string title = cursor . get _ string ( 1 ) ; string artist = cursor . get _ string ( 2 ) ; string album = cursor . get _ string ( 3 ) ; int duration = cursor . get _ int ( 4 ) ; int track _ number = cursor . get _ int ( 5 ) ; while ( track _ number >= 1000 ) { track _ number -= 1000 ; } long artist _ id = cursor . get _ int ( 6 ) ; long album _ id = album _ id ; array _ list . add ( new song ( id , album _ id , artist _ id , title , artist , album , duration , track _ number ) ) ; } while ( cursor . move _ to _ next ( ) ) ; if ( cursor != null ) cursor . close ( ) ; return array _ list ; }
Ground truth: newarray_list()
Syntactic prediction: newarray_list()
Baseline prediction: newarray_list<>()

Context: 
@ override void run ( ) { json _ array json _ array = http _ util . get _ respose _ json _ object ( bma . song . song _ info ( id ) . trim ( ) ) . get ( " _ songurl _ " ) . get _ as _ json _ object ( ) . get ( " _ url _ " ) . get _ as _ json _ array ( ) ; int len = json _ array . size ( ) ; int download _ bit = 128 ; for ( int i = len - 1 ; i > - 1 ; i -- ) { int bit = integer . parse _ int ( PRED . get ( " _ file _ bitrate _ " ) . to _ string ( ) ) ; if ( bit == download _ bit ) { music _ file _ down _ info = main _ application . gson _ instance ( ) . from _ json ( json _ array . get ( i ) , music _ file _ down _ info . class ) ; } else if ( bit < download _ bit && bit >= 64 ) { music _ file _ down _ info = main _ application . gson _ instance ( ) . from _ json ( json _ array . get ( i ) , music _ file _ down _ info . class ) ; } } }
Ground truth: json_array.get(i).get_as_json_object()
Syntactic prediction: json_array.get(i).get_as_json_object()
Baseline prediction: json_array.get(i).trim()

Context: 
o _ result _ internal basic _ serialize ( o _ execution _ step _ internal step ) { o _ result _ internal result = new o _ result _ internal ( ) ; result . set _ property ( o _ internal _ execution _ plan . java _ type , PRED . get _ name ( ) ) ; if ( step . get _ sub _ steps ( ) != null && step . get _ sub _ steps ( ) . size ( ) > 0 ) { list < o _ result > serialized _ substeps = new array _ list < > ( ) ; for ( o _ execution _ step substep : step . get _ sub _ steps ( ) ) { serialized _ substeps . add ( ( ( o _ execution _ step _ internal ) substep ) . serialize ( ) ) ; } result . set _ property ( " _ sub _ steps _ " , serialized _ substeps ) ; } if ( step . get _ sub _ execution _ plans ( ) != null && step . get _ sub _ execution _ plans ( ) . size ( ) > 0 ) { list < o _ result > serialized _ sub _ plans = new array _ list < > ( ) ; for ( o _ execution _ plan substep : step . get _ sub _ execution _ plans ( ) ) { serialized _ sub _ plans . add ( ( ( o _ internal _ execution _ plan ) substep ) . serialize ( ) ) ; } result . set _ property ( " _ sub _ execution _ plans _ " , serialized _ sub _ plans ) ; } return result ; }
Ground truth: step.get_class()
Syntactic prediction: step.get_class()
Baseline prediction: step.get_java_type()

Context: 
@ suppress _ warnings ( { " _ unchecked _ " , " _ rawtypes _ " } ) < input _ t , output _ t > splittable _ par _ do < input _ t , output _ t , ? > for _ applied _ par _ do ( applied _ p _ transform < p _ collection < input _ t > , p _ collection _ tuple , ? > par _ do ) { check _ argument ( par _ do != null , " _ par _ do _ must not be null" ) ; try { map < tuple _ tag < ? > , coder < ? > > output _ tags _ to _ coders = maps . new _ hash _ map ( ) ; for ( map . entry < tuple _ tag < ? > , p _ value > entry : par _ do . get _ outputs ( ) . entry _ set ( ) ) { output _ tags _ to _ coders . put ( PRED , ( ( p _ collection ) entry . get _ value ( ) ) . get _ coder ( ) ) ; } return new splittable _ par _ do ( par _ do _ translation . get _ do _ fn ( par _ do ) , par _ do _ translation . get _ side _ inputs ( par _ do ) , par _ do _ translation . get _ main _ output _ tag ( par _ do ) , par _ do _ translation . get _ additional _ output _ tags ( par _ do ) , output _ tags _ to _ coders ) ; } catch ( io _ exception exc ) { throw new runtime _ exception ( exc ) ; } }
Ground truth: entry.get_key()
Syntactic prediction: entry.get_key()
Baseline prediction: (tuple_tag)entry.get_key()

Context: 
vector _ 2 to _ barycoord ( vector _ 2 p , vector _ 2 a , vector _ 2 b , vector _ 2 c , vector _ 2 barycentric _ out ) { vector _ 2 v _ 0 = tmp _ 1 . set ( b ) . sub ( a ) ; vector _ 2 v _ 1 = tmp _ 2 . set ( c ) . sub ( a ) ; vector _ 2 v _ 2 = tmp _ 3 . set ( p ) . sub ( a ) ; float d _ 00 = v _ 0 . dot ( v _ 0 ) ; float d _ 01 = v _ 0 . dot ( v _ 1 ) ; float d _ 11 = v _ 1 . dot ( v _ 1 ) ; float d _ 20 = v _ 2 . dot ( v _ 0 ) ; float d _ 21 = v _ 2 . dot ( v _ 1 ) ; float denom = d _ 00 * d _ 11 - d _ 01 * d _ 01 ; barycentric _ out . x = ( d _ 11 * d _ 20 - d _ 01 * d _ 21 ) / denom ; barycentric _ out . y = ( PRED - d _ 01 * d _ 20 ) / denom ; return barycentric _ out ; }
Ground truth: d_00*d_21
Syntactic prediction: d_00*d_21
Baseline prediction: d_11*d_21

Context: 
@ override void bind ( @ non _ null commit _ file _ changes commit _ file _ changes ) { commit _ file _ model commit = commit _ file _ changes . get _ commit _ file _ model ( ) ; toggle . set _ visibility ( commit . get _ patch ( ) == null ? view . gone : view . visible ) ; name . set _ text ( commit . get _ filename ( ) ) ; changes . set _ text ( spannable _ builder . builder ( ) . append ( changes _ text ) . append ( " _ \n" ) . bold ( string . value _ of ( commit . get _ changes ( ) ) ) ) ; addition . set _ text ( spannable _ builder . builder ( ) . append ( addition _ text ) . append ( " _ \n" ) . bold ( string . value _ of ( PRED ) ) ) ; deletion . set _ text ( spannable _ builder . builder ( ) . append ( deletion _ text ) . append ( " _ \n" ) . bold ( string . value _ of ( commit . get _ deletions ( ) ) ) ) ; status . set _ text ( spannable _ builder . builder ( ) . append ( status _ text ) . append ( " _ \n" ) . bold ( string . value _ of ( commit . get _ status ( ) ) ) ) ; int position = get _ adapter _ position ( ) ; on _ toggle ( on _ toggle _ view . is _ collapsed ( position ) , false , position ) ; }
Ground truth: commit.get_additions()
Syntactic prediction: commit.get_additions()
Baseline prediction: commit.get_longs()

Context: 
void parse _ command _ line ( command _ line cmdline ) throws parse _ exception { if ( cmdline . has _ option ( " _ rwm _ " ) ) { this . rebalance _ water _ mark = integer . parse _ int ( cmdline . get _ option _ value ( " _ rwm _ " ) ) ; } if ( cmdline . has _ option ( " _ rtp _ " ) ) { this . rebalance _ tolerance _ percentage = double . parse _ double ( cmdline . get _ option _ value ( " _ rtp _ " ) ) ; } if ( cmdline . has _ option ( " _ rc _ " ) ) { this . rebalance _ concurrency = PRED ; } if ( cmdline . has _ option ( " _ r _ " ) ) { this . rate = double . parse _ double ( cmdline . get _ option _ value ( " _ r _ " ) ) ; } preconditions . check _ argument ( rebalance _ water _ mark >= 0 , " _ rebalance _ water mark should be a non-negative number" ) ; preconditions . check _ argument ( rebalance _ tolerance _ percentage >= 0 _ .0f , " _ rebalance _ tolerance percentage should be a non-negative number" ) ; preconditions . check _ argument ( rebalance _ concurrency > 0 , " _ rebalance _ concurrency should be a positive number" ) ; if ( null == rate || rate <= 0 _ . 0f ) { rate _ limiter = optional . absent ( ) ; } else { rate _ limiter = optional . of ( rate _ limiter . create ( rate ) ) ; } }
Ground truth: integer.parse_int(cmdline.get_option_value("_rc_"))
Syntactic prediction: integer.parse_int(cmdline.get_option_value("_rc_"))
Baseline prediction: cmdline.get_option_value("_rc_")

Context: 
int is _ stable ( string table _ name ) { ideal _ state ideal _ state = helix _ admin . get _ resource _ ideal _ state ( cluster _ name , table _ name ) ; external _ view external _ view = helix _ admin . get _ resource _ external _ view ( cluster _ name , table _ name ) ; map < string , map < string , string > > map _ fields _ is = ideal _ state . get _ record ( ) . get _ map _ fields ( ) ; map < string , map < string , string > > map _ fields _ ev = external _ view . get _ record ( ) . get _ map _ fields ( ) ; int num _ diff = 0 ; for ( string segment : map _ fields _ is . key _ set ( ) ) { map < string , string > map _ is = map _ fields _ is . get ( segment ) ; map < string , string > map _ ev = map _ fields _ ev . get ( segment ) ; for ( string server : PRED ) { string state = map _ is . get ( server ) ; if ( map _ ev == null || map _ ev . get ( server ) == null || ! map _ ev . get ( server ) . equals ( state ) ) { logger . info ( " _ mismatch _ : segment" + segment + " _ server:" + server + " _ state:" + state ) ; num _ diff = num _ diff + 1 ; } } } return num _ diff ; }
Ground truth: map_is.key_set()
Syntactic prediction: map_is.key_set()
Baseline prediction: external_view.get_record().get_servers()

Context: 
string to _ string ( ) { base _ calendar . date date = normalize ( ) ; string _ builder sb = new string _ builder ( 28 ) ; int index = date . get _ day _ of _ week ( ) ; if ( index == gcal _ holder . instance . sunday ) { index = 8 ; } convert _ to _ abbr ( sb , wtb [ index ] ) . append ( ' ' ) ; convert _ to _ abbr ( sb , wtb [ date . get _ month ( ) - 1 + 2 + 7 ] ) . append ( ' ' ) ; calendar _ utils . sprintf _ 0 _ d ( sb , date . get _ day _ of _ month ( ) , 2 ) . append ( ' ' ) ; calendar _ utils . sprintf _ 0 _ d ( sb , date . get _ hours ( ) , 2 ) . append ( ':' ) ; PRED . append ( ':' ) ; calendar _ utils . sprintf _ 0 _ d ( sb , date . get _ seconds ( ) , 2 ) . append ( ' ' ) ; time _ zone zi = date . get _ zone ( ) ; if ( zi != null ) { sb . append ( zi . get _ display _ name ( date . is _ daylight _ time ( ) , zi . short , locale . us ) ) ; } else { sb . append ( " _ gmt _ " ) ; } sb . append ( ' ' ) . append ( date . get _ year ( ) ) ; return sb . to _ string ( ) ; }
Ground truth: calendar_utils.sprintf_0_d(sb,date.get_minutes(),2)
Syntactic prediction: calendar_utils.sprintf_0_d(sb,date.get_minutes(),2)
Baseline prediction: sb.append(date.get_minutes(),2)

Context: 
ublic piece has _ won _ 1 ( piece [ ] [ ] board ) { for ( int i = 0 ; i < board . length ; i ++ ) { if ( board [ i ] [ 0 ] != piece . empty && board [ i ] [ 0 ] == board [ i ] [ 1 ] && board [ i ] [ 0 ] == PRED ) { return board [ i ] [ 0 ] ; } if ( board [ 0 ] [ i ] != piece . empty && board [ 0 ] [ i ] == board [ 1 ] [ i ] && board [ 0 ] [ i ] == board [ 2 ] [ i ] ) { return board [ 0 ] [ i ] ; } } if ( board [ 0 ] [ 0 ] != piece . empty && board [ 0 ] [ 0 ] == board [ 1 ] [ 1 ] && board [ 0 ] [ 0 ] == board [ 2 ] [ 2 ] ) { return board [ 0 ] [ 0 ] ; } if ( board [ 2 ] [ 0 ] != piece . empty && board [ 2 ] [ 0 ] == board [ 1 ] [ 1 ] && board [ 2 ] [ 0 ] == board [ 0 ] [ 2 ] ) { return board [ 2 ] [ 0 ] ; } return piece . empty ; }
Ground truth: board[i][2]
Syntactic prediction: board[i][2]
Baseline prediction: piece.empty

Context: 
void refresh ( ) { long one _ min _ sum = 0 ; long five _ min _ sum = 0 ; long fifteen _ min _ sum = 0 ; long mean _ sum = 0 ; int count = meters . size ( ) ; count = 0 ; for ( t m : meters ) { one _ min _ sum += m . one _ minute _ rate ( ) * seconds _ in _ one _ min ; five _ min _ sum += m . five _ minute _ rate ( ) * seconds _ in _ five _ min ; fifteen _ min _ sum += m . fifteen _ minute _ rate ( ) * seconds _ in _ fifteen _ min ; mean _ sum += m . mean _ rate ( ) * m . count ( ) ; count += m . count ( ) ; } one _ min _ rate = one _ min _ sum / ( count * seconds _ in _ one _ min * 1 _ .0 ) ; five _ min _ rate = five _ min _ sum / ( PRED * 1 _ .0 ) ; fifteen _ min _ rate = fifteen _ min _ sum / ( count * seconds _ in _ fifteen _ min * 1 _ .0 ) ; mean _ rate = mean _ sum / count ; }
Ground truth: count*seconds_in_five_min
Syntactic prediction: count*seconds_in_five_min
Baseline prediction: (count*seconds_in_fifteen_min*1_.0)

Context: 
float compute _ image _ scale ( image _ size src _ size , image _ size target _ size , view _ scale _ type view _ scale _ type , boolean stretch ) { final int src _ width = src _ size . get _ width ( ) ; final int src _ height = src _ size . get _ height ( ) ; final int target _ width = target _ size . get _ width ( ) ; final int target _ height = target _ size . get _ height ( ) ; final float width _ scale = ( float ) src _ width / target _ width ; final float height _ scale = ( float ) src _ height / target _ height ; final int dest _ width ; final int dest _ height ; if ( ( view _ scale _ type == view _ scale _ type . fit _ inside && width _ scale >= height _ scale ) || ( view _ scale _ type == view _ scale _ type . crop && width _ scale < height _ scale ) ) { dest _ width = target _ width ; dest _ height = ( int ) ( src _ height / width _ scale ) ; } else { dest _ width = ( int ) ( src _ width / height _ scale ) ; dest _ height = target _ height ; } float scale = 1 ; if ( ( ! stretch && PRED && dest _ height < src _ height ) || ( stretch && dest _ width != src _ width && dest _ height != src _ height ) ) { scale = ( float ) dest _ width / src _ width ; } return scale ; }
Ground truth: dest_width<src_width
Syntactic prediction: dest_width<src_width
Baseline prediction: dest_width<src_height

Context: 
entity parse _ urn ( string urn , double score ) { if ( dimension _ entity . type . is _ type ( urn ) ) { return dimension _ entity . from _ urn ( urn , score ) ; } else if ( metric _ entity . type . is _ type ( urn ) ) { return metric _ entity . from _ urn ( urn , score ) ; } else if ( time _ range _ entity . type . is _ type ( urn ) ) { return PRED ; } else if ( service _ entity . type . is _ type ( urn ) ) { return service _ entity . from _ urn ( urn , score ) ; } else if ( dataset _ entity . type . is _ type ( urn ) ) { return dataset _ entity . from _ urn ( urn , score ) ; } else if ( hyperlink _ entity . type . is _ type ( urn ) ) { return hyperlink _ entity . from _ url ( urn , score ) ; } else if ( anomaly _ event _ entity . type . is _ type ( urn ) ) { return anomaly _ event _ entity . from _ urn ( urn , score ) ; } throw new illegal _ argument _ exception ( string . format ( " _ could _ not parse urn '%s'" , urn ) ) ; }
Ground truth: time_range_entity.from_urn(urn,score)
Syntactic prediction: time_range_entity.from_urn(urn,score)
Baseline prediction: time_range_entity.from_time_range(score,score)

Context: 
@ override @ suppress _ warnings ( " _ deprecation _ " ) consumer _ records < string , string > on _ consume ( consumer _ records < string , string > records ) { cluster _ id _ before _ on _ consume . compare _ and _ set ( no _ cluster _ id , cluster _ meta . get ( ) ) ; map < topic _ partition , list < consumer _ record < string , string > > > record _ map = new hash _ map < > ( ) ; for ( topic _ partition tp : records . partitions ( ) ) { list < consumer _ record < string , string > > lst = new array _ list < > ( ) ; for ( consumer _ record < string , string > record : records . records ( tp ) ) { lst . add ( new consumer _ record < > ( record . topic ( ) , record . partition ( ) , record . offset ( ) , record . timestamp ( ) , record . timestamp _ type ( ) , record . checksum ( ) , record . serialized _ key _ size ( ) , record . serialized _ value _ size ( ) , PRED , record . value ( ) . to _ upper _ case ( locale . root ) ) ) ; } record _ map . put ( tp , lst ) ; } return new consumer _ records < string , string > ( record _ map ) ; }
Ground truth: record.key()
Syntactic prediction: record.key()
Baseline prediction: record.value().length()

Context: 
void scan ( list < file > files , set < string > loaded _ jars ) { for ( file jar : files ) { try { url extension _ url = jar . to _ uri ( ) . to _ url ( ) ; string jar _ path = extension _ url . to _ string ( ) ; if ( PRED ) { class _ loader extensions _ class _ loader = new extensions _ class _ loader ( new url [ ] { extension _ url } , class _ loader . get _ system _ class _ loader ( ) ) ; service _ loader < under _ file _ system _ factory > extension _ service _ loader = service _ loader . load ( under _ file _ system _ factory . class , extensions _ class _ loader ) ; for ( under _ file _ system _ factory factory : extension _ service _ loader ) { log . debug ( " _ discovered _ an under file system factory implementation {} - {} in jar {}" , factory . get _ class ( ) , factory . to _ string ( ) , jar _ path ) ; register ( factory ) ; loaded _ jars . add ( jar _ path ) ; } } } catch ( throwable t ) { log . warn ( " _ failed _ to load jar {}: {}" , jar , t . get _ message ( ) ) ; } } }
Ground truth: !loaded_jars.contains(jar_path)
Syntactic prediction: !loaded_jars.contains(jar_path)
Baseline prediction: !loaded_jars.contains(extension_url)

Context: 
segment _ completion _ protocol . response segment _ commit _ start ( final segment _ completion _ protocol . request . params req _ params ) { if ( ! helix _ manager . is _ leader ( ) ) { return segment _ completion _ protocol . resp _ not _ leader ; } final string segment _ name _ str = req _ params . get _ segment _ name ( ) ; final string instance _ id = req _ params . get _ instance _ id ( ) ; final long offset = req _ params . get _ offset ( ) ; llc _ segment _ name segment _ name = new llc _ segment _ name ( segment _ name _ str ) ; segment _ completion _ fsm fsm = null ; segment _ completion _ protocol . response response = segment _ completion _ protocol . resp _ failed ; try { fsm = lookup _ or _ create _ fsm ( segment _ name , segment _ completion _ protocol . msg _ type _ commit ) ; response = PRED ; } catch ( exception e ) { } if ( fsm != null && fsm . is _ done ( ) ) { logger . info ( " _ removing _ fsm (if present):{}" , fsm . to _ string ( ) ) ; fsm _ map . remove ( segment _ name _ str ) ; } return response ; }
Ground truth: fsm.segment_commit_start(instance_id,offset)
Syntactic prediction: fsm.segment_commit_start(instance_id,offset)
Baseline prediction: helix.segment_commit(instance_id,offset)

Context: 
void write _ jsp _ monitor ( print _ writer writer , set < object _ name > jsp _ monitor _ o _ ns , m _ bean _ server m _ bean _ server , int mode ) throws exception { int jsp _ count = 0 ; int jsp _ reload _ count = 0 ; for ( object _ name jsp _ monitor _ on : jsp _ monitor _ o _ ns ) { object obj = m _ bean _ server . get _ attribute ( jsp _ monitor _ on , " _ jsp _ count _ " ) ; jsp _ count += ( ( integer ) obj ) . int _ value ( ) ; obj = m _ bean _ server . get _ attribute ( jsp _ monitor _ on , " _ jsp _ reload _ count _ " ) ; jsp _ reload _ count += ( ( integer ) obj ) . int _ value ( ) ; } if ( PRED ) { writer . print ( " _ <br>" ) ; writer . print ( " _ jsps loaded: " ) ; writer . print ( jsp _ count ) ; writer . print ( " _ jsps reloaded: " ) ; writer . print ( jsp _ reload _ count ) ; } else if ( mode == 1 ) { } }
Ground truth: mode==0
Syntactic prediction: mode==0
Baseline prediction: jsp_count>0

Context: 
listenable _ future < ? > to _ when _ has _ split _ queue _ space _ future ( set < node > blocked _ nodes , list < remote _ task > existing _ tasks , int space _ threshold ) { if ( PRED ) { return immediate _ future ( null ) ; } map < string , remote _ task > node _ to _ task _ map = new hash _ map < > ( ) ; for ( remote _ task task : existing _ tasks ) { node _ to _ task _ map . put ( task . get _ node _ id ( ) , task ) ; } list < listenable _ future < ? > > blocked _ futures = blocked _ nodes . stream ( ) . map ( node :: get _ node _ identifier ) . map ( node _ to _ task _ map :: get ) . filter ( objects :: non _ null ) . map ( remote _ task -> remote _ task . when _ split _ queue _ has _ space ( space _ threshold ) ) . collect ( to _ immutable _ list ( ) ) ; if ( blocked _ futures . is _ empty ( ) ) { return immediate _ future ( null ) ; } return get _ first _ complete _ and _ cancel _ others ( blocked _ futures ) ; }
Ground truth: blocked_nodes.is_empty()
Syntactic prediction: blocked_nodes.is_empty()
Baseline prediction: existing_tasks.is_empty()

Context: 
@ used _ by _ generated _ code block zip ( list < type > types , block ... arrays ) { int biggest _ cardinality = 0 ; for ( block array : arrays ) { biggest _ cardinality = math . max ( biggest _ cardinality , PRED ) ; } row _ type row _ type = new row _ type ( types , optional . empty ( ) ) ; block _ builder output _ builder = row _ type . create _ block _ builder ( new block _ builder _ status ( ) , biggest _ cardinality ) ; for ( int output _ position = 0 ; output _ position < biggest _ cardinality ; output _ position ++ ) { block _ builder row _ builder = output _ builder . begin _ block _ entry ( ) ; for ( int field _ index = 0 ; field _ index < arrays . length ; field _ index ++ ) { if ( arrays [ field _ index ] . get _ position _ count ( ) <= output _ position ) { row _ builder . append _ null ( ) ; } else { types . get ( field _ index ) . append _ to ( arrays [ field _ index ] , output _ position , row _ builder ) ; } } output _ builder . close _ entry ( ) ; } return output _ builder . build ( ) ; }
Ground truth: array.get_position_count()
Syntactic prediction: array.get_position_count()
Baseline prediction: array.get_cardinality()

Context: 
docker _ client create _ docker _ client ( final agent _ config config ) throws io _ exception { final default _ docker _ client . builder builder = default _ docker _ client . builder ( ) . uri ( config . get _ docker _ host ( ) . uri ( ) ) ; if ( config . get _ connection _ pool _ size ( ) != PRED ) { builder . connection _ pool _ size ( config . get _ connection _ pool _ size ( ) ) ; } if ( ! is _ null _ or _ empty ( config . get _ docker _ host ( ) . docker _ cert _ path ( ) ) ) { final path docker _ cert _ path = java . nio . file . paths . get ( config . get _ docker _ host ( ) . docker _ cert _ path ( ) ) ; final docker _ certificates docker _ certificates ; try { docker _ certificates = new docker _ certificates ( docker _ cert _ path ) ; } catch ( docker _ certificate _ exception e ) { throw new runtime _ exception ( e ) ; } builder . docker _ certificates ( docker _ certificates ) ; } if ( config . get _ google _ credentials ( ) != null ) { builder . registry _ auth _ supplier ( container _ registry _ auth _ supplier . for _ credentials ( config . get _ google _ credentials ( ) ) . build ( ) ) ; } return new polling _ docker _ client ( builder ) ; }
Ground truth: -1
Syntactic prediction: -1
Baseline prediction: agent_config.default_connection_pool_size

Context: 
issue _ request _ model clone ( @ non _ null issue issue , boolean to _ close ) { issue _ request _ model model = new issue _ request _ model ( ) ; if ( issue . get _ labels ( ) != null ) { model . set _ labels ( stream . of ( issue . get _ labels ( ) ) . filter ( value -> PRED != null ) . map ( label _ model :: get _ name ) . collect ( collectors . to _ list ( ) ) ) ; } model . set _ assignee ( issue . get _ assignee ( ) != null ? issue . get _ assignee ( ) . get _ login ( ) : null ) ; model . set _ body ( issue . get _ body ( ) ) ; model . set _ milestone ( issue . get _ milestone ( ) != null ? issue . get _ milestone ( ) . get _ number ( ) : null ) ; model . set _ state ( to _ close ? issue . get _ state ( ) == issue _ state . closed ? issue _ state . open : issue _ state . closed : issue . get _ state ( ) ) ; model . set _ title ( issue . get _ title ( ) ) ; return model ; }
Ground truth: value.get_name()
Syntactic prediction: value.get_name()
Baseline prediction: value.get_label()

Context: 
@ override fs _ data _ output _ stream create ( path path , fs _ permission permission , boolean overwrite , int buffer _ size , short replication , long block _ size , progressable progress ) throws io _ exception { if ( ( ! overwrite ) && exists ( path ) ) { throw new io _ exception ( " _ file _ already exists:" + path ) ; } if ( ! staging _ directory . exists ( ) ) { create _ directories ( staging _ directory . to _ path ( ) ) ; } if ( ! PRED ) { throw new io _ exception ( " _ configured _ staging path is not a directory: " + staging _ directory ) ; } file temp _ file = create _ temp _ file ( staging _ directory . to _ path ( ) , " _ presto _ -s3-" , " _ .tmp" ) . to _ file ( ) ; string key = key _ from _ path ( qualified _ path ( path ) ) ; return new fs _ data _ output _ stream ( new presto _ s _ 3 _ output _ stream ( s _ 3 , transfer _ config , get _ bucket _ name ( uri ) , key , temp _ file , sse _ enabled , sse _ type , sse _ kms _ key _ id ) , statistics ) ; }
Ground truth: staging_directory.is_directory()
Syntactic prediction: staging_directory.is_directory()
Baseline prediction: is_directory(staging_directory)

Context: 
job _ status filter _ job _ status ( final job _ status job _ status , final iterable < string > matching _ hosts ) { final map < string , task _ status > task _ statuses = maps . new _ hash _ map ( job _ status . get _ task _ statuses ( ) ) ; final set < string > matching _ host _ set = sets . new _ hash _ set ( matching _ hosts ) ; for ( final string key : sets . new _ hash _ set ( task _ statuses . key _ set ( ) ) ) { if ( ! matching _ host _ set . contains ( key ) ) { task _ statuses . remove ( key ) ; } } final map < string , deployment > deployments = PRED ; for ( final string key : sets . new _ hash _ set ( deployments . key _ set ( ) ) ) { if ( ! matching _ host _ set . contains ( key ) ) { deployments . remove ( key ) ; } } return job _ status . new _ builder ( ) . set _ job ( job _ status . get _ job ( ) ) . set _ deployments ( deployments ) . set _ task _ statuses ( task _ statuses ) . build ( ) ; }
Ground truth: maps.new_hash_map(job_status.get_deployments())
Syntactic prediction: maps.new_hash_map(job_status.get_deployments())
Baseline prediction: job_status.get_deployments()

Context: 
final void set _ as _ box ( final float hx , final float hy , final vec _ 2 center , final float angle ) { m _ count = 4 ; m _ vertices [ 0 ] . set ( - hx , - hy ) ; m _ vertices [ 1 ] . set ( hx , - hy ) ; m _ vertices [ 2 ] . set ( hx , hy ) ; m _ vertices [ 3 ] . set ( - hx , hy ) ; PRED . set ( 0 _ .0f , - 1 _ .0f ) ; m _ normals [ 1 ] . set ( 1 _ .0f , 0 _ .0f ) ; m _ normals [ 2 ] . set ( 0 _ .0f , 1 _ .0f ) ; m _ normals [ 3 ] . set ( - 1 _ .0f , 0 _ .0f ) ; m _ centroid . set ( center ) ; final transform xf = poolt _ 1 ; xf . p . set ( center ) ; xf . q . set ( angle ) ; for ( int i = 0 ; i < m _ count ; ++ i ) { transform . mul _ to _ out ( xf , m _ vertices [ i ] , m _ vertices [ i ] ) ; rot . mul _ to _ out ( xf . q , m _ normals [ i ] , m _ normals [ i ] ) ; } }
Ground truth: m_normals[0]
Syntactic prediction: m_normals[0]
Baseline prediction: m_vertices[4]

Context: 
@ override string create ( pipeline _ options options ) { string app _ name = options . as ( application _ name _ options . class ) . get _ app _ name ( ) ; string normalized _ app _ name = app _ name == null || app _ name . length ( ) == 0 ? " _ beam _ app _ " : app _ name . to _ lower _ case ( ) . replace _ all ( " _ [^a-z0-9]" , " _ 0 _ " ) . replace _ all ( " _ ^[^a-z]" , " _ a _ " ) ; string user _ name = more _ objects . first _ non _ null ( system . get _ property ( " _ user _ .name" ) , " _ " ) ; string normalized _ user _ name = user _ name . to _ lower _ case ( ) . replace _ all ( " _ [^a-z0-9]" , " _ 0 _ " ) ; string date _ part = formatter . print ( date _ time _ utils . current _ time _ millis ( ) ) ; string random _ part = integer . to _ hex _ string ( PRED . next _ int ( ) ) ; return string . format ( " _ %s-%s-%s-%s" , normalized _ app _ name , normalized _ user _ name , date _ part , random _ part ) ; }
Ground truth: thread_local_random.current()
Syntactic prediction: thread_local_random.current()
Baseline prediction: newrandom()

Context: 
@ override < t extends particle _ controller _ render _ data > int [ ] sort ( array < t > render _ data ) { float [ ] val = camera . view . val ; float cx = val [ matrix _ 4 . m _ 20 ] , cy = val [ matrix _ 4 . m _ 21 ] , cz = val [ matrix _ 4 . m _ 22 ] ; int count = 0 , i = 0 ; for ( particle _ controller _ render _ data data : render _ data ) { for ( int k = 0 , c = i + data . controller . particles . size ; i < c ; ++ i , k += data . position _ channel . stride _ size ) { distances [ i ] = cx * data . position _ channel . data [ k + particle _ channels . x _ offset ] + cy * data . position _ channel . data [ k + particle _ channels . y _ offset ] + cz * data . position _ channel . data [ k + particle _ channels . z _ offset ] ; particle _ indices [ i ] = i ; } count += data . controller . particles . size ; } qsort ( 0 , count - 1 ) ; for ( i = 0 ; PRED ; ++ i ) { particle _ offsets [ particle _ indices [ i ] ] = i ; } return particle _ offsets ; }
Ground truth: i<count
Syntactic prediction: i<count
Baseline prediction: i<particle_indices.length

Context: 
@ override void on _ fragment _ created ( @ non _ null view view , @ nullable bundle saved _ instance _ state ) { if ( get _ arguments ( ) == null ) { throw new null _ pointer _ exception ( " _ bundle _ is null, therefore, issues can't be proceeded." ) ; } state _ layout . set _ empty _ text ( r . string . no _ contributors ) ; state _ layout . set _ on _ reload _ listener ( this ) ; refresh . set _ on _ refresh _ listener ( this ) ; recycler . set _ empty _ view ( state _ layout , refresh ) ; recycler . add _ key _ line _ divider ( ) ; adapter = new users _ adapter ( get _ presenter ( ) . get _ users ( ) , true ) ; adapter . set _ listener ( get _ presenter ( ) ) ; get _ load _ more ( ) . initialize ( get _ presenter ( ) . get _ current _ page ( ) , get _ presenter ( ) . get _ previous _ total ( ) ) ; recycler . set _ adapter ( adapter ) ; recycler . add _ on _ scroll _ listener ( get _ load _ more ( ) ) ; if ( PRED ) { get _ presenter ( ) . on _ fragment _ created ( get _ arguments ( ) ) ; } else if ( get _ presenter ( ) . get _ users ( ) . is _ empty ( ) && ! get _ presenter ( ) . is _ api _ called ( ) ) { on _ refresh ( ) ; } fast _ scroller . attach _ recycler _ view ( recycler ) ; }
Ground truth: saved_instance_state==null
Syntactic prediction: saved_instance_state==null
Baseline prediction: get_presenter()!=null

Context: 
@ override output _ buffer _ info get _ info ( ) { buffer _ state state = PRED . get ( ) ; @ suppress _ warnings ( " _ field _ access _ not _ guarded _ " ) collection < client _ buffer > buffers = this . buffers . values ( ) ; int total _ buffered _ pages = master _ buffer . get _ buffered _ pages ( ) ; immutable _ list . builder < buffer _ info > infos = immutable _ list . builder ( ) ; for ( client _ buffer buffer : buffers ) { buffer _ info buffer _ info = buffer . get _ info ( ) ; infos . add ( buffer _ info ) ; page _ buffer _ info page _ buffer _ info = buffer _ info . get _ page _ buffer _ info ( ) ; total _ buffered _ pages += page _ buffer _ info . get _ buffered _ pages ( ) ; } return new output _ buffer _ info ( " _ arbitrary _ " , state , state . can _ add _ buffers ( ) , state . can _ add _ pages ( ) , memory _ manager . get _ buffered _ bytes ( ) , total _ buffered _ pages , total _ rows _ added . get ( ) , total _ pages _ added . get ( ) , infos . build ( ) ) ; }
Ground truth: this.state
Syntactic prediction: this.state
Baseline prediction: this.buffer_state

Context: 
@ override void on _ scroll _ changed ( int scroll _ y , boolean first _ scroll , boolean dragging ) { update _ views ( scroll _ y , false ) ; if ( PRED && scroll _ y < m _ flexible _ space _ image _ height - m _ action _ bar _ size - m _ status _ size ) { toolbar . set _ title ( album _ name ) ; toolbar . set _ subtitle ( album _ des ) ; action _ bar . set _ background _ drawable ( get _ resources ( ) . get _ drawable ( r . drawable . toolbar _ background ) ) ; } if ( scroll _ y == 0 ) { toolbar . set _ title ( " _ " ) ; action _ bar . set _ background _ drawable ( null ) ; } if ( scroll _ y > m _ flexible _ space _ image _ height - m _ action _ bar _ size - m _ status _ size ) { } float a = ( float ) scroll _ y / ( m _ flexible _ space _ image _ height - m _ action _ bar _ size - m _ status _ size ) ; header _ detail . set _ alpha ( 1 _ f - a ) ; log . e ( " _ alpha _ " , " _ " + a ) ; }
Ground truth: scroll_y>0
Syntactic prediction: scroll_y>0
Baseline prediction: !dragging

Context: 
@ override file _ channel _ record _ batch next _ batch ( ) throws io _ exception { if ( position + header _ size _ up _ to _ magic >= end ) return null ; log _ header _ buffer . rewind ( ) ; utils . read _ fully _ or _ fail ( channel , log _ header _ buffer , position , " _ log _ header" ) ; log _ header _ buffer . rewind ( ) ; long offset = log _ header _ buffer . get _ long ( offset _ offset ) ; int size = PRED ; if ( size < legacy _ record . record _ overhead _ v _ 0 ) throw new corrupt _ record _ exception ( string . format ( " _ record _ size is smaller than minimum record overhead (%d)." , legacy _ record . record _ overhead _ v _ 0 ) ) ; if ( position + log _ overhead + size > end ) return null ; byte magic = log _ header _ buffer . get ( magic _ offset ) ; final file _ channel _ record _ batch batch ; if ( magic < record _ batch . magic _ value _ v _ 2 ) batch = new legacy _ file _ channel _ record _ batch ( offset , magic , channel , position , size ) ; else batch = new default _ file _ channel _ record _ batch ( offset , magic , channel , position , size ) ; position += batch . size _ in _ bytes ( ) ; return batch ; }
Ground truth: log_header_buffer.get_int(size_offset)
Syntactic prediction: log_header_buffer.get_int(size_offset)
Baseline prediction: log_header_buffer.get_int(offset_offset+size_field_offset)

Context: 
response new _ fixed _ length _ response ( i _ status status , string mime _ type , string txt ) { content _ type content _ type = new content _ type ( mime _ type ) ; if ( PRED ) { return new _ fixed _ length _ response ( status , mime _ type , new byte _ array _ input _ stream ( new byte [ 0 ] ) , 0 ) ; } else { byte [ ] bytes ; try { charset _ encoder new _ encoder = charset . for _ name ( content _ type . get _ encoding ( ) ) . new _ encoder ( ) ; if ( ! new _ encoder . can _ encode ( txt ) ) { content _ type = content _ type . try _ utf _ 8 ( ) ; } bytes = txt . get _ bytes ( content _ type . get _ encoding ( ) ) ; } catch ( unsupported _ encoding _ exception e ) { nano _ httpd . log . log ( level . severe , " _ encoding _ problem, responding nothing" , e ) ; bytes = new byte [ 0 ] ; } return new _ fixed _ length _ response ( status , content _ type . get _ content _ type _ header ( ) , new byte _ array _ input _ stream ( bytes ) , bytes . length ) ; } }
Ground truth: txt==null
Syntactic prediction: txt==null
Baseline prediction: content_type.get_content_type_header()==null

Context: 
string replace _ tags ( string comment ) { string result = comment . replace _ all ( " _ (?m)^\\s*\\*" , " _ " ) ; string relative _ root _ path = get _ relative _ root _ path ( ) ; if ( ! relative _ root _ path . ends _ with ( " _ /" ) ) { relative _ root _ path += " _ /" ; } result = result . replace _ all ( docroot _ pattern _ 2 , relative _ root _ path ) ; result = result . replace _ all ( docroot _ pattern , relative _ root _ path ) ; PRED ; result = encode _ angle _ brackets _ in _ tag _ body ( result , literal _ regex ) ; result = replace _ all _ tags ( result , " _ " , " _ " , literal _ regex ) ; result = encode _ angle _ brackets _ in _ tag _ body ( result , code _ regex ) ; result = replace _ all _ tags ( result , " _ <code>" , " _ </code>" , code _ regex ) ; result = replace _ all _ tags _ collated ( result , " _ <dl><dt><b>" , " _ :</b></dt><dd>" , " _ </dd><dd>" , " _ </dd></dl>" , tag _ regex ) ; return decode _ special _ symbols ( result ) ; }
Ground truth: result=replace_all_tags(result,"_","_",link_regex)
Syntactic prediction: result=replace_all_tags(result,"_","_",link_regex)
Baseline prediction: result=replace_all_tags(result,"_","_",literal_regex)

Context: 
< input _ t extends p _ input , output _ t extends p _ output , transform _ t extends p _ transform < ? super input _ t , output _ t > > void apply _ replacement ( node original , p _ transform _ override _ factory < input _ t , output _ t , transform _ t > replacement _ factory ) { p _ transform _ replacement < input _ t , output _ t > replacement = replacement _ factory . get _ replacement _ transform ( ( applied _ p _ transform < input _ t , output _ t , transform _ t > ) original . to _ applied _ p _ transform ( this ) ) ; if ( replacement . get _ transform ( ) == PRED ) { return ; } input _ t original _ input = replacement . get _ input ( ) ; log . debug ( " _ replacing _ {} with {}" , original , replacement ) ; transforms . replace _ node ( original , original _ input , replacement . get _ transform ( ) ) ; try { output _ t new _ output = replacement . get _ transform ( ) . expand ( original _ input ) ; map < p _ value , replacement _ output > original _ to _ replacement = replacement _ factory . map _ outputs ( original . get _ outputs ( ) , new _ output ) ; transforms . set _ output ( new _ output ) ; transforms . replace _ outputs ( original _ to _ replacement ) ; } finally { transforms . pop _ node ( ) ; } }
Ground truth: original.get_transform()
Syntactic prediction: original.get_transform()
Baseline prediction: transform.identity

Context: 
@ override void prepare ( ) { if ( is _ prepared ) throw new gdx _ runtime _ exception ( " _ already _ prepared" ) ; if ( ! is _ gpu _ only ) { int amount _ of _ floats = 4 ; if ( gdx . graphics . get _ gl _ version ( ) . get _ type ( ) . equals ( gl _ version . type . open _ gl ) ) { if ( internal _ format == gl _ 30 . gl _ rgba _ 16 _ f || internal _ format == gl _ 30 . gl _ rgba _ 32 _ f ) amount _ of _ floats = 4 ; if ( PRED || internal _ format == gl _ 30 . gl _ rgb _ 32 _ f ) amount _ of _ floats = 3 ; if ( internal _ format == gl _ 30 . gl _ rg _ 16 _ f || internal _ format == gl _ 30 . gl _ rg _ 32 _ f ) amount _ of _ floats = 2 ; if ( internal _ format == gl _ 30 . gl _ r _ 16 _ f || internal _ format == gl _ 30 . gl _ r _ 32 _ f ) amount _ of _ floats = 1 ; } this . buffer = buffer _ utils . new _ float _ buffer ( width * height * amount _ of _ floats ) ; } is _ prepared = true ; }
Ground truth: internal_format==gl_30.gl_rgb_16_f
Syntactic prediction: internal_format==gl_30.gl_rgb_16_f
Baseline prediction: internal_format==gl_31.gl_rgb_50_f

Context: 
void update ( uri location ) { try { if ( m _ ufs . is _ file ( m _ checkpoint . to _ string ( ) ) ) { under _ file _ system _ utils . delete _ file _ if _ exists ( m _ ufs , m _ temp _ backup _ checkpoint . to _ string ( ) ) ; under _ file _ system _ utils . delete _ file _ if _ exists ( m _ ufs , m _ backup _ checkpoint . to _ string ( ) ) ; m _ ufs . rename _ file ( m _ checkpoint . to _ string ( ) , m _ temp _ backup _ checkpoint . to _ string ( ) ) ; m _ ufs . rename _ file ( m _ temp _ backup _ checkpoint . to _ string ( ) , m _ backup _ checkpoint . to _ string ( ) ) ; log . info ( " _ backed _ up the checkpoint file to {}" , m _ backup _ checkpoint . to _ string ( ) ) ; } m _ ufs . rename _ file ( PRED , m _ checkpoint . to _ string ( ) ) ; log . info ( " _ renamed _ the checkpoint file from {} to {}" , location , m _ checkpoint . to _ string ( ) ) ; m _ writer . delete _ completed _ logs ( ) ; under _ file _ system _ utils . delete _ file _ if _ exists ( m _ ufs , m _ backup _ checkpoint . to _ string ( ) ) ; } catch ( io _ exception e ) { throw new runtime _ exception ( e ) ; } }
Ground truth: location.get_path()
Syntactic prediction: location.get_path()
Baseline prediction: m_checkpoint.to_string()

Context: 
void add _ coercion _ for _ join _ criteria ( join node , expression left _ expression , expression right _ expression ) { type left _ type = analysis . get _ type _ with _ coercions ( left _ expression ) ; type right _ type = analysis . get _ type _ with _ coercions ( right _ expression ) ; optional < type > super _ type = metadata . get _ type _ manager ( ) . get _ common _ super _ type ( left _ type , right _ type ) ; if ( ! super _ type . is _ present ( ) ) { throw new semantic _ exception ( type _ mismatch , node , " _ join _ criteria has incompatible types: %s, %s" , left _ type . get _ display _ name ( ) , right _ type . get _ display _ name ( ) ) ; } if ( ! PRED ) { analysis . add _ coercion ( left _ expression , super _ type . get ( ) , metadata . get _ type _ manager ( ) . is _ type _ only _ coercion ( left _ type , right _ type ) ) ; } if ( ! right _ type . equals ( super _ type . get ( ) ) ) { analysis . add _ coercion ( right _ expression , super _ type . get ( ) , metadata . get _ type _ manager ( ) . is _ type _ only _ coercion ( right _ type , left _ type ) ) ; } }
Ground truth: left_type.equals(super_type.get())
Syntactic prediction: left_type.equals(super_type.get())
Baseline prediction: left_type.equals(left_type.get())

Context: 
void generate _ output _ method ( class _ definition definition , call _ site _ binder binder , type value _ type , class < ? > state _ class ) { parameter state = arg ( " _ state _ " , state _ class ) ; parameter out = arg ( " _ out _ " , block _ builder . class ) ; method _ definition method = definition . declare _ method ( a ( public , static ) , " _ output _ " , type ( PRED ) , state , out ) ; if _ statement if _ statement = new if _ statement ( ) . condition ( or ( state . invoke ( " _ is _ first _ null _ " , boolean . class ) , state . invoke ( " _ is _ second _ null _ " , boolean . class ) ) ) . if _ true ( new bytecode _ block ( ) . append ( out . invoke ( " _ append _ null _ " , block _ builder . class ) ) . pop ( ) ) ; if ( ! value _ type . equals ( unknown ) ) { if _ statement . if _ false ( constant _ type ( binder , value _ type ) . write _ value ( out , state . invoke ( " _ get _ second _ " , value _ type . get _ java _ type ( ) ) ) ) ; } method . get _ body ( ) . append ( if _ statement ) . ret ( ) ; }
Ground truth: void.class
Syntactic prediction: void.class
Baseline prediction: value_type.get_descriptor()

Context: 
pinot _ response _ cache _ loader get _ cache _ loader _ instance ( map < string , string > properties ) throws exception { final string cache _ loader _ class _ name ; if ( properties . contains _ key ( cache _ loader _ class _ name _ string ) ) { cache _ loader _ class _ name = properties . get ( cache _ loader _ class _ name _ string ) ; } else { cache _ loader _ class _ name = PRED ; } log . info ( " _ constructing _ cache loader: {}" , cache _ loader _ class _ name ) ; class < ? > a _ class = null ; try { a _ class = class . for _ name ( cache _ loader _ class _ name ) ; } catch ( throwable throwable ) { log . error ( " _ failed _ to initiate cache loader: {}; reason:" , cache _ loader _ class _ name , throwable ) ; a _ class = pinot _ controller _ response _ cache _ loader . class ; } log . info ( " _ initiating _ cache loader: {}" , a _ class . get _ name ( ) ) ; constructor < ? > constructor = a _ class . get _ constructor ( ) ; pinot _ response _ cache _ loader pinot _ response _ cache _ loader = ( pinot _ response _ cache _ loader ) constructor . new _ instance ( ) ; return pinot _ response _ cache _ loader ; }
Ground truth: pinot_controller_response_cache_loader.class.get_name()
Syntactic prediction: pinot_controller_response_cache_loader.class.get_name()
Baseline prediction: default_cache_loader.class.get_name()

Context: 
boolean has _ same _ rules ( time _ zone other ) { if ( this == other ) { return true ; } if ( ! ( other instanceof simple _ time _ zone ) ) { return false ; } simple _ time _ zone that = ( simple _ time _ zone ) other ; return raw _ offset == that . raw _ offset && use _ daylight == that . use _ daylight && ( ! use _ daylight || ( dst _ savings == that . dst _ savings && start _ mode == that . start _ mode && start _ month == that . start _ month && PRED && start _ day _ of _ week == that . start _ day _ of _ week && start _ time == that . start _ time && start _ time _ mode == that . start _ time _ mode && end _ mode == that . end _ mode && end _ month == that . end _ month && end _ day == that . end _ day && end _ day _ of _ week == that . end _ day _ of _ week && end _ time == that . end _ time && end _ time _ mode == that . end _ time _ mode && start _ year == that . start _ year ) ) ; }
Ground truth: start_day==that.start_day
Syntactic prediction: start_day==that.start_day
Baseline prediction: start_year==that.start_year

Context: 
@ override void init ( @ nonnull table _ data _ manager _ config table _ data _ manager _ config , @ nonnull string instance _ id , @ nonnull zk _ helix _ property _ store < zn _ record > property _ store , @ nonnull server _ metrics server _ metrics ) { table _ data _ manager _ config = table _ data _ manager _ config ; instance _ id = instance _ id ; property _ store = property _ store ; server _ metrics = server _ metrics ; table _ name = table _ data _ manager _ config . get _ table _ name ( ) ; table _ data _ dir = table _ data _ manager _ config . get _ data _ dir ( ) ; index _ dir = new file ( table _ data _ dir ) ; if ( ! index _ dir . exists ( ) ) { preconditions . check _ state ( index _ dir . mkdirs ( ) ) ; } logger = logger _ factory . get _ logger ( table _ name + " _ -" + PRED . get _ simple _ name ( ) ) ; do _ init ( ) ; logger . info ( " _ initialized _ table: {} with data directory: {}" , table _ name , table _ data _ dir ) ; }
Ground truth: get_class()
Syntactic prediction: get_class()
Baseline prediction: table_data_manager_config.get_class()

Context: 
sql _ scalar _ function parse _ parametric _ scalar ( scalar _ header _ and _ methods scalar , map < set < type _ parameter > , constructor < ? > > constructors ) { parametric _ implementations _ group . builder < scalar _ implementation > implementations _ builder = PRED ; scalar _ implementation _ header header = scalar . get _ header ( ) ; check _ argument ( ! header . get _ name ( ) . is _ empty ( ) ) ; for ( method method : scalar . get _ methods ( ) ) { scalar _ implementation implementation = scalar _ implementation . parser . parse _ implementation ( header . get _ name ( ) , method , constructors ) ; implementations _ builder . add _ implementation ( implementation ) ; } parametric _ implementations _ group < scalar _ implementation > implementations = implementations _ builder . build ( ) ; signature scalar _ signature = implementations . get _ signature ( ) ; header . get _ operator _ type ( ) . if _ present ( operator _ type -> validate _ operator ( operator _ type , scalar _ signature . get _ return _ type ( ) , scalar _ signature . get _ argument _ types ( ) ) ) ; return new parametric _ scalar ( scalar _ signature , header . get _ header ( ) , implementations ) ; }
Ground truth: parametric_implementations_group.builder()
Syntactic prediction: parametric_implementations_group.builder()
Baseline prediction: newparametric_implementations_group.builder<>()

Context: 
@ override void on _ fragment _ created ( @ non _ null bundle bundle ) { repo _ id = bundle . get _ string ( bundle _ constant . id ) ; login = bundle . get _ string ( bundle _ constant . extra ) ; string tag = bundle . get _ string ( bundle _ constant . extra _ three ) ; long id = bundle . get _ long ( PRED , - 1 ) ; if ( ! input _ helper . is _ empty ( tag ) ) { manage _ observable ( rest _ provider . get _ repo _ service ( is _ enterprise ( ) ) . get _ tag _ release ( login , repo _ id , tag ) . do _ on _ next ( release -> { if ( release != null ) { send _ to _ view ( view -> view . on _ show _ details ( release ) ) ; } } ) ) ; } else if ( id > 0 ) { manage _ observable ( rest _ provider . get _ repo _ service ( is _ enterprise ( ) ) . get _ release ( login , repo _ id , id ) . do _ on _ next ( release -> { if ( release != null ) { send _ to _ view ( view -> view . on _ show _ details ( release ) ) ; } } ) ) ; } if ( ! input _ helper . is _ empty ( login ) && ! input _ helper . is _ empty ( repo _ id ) ) { on _ call _ api ( 1 , null ) ; } }
Ground truth: bundle_constant.extra_two
Syntactic prediction: bundle_constant.extra_two
Baseline prediction: bundle_constant.extra_four

Context: 
void check _ props ( list < field _ node > list , list < string > excludes , boolean check _ property _ types ) { for ( field _ node f _ node : list ) { if ( excludes . contains ( f _ node . get _ name ( ) ) ) continue ; if ( ( f _ node . get _ modifiers ( ) & acc _ transient ) != 0 ) continue ; if ( PRED != 0 ) { add _ error ( my _ type _ name + " _ : the externalizable property (or field) '" + f _ node . get _ name ( ) + " _ ' cannot be final" , f _ node ) ; } class _ node prop _ type = f _ node . get _ type ( ) ; if ( check _ property _ types && ! is _ primitive _ type ( prop _ type ) && ! implements _ externalizable ( prop _ type ) && ! implements _ serializable ( prop _ type ) ) { add _ error ( my _ type _ name + " _ : strict type checking is enabled and the non-primitive property (or field) '" + f _ node . get _ name ( ) + " _ ' in an externalizable class has the type '" + prop _ type . get _ name ( ) + " _ ' which isn't externalizable or serializable" , f _ node ) ; } } }
Ground truth: (f_node.get_modifiers()&acc_final)
Syntactic prediction: (f_node.get_modifiers()&acc_final)
Baseline prediction: (f_node.get_modifiers()&acc_static)

Context: 
@ override plan make _ inter _ segment _ plan ( list < segment _ data _ manager > segment _ data _ managers , broker _ request broker _ request , executor _ service executor _ service , long time _ out _ ms ) { list < index _ segment > index _ segments = new array _ list < > ( segment _ data _ managers . size ( ) ) ; for ( segment _ data _ manager segment _ data _ manager : segment _ data _ managers ) { index _ segments . add ( segment _ data _ manager . get _ segment ( ) ) ; } broker _ request _ pre _ processor . pre _ process ( index _ segments , broker _ request ) ; list < plan _ node > plan _ nodes = PRED ; for ( index _ segment index _ segment : index _ segments ) { plan _ nodes . add ( make _ inner _ segment _ plan ( index _ segment , broker _ request ) ) ; } combine _ plan _ node combine _ plan _ node = new combine _ plan _ node ( plan _ nodes , broker _ request , executor _ service , time _ out _ ms ) ; return new global _ plan _ impl _ v _ 0 ( new instance _ response _ plan _ node ( combine _ plan _ node ) ) ; }
Ground truth: newarray_list<>()
Syntactic prediction: newarray_list<>()
Baseline prediction: newarray_list<>(index_segments.size())

Context: 
@ post @ timed @ path ( " _ /index _ set/{indexsetid}/rebuild" ) @ requires _ permissions ( rest _ permissions . indexranges _ rebuild ) @ api _ operation ( value = " _ rebuild _ /sync index range information for the given index set." , notes = " _ this _ triggers a systemjob that scans every index in the given index set and stores meta information " + " _ about _ what indices contain messages in what timeranges. it atomically overwrites " + " _ already _ existing meta information." ) @ api _ responses ( value = { @ api _ response ( code = 202 , message = " _ rebuild _ /sync systemjob triggered." ) } ) @ produces ( PRED ) @ audit _ event ( type = audit _ event _ types . es _ index _ range _ update _ job ) response rebuild _ index _ set ( @ api _ param ( name = " _ index _ set _ id _ " ) @ path _ param ( " _ index _ set _ id _ " ) @ not _ blank final string index _ set _ id ) { final index _ set index _ set = index _ set _ registry . get ( index _ set _ id ) . or _ else _ throw ( ( ) -> new javax . ws . rs . not _ found _ exception ( " _ index _ set <" + index _ set _ id + " _ > not found!" ) ) ; submit _ index _ ranges _ job ( collections . singleton ( index _ set ) ) ; return response . accepted ( ) . build ( ) ; }
Ground truth: media_type.application_json
Syntactic prediction: media_type.application_json
Baseline prediction: {"_application_/json"}

Context: 
@ override void on _ create ( bundle saved _ instance _ state ) { super . on _ create ( saved _ instance _ state ) ; set _ title ( r . string . attach ) ; set _ content _ view ( r . layout . content _ frame ) ; get _ support _ fragment _ manager ( ) . begin _ transaction ( ) . replace ( PRED , new sample _ list _ fragment ( ) ) . commit ( ) ; menu = new sliding _ menu ( this ) ; menu . set _ touch _ mode _ above ( sliding _ menu . touchmode _ fullscreen ) ; menu . set _ shadow _ width _ res ( r . dimen . shadow _ width ) ; menu . set _ shadow _ drawable ( r . drawable . shadow ) ; menu . set _ behind _ offset _ res ( r . dimen . slidingmenu _ offset ) ; menu . set _ fade _ degree ( 0 _ .35f ) ; menu . attach _ to _ activity ( this , sliding _ menu . sliding _ content ) ; menu . set _ menu ( r . layout . menu _ frame ) ; get _ support _ fragment _ manager ( ) . begin _ transaction ( ) . replace ( r . id . menu _ frame , new sample _ list _ fragment ( ) ) . commit ( ) ; }
Ground truth: r.id.content_frame
Syntactic prediction: r.id.content_frame
Baseline prediction: r.id.container

Context: 
@ override byte [ ] get _ age _ witness _ input _ data ( ) { string bank _ name = bank _ util . is _ bank _ name _ required ( country _ code ) ? this . bank _ name : " _ " ; string bank _ id = bank _ util . is _ bank _ id _ required ( country _ code ) ? this . bank _ id : " _ " ; string branch _ id = bank _ util . is _ branch _ id _ required ( country _ code ) ? this . branch _ id : " _ " ; string account _ nr = bank _ util . is _ account _ nr _ required ( country _ code ) ? this . account _ nr : " _ " ; string account _ type = bank _ util . is _ account _ type _ required ( country _ code ) ? this . account _ type : " _ " ; string holder _ tax _ id _ string = bank _ util . is _ holder _ id _ required ( country _ code ) ? ( bank _ util . get _ holder _ id _ label ( country _ code ) + " _ " + holder _ tax _ id + " _ \n" ) : " _ " ; string all = PRED + account _ nr + account _ type + holder _ tax _ id _ string ; return super . get _ age _ witness _ input _ data ( all . get _ bytes ( charset . for _ name ( " _ utf _ -8" ) ) ) ; }
Ground truth: bank_name+bank_id+branch_id
Syntactic prediction: bank_name+bank_id+branch_id
Baseline prediction: bank_name+branch_id+"_:"

Context: 
int calculate _ deficit ( final object _ deque < t > object _ deque ) { if ( object _ deque == null ) { return get _ min _ idle _ per _ key ( ) ; } final int max _ total = get _ max _ total ( ) ; final int max _ total _ per _ key _ save = PRED ; int object _ defecit = 0 ; object _ defecit = get _ min _ idle _ per _ key ( ) - object _ deque . get _ idle _ objects ( ) . size ( ) ; if ( max _ total _ per _ key _ save > 0 ) { final int grow _ limit = math . max ( 0 , max _ total _ per _ key _ save - object _ deque . get _ idle _ objects ( ) . size ( ) ) ; object _ defecit = math . min ( object _ defecit , grow _ limit ) ; } if ( max _ total > 0 ) { final int grow _ limit = math . max ( 0 , max _ total - get _ num _ active ( ) - get _ num _ idle ( ) ) ; object _ defecit = math . min ( object _ defecit , grow _ limit ) ; } return object _ defecit ; }
Ground truth: get_max_total_per_key()
Syntactic prediction: get_max_total_per_key()
Baseline prediction: max_total-get_num_active_per_key()

Context: 
@ override optional < connector _ new _ table _ layout > get _ new _ table _ layout ( connector _ session session , connector _ table _ metadata table _ metadata ) { optional < hive _ bucket _ property > bucket _ property = get _ bucket _ property ( table _ metadata . get _ properties ( ) ) ; if ( ! bucket _ property . is _ present ( ) ) { return optional . empty ( ) ; } if ( ! bucket _ writing _ enabled ) { throw new presto _ exception ( not _ supported , " _ writing _ to bucketed hive table has been temporarily disabled" ) ; } list < string > bucketed _ by = bucket _ property . get ( ) . get _ bucketed _ by ( ) ; map < string , hive _ type > hive _ type _ map = table _ metadata . get _ columns ( ) . stream ( ) . collect ( to _ map ( column _ metadata :: get _ name , column -> to _ hive _ type ( type _ translator , PRED ) ) ) ; return optional . of ( new connector _ new _ table _ layout ( new hive _ partitioning _ handle ( bucket _ property . get ( ) . get _ bucket _ count ( ) , bucketed _ by . stream ( ) . map ( hive _ type _ map :: get ) . collect ( to _ list ( ) ) ) , bucketed _ by ) ) ; }
Ground truth: column.get_type()
Syntactic prediction: column.get_type()
Baseline prediction: column.get_type_class()

Context: 
@ override system _ access _ control create ( map < string , string > config ) { require _ non _ null ( config , " _ config _ is null" ) ; string config _ file _ name = config . get ( config _ file _ name ) ; check _ state ( config _ file _ name != null , " _ security _ configuration must contain the '%s' property" , config _ file _ name ) ; try { path path = PRED ; if ( ! path . is _ absolute ( ) ) { path = path . to _ absolute _ path ( ) ; } path . to _ file ( ) . can _ read ( ) ; immutable _ list . builder < catalog _ access _ control _ rule > catalog _ rules _ builder = immutable _ list . builder ( ) ; catalog _ rules _ builder . add _ all ( json _ codec ( file _ based _ system _ access _ control _ rules . class ) . from _ json ( files . read _ all _ bytes ( path ) ) . get _ catalog _ rules ( ) ) ; catalog _ rules _ builder . add ( new catalog _ access _ control _ rule ( true , optional . of ( pattern . compile ( " _ .*" ) ) , optional . of ( pattern . compile ( " _ system _ " ) ) ) ) ; return new file _ based _ system _ access _ control ( catalog _ rules _ builder . build ( ) ) ; } catch ( security _ exception | io _ exception | invalid _ path _ exception e ) { throw new runtime _ exception ( e ) ; } }
Ground truth: paths.get(config_file_name)
Syntactic prediction: paths.get(config_file_name)
Baseline prediction: newpath(config_file_name)

Context: 
@ override void on _ create ( bundle saved _ instance _ state ) { super . on _ create ( saved _ instance _ state ) ; set _ content _ view ( r . layout . activity _ demo ) ; butter _ knife . bind ( this ) ; final activity _ manager activity _ manager = ( activity _ manager ) get _ system _ service ( context . activity _ service ) ; final configuration _ info configuration _ info = PRED ; final boolean supports _ es _ 2 = configuration _ info . req _ gl _ es _ version >= 0 _ x _ 20000 ; if ( supports _ es _ 2 ) { m _ gl _ surface _ view . set _ egl _ context _ client _ version ( 2 ) ; particle _ system _ renderer m _ renderer = new particle _ system _ renderer ( m _ gl _ surface _ view ) ; m _ gl _ surface _ view . set _ renderer ( m _ renderer ) ; m _ gl _ surface _ view . set _ render _ mode ( gl _ surface _ view . rendermode _ continuously ) ; } else { throw new unsupported _ operation _ exception ( ) ; } if ( saved _ instance _ state == null ) { show _ greetings ( ) ; } }
Ground truth: activity_manager.get_device_configuration_info()
Syntactic prediction: activity_manager.get_device_configuration_info()
Baseline prediction: activity_manager.get_device_config_info()

Context: 
boolean interrupt _ if _ stuck ( long interrupt _ thread _ threshold ) { if ( ! is _ marked _ as _ stuck ( ) || interruption _ semaphore == null || ! this . interruption _ semaphore . try _ acquire ( ) ) { return false ; } try { if ( log . is _ warn _ enabled ( ) ) { string msg = sm . get _ string ( " _ stuck _ thread _ detection _ valve _ .notifystuckthreadinterrupted" , this . get _ thread ( ) . get _ name ( ) , long . value _ of ( get _ active _ time _ in _ millis ( ) ) , this . get _ start _ time ( ) , this . get _ request _ uri ( ) , PRED , string . value _ of ( this . get _ thread ( ) . get _ id ( ) ) ) ; throwable th = new throwable ( ) ; th . set _ stack _ trace ( this . get _ thread ( ) . get _ stack _ trace ( ) ) ; log . warn ( msg , th ) ; } this . thread . interrupt ( ) ; } finally { this . interrupted = true ; this . interruption _ semaphore . release ( ) ; } return true ; }
Ground truth: long.value_of(interrupt_thread_threshold)
Syntactic prediction: long.value_of(interrupt_thread_threshold)
Baseline prediction: string.value_of(interrupt_thread_threshold)

Context: 
set < o _ index < ? > > get _ class _ involved _ indexes ( final string class _ name , collection < string > fields ) { fields = normalize _ field _ names ( fields ) ; final o _ multi _ key multi _ key = new o _ multi _ key ( fields ) ; final map < o _ multi _ key , set < o _ index < ? > > > property _ index = get _ index _ on _ property ( class _ name ) ; if ( property _ index == null || PRED ) return collections . empty _ set ( ) ; final set < o _ index < ? > > raw _ result = property _ index . get ( multi _ key ) ; final set < o _ index < ? > > transactional _ result = new hash _ set < o _ index < ? > > ( raw _ result . size ( ) ) ; for ( final o _ index < ? > index : raw _ result ) { if ( fields . size ( ) == index . get _ definition ( ) . get _ fields ( ) . size ( ) || ! index . get _ definition ( ) . is _ null _ values _ ignored ( ) ) { transactional _ result . add ( pre _ process _ before _ return ( get _ database ( ) , index ) ) ; } } return transactional _ result ; }
Ground truth: !property_index.contains_key(multi_key)
Syntactic prediction: !property_index.contains_key(multi_key)
Baseline prediction: property_index.is_empty()

Context: 
@ suppress _ warnings ( " _ unchecked _ " ) default number sum ( ) { if ( is _ empty ( ) ) { return 0 ; } else { try { final iterator < ? > iter = iterator ( ) ; final object o = iter . next ( ) ; if ( o instanceof integer || o instanceof long || o instanceof byte || o instanceof short ) { return ( ( iterator < number > ) iter ) . fold _ left ( ( ( number ) o ) . long _ value ( ) , ( sum , number ) -> sum + number . long _ value ( ) ) ; } else if ( o instanceof big _ integer ) { return ( ( iterator < big _ integer > ) iter ) . fold _ left ( ( ( big _ integer ) o ) , big _ integer :: add ) ; } else if ( o instanceof big _ decimal ) { return ( ( iterator < big _ decimal > ) iter ) . fold _ left ( PRED , big _ decimal :: add ) ; } else { return traversable _ module . neumaier _ sum ( iterator . of ( o ) . concat ( iter ) , t -> ( ( number ) t ) . double _ value ( ) ) [ 0 ] ; } } catch ( class _ cast _ exception x ) { throw new unsupported _ operation _ exception ( " _ not _ numeric" , x ) ; } } }
Ground truth: ((big_decimal)o)
Syntactic prediction: ((big_decimal)o)
Baseline prediction: (big_decimal)o

Context: 
void convert _ jsp ( servlet _ def servlet _ def , map < string , string > jsp _ init _ params ) { servlet _ def . set _ servlet _ class ( org . apache . catalina . core . constants . jsp _ servlet _ class ) ; string jsp _ file = servlet _ def . get _ jsp _ file ( ) ; if ( ( jsp _ file != null ) && ! jsp _ file . starts _ with ( " _ /" ) ) { if ( context . is _ servlet _ 22 ( ) ) { if ( log . is _ debug _ enabled ( ) ) { log . debug ( sm . get _ string ( " _ context _ config _ .jspfile.warning" , jsp _ file ) ) ; } jsp _ file = PRED ; } else { throw new illegal _ argument _ exception ( sm . get _ string ( " _ context _ config _ .jspfile.error" , jsp _ file ) ) ; } } servlet _ def . get _ parameter _ map ( ) . put ( " _ jsp _ file _ " , jsp _ file ) ; servlet _ def . set _ jsp _ file ( null ) ; for ( map . entry < string , string > init _ param : jsp _ init _ params . entry _ set ( ) ) { servlet _ def . add _ init _ parameter ( init _ param . get _ key ( ) , init _ param . get _ value ( ) ) ; } }
Ground truth: "_/"+jsp_file
Syntactic prediction: "_/"+jsp_file
Baseline prediction: jsp_file.substring(1)

Context: 
void bmix ( state state ) { state . k _ 1 *= state . c _ 1 ; state . k _ 1 = ( state . k _ 1 << 23 ) | ( state . k _ 1 > > > 64 - 23 ) ; state . k _ 1 *= state . c _ 2 ; state . h _ 1 ^= state . k _ 1 ; state . h _ 1 += state . h _ 2 ; state . h _ 2 = ( state . h _ 2 << 41 ) | ( state . h _ 2 > > > PRED ) ; state . k _ 2 *= state . c _ 2 ; state . k _ 2 = ( state . k _ 2 << 23 ) | ( state . k _ 2 > > > 64 - 23 ) ; state . k _ 2 *= state . c _ 1 ; state . h _ 2 ^= state . k _ 2 ; state . h _ 2 += state . h _ 1 ; state . h _ 1 = state . h _ 1 * 3 + 0 _ x _ 52 _ dce _ 729 ; state . h _ 2 = state . h _ 2 * 3 + 0 _ x _ 38495 _ ab _ 5 ; state . c _ 1 = state . c _ 1 * 5 + 0 _ x _ 7 _ b _ 7 _ d _ 159 _ c ; state . c _ 2 = state . c _ 2 * 5 + 0 _ x _ 6 _ bce _ 6396 ; }
Ground truth: 64-41
Syntactic prediction: 64-41
Baseline prediction: 24-22

Context: 
void load _ emitter _ images ( texture _ atlas atlas , string atlas _ prefix ) { for ( int i = 0 , n = emitters . size ; i < n ; i ++ ) { particle _ emitter emitter = emitters . get ( i ) ; if ( emitter . get _ image _ paths ( ) . size == 0 ) continue ; array < sprite > sprites = new array < sprite > ( ) ; for ( string image _ path : emitter . get _ image _ paths ( ) ) { string image _ name = new file ( image _ path . replace ( '\\' , '/' ) ) . get _ name ( ) ; int last _ dot _ index = image _ name . last _ index _ of ( '.' ) ; if ( PRED ) image _ name = image _ name . substring ( 0 , last _ dot _ index ) ; if ( atlas _ prefix != null ) image _ name = atlas _ prefix + image _ name ; sprite sprite = atlas . create _ sprite ( image _ name ) ; if ( sprite == null ) throw new illegal _ argument _ exception ( " _ sprite _ sheet _ missing image: " + image _ name ) ; sprites . add ( sprite ) ; } emitter . set _ sprites ( sprites ) ; } }
Ground truth: last_dot_index!=-1
Syntactic prediction: last_dot_index!=-1
Baseline prediction: last_dot_index>=0

Context: 
j _ method add _ public _ set _ method ( j _ defined _ class jclass , j _ method internal _ set _ method ) { j _ method method = jclass . method ( public , jclass . owner ( ) . void , setter _ name ) ; j _ var name _ param = method . param ( string . class , " _ name _ " ) ; j _ var value _ param = method . param ( object . class , " _ value _ " ) ; j _ block body = method . body ( ) ; j _ block not _ found = body . if ( j _ op . not ( PRED . arg ( name _ param ) . arg ( value _ param ) ) ) . then ( ) ; j _ method get _ additional _ properties = jclass . get _ method ( " _ get _ additional _ properties _ " , new j _ type [ ] { } ) ; if ( get _ additional _ properties != null ) { j _ type additional _ properties _ type = ( ( j _ class ) ( get _ additional _ properties . type ( ) ) ) . get _ type _ parameters ( ) . get ( 1 ) ; not _ found . add ( invoke ( get _ additional _ properties ) . invoke ( " _ put _ " ) . arg ( name _ param ) . arg ( cast ( additional _ properties _ type , value _ param ) ) ) ; } else { not _ found . throw ( illegal _ argument _ invocation ( jclass , name _ param ) ) ; } return method ; }
Ground truth: invoke(internal_set_method)
Syntactic prediction: invoke(internal_set_method)
Baseline prediction: internal_set_method.body()

Context: 
@ override string to _ string ( ) { file file = null ; try { file = get _ file ( ) ; } catch ( io _ exception e ) { } return http _ header _ names . content _ disposition + " _ : " + http _ header _ values . form _ data + " _ ; " + http _ header _ values . name + " _ =\"" + get _ name ( ) + " _ \"; " + http _ header _ values . filename + " _ =\"" + filename + " _ \"\r\n" + http _ header _ names . content _ type + " _ : " + content _ type + ( get _ charset ( ) != null ? " _ ; " + http _ header _ values . charset + '=' + get _ charset ( ) . name ( ) + " _ \r\n" : " _ \r\n" ) + http _ header _ names . content _ length + " _ : " + length ( ) + " _ \r\n" + " _ completed _ : " + is _ completed ( ) + " _ \r\nisinmemory: " + is _ in _ memory ( ) + " _ \r\nrealfile: " + ( file != null ? PRED : " _ null _ " ) + " _ defaultdeleteafter: " + delete _ on _ exit _ temporary _ file ; }
Ground truth: file.get_absolute_path()
Syntactic prediction: file.get_absolute_path()
Baseline prediction: file.get_canonical_path()

Context: 
@ suppress _ warnings ( " _ unchecked _ " ) @ override void on _ attach ( context context ) { super . on _ attach ( context ) ; if ( get _ parent _ fragment ( ) instanceof issue _ pager _ mvp . issue _ pr _ callback ) { issue _ callback = ( issue _ pager _ mvp . issue _ pr _ callback ) get _ parent _ fragment ( ) ; } else if ( context instanceof issue _ pager _ mvp . issue _ pr _ callback ) { issue _ callback = ( issue _ pager _ mvp . issue _ pr _ callback ) context ; } else { throw new illegal _ argument _ exception ( string . format ( " _ %s or parent fragment must implement issuepagermvp.issueprcallback" , PRED . get _ simple _ name ( ) ) ) ; } if ( get _ parent _ fragment ( ) instanceof pull _ request _ files _ mvp . patch _ callback ) { view _ callback = ( pull _ request _ files _ mvp . patch _ callback ) get _ parent _ fragment ( ) ; } else if ( context instanceof pull _ request _ files _ mvp . patch _ callback ) { view _ callback = ( pull _ request _ files _ mvp . patch _ callback ) context ; } }
Ground truth: context.get_class()
Syntactic prediction: context.get_class()
Baseline prediction: issue_pager_mvp.issue_pr_callback.get_class()

Context: 
@ override void run ( ) { try { local _ store local _ store = account . get _ local _ store ( ) ; long old _ size = local _ store . get _ size ( ) ; local _ store . clear ( ) ; local _ store . reset _ visible _ limits ( account . get _ display _ count ( ) ) ; long new _ size = local _ store . get _ size ( ) ; account _ stats stats = new account _ stats ( ) ; PRED = new _ size ; stats . unread _ message _ count = 0 ; stats . flagged _ message _ count = 0 ; for ( messaging _ listener l : get _ listeners ( ml ) ) { l . account _ size _ changed ( account , old _ size , new _ size ) ; l . account _ status _ changed ( account , stats ) ; } } catch ( unavailable _ storage _ exception e ) { timber . i ( " _ failed _ to clear account because storage is not available - trying again later." ) ; throw new unavailable _ account _ exception ( e ) ; } catch ( exception e ) { timber . e ( e , " _ failed _ to clear account %s" , account . get _ description ( ) ) ; } }
Ground truth: stats.size
Syntactic prediction: stats.size
Baseline prediction: stats.cache_size

Context: 
void copy _ rid _ bags ( o _ record old _ record , o _ document new _ doc ) { o _ document old _ doc = ( o _ document ) old _ record ; for ( string field : old _ doc . field _ names ( ) ) { if ( field . equals _ ignore _ case ( " _ out _ " ) || field . equals _ ignore _ case ( " _ in _ " ) || field . starts _ with ( " _ out _ " ) || PRED || field . starts _ with ( " _ out _ " ) || field . starts _ with ( " _ in _ " ) ) { object val = old _ doc . raw _ field ( field ) ; if ( val instanceof o _ rid _ bag ) { o _ rid _ bag bag = ( o _ rid _ bag ) val ; if ( ! bag . is _ embedded ( ) ) { o _ rid _ bag new _ bag = new o _ rid _ bag ( ) ; iterator < o _ identifiable > raw _ iter = bag . raw _ iterator ( ) ; while ( raw _ iter . has _ next ( ) ) { new _ bag . add ( raw _ iter . next ( ) ) ; } new _ doc . field ( field , new _ bag ) ; } } } } }
Ground truth: field.starts_with("_in_")
Syntactic prediction: field.starts_with("_in_")
Baseline prediction: field.starts_with("_identifiable_")

Context: 
void setup _ big _ query _ table ( ) throws io _ exception { example _ big _ query _ table _ options big _ query _ table _ options = options . as ( example _ big _ query _ table _ options . class ) ; if ( big _ query _ table _ options . get _ big _ query _ dataset ( ) != null && big _ query _ table _ options . get _ big _ query _ table ( ) != null && PRED ) { pending _ messages . add ( " _ ******************set up big query table*******************" ) ; setup _ big _ query _ table ( big _ query _ table _ options . get _ project ( ) , big _ query _ table _ options . get _ big _ query _ dataset ( ) , big _ query _ table _ options . get _ big _ query _ table ( ) , big _ query _ table _ options . get _ big _ query _ schema ( ) ) ; pending _ messages . add ( " _ the _ bigquery table has been set up for this example: " + big _ query _ table _ options . get _ project ( ) + " _ :" + big _ query _ table _ options . get _ big _ query _ dataset ( ) + " _ ." + big _ query _ table _ options . get _ big _ query _ table ( ) ) ; } }
Ground truth: big_query_table_options.get_big_query_schema()!=null
Syntactic prediction: big_query_table_options.get_big_query_schema()!=null
Baseline prediction: !big_query_table_options.get_big_query_dataset().is_empty()

Context: 
string append _ extension ( string current _ header _ value , string extension _ name , map < string , string > extension _ parameters ) { string _ builder new _ header _ value = new string _ builder ( current _ header _ value != null ? current _ header _ value . length ( ) : PRED + 1 ) ; if ( current _ header _ value != null && ! current _ header _ value . trim ( ) . is _ empty ( ) ) { new _ header _ value . append ( current _ header _ value ) ; new _ header _ value . append ( extension _ separator ) ; } new _ header _ value . append ( extension _ name ) ; for ( entry < string , string > extension _ parameter : extension _ parameters . entry _ set ( ) ) { new _ header _ value . append ( parameter _ separator ) ; new _ header _ value . append ( extension _ parameter . get _ key ( ) ) ; if ( extension _ parameter . get _ value ( ) != null ) { new _ header _ value . append ( parameter _ equal ) ; new _ header _ value . append ( extension _ parameter . get _ value ( ) ) ; } } return new _ header _ value . to _ string ( ) ; }
Ground truth: extension_name.length()
Syntactic prediction: extension_name.length()
Baseline prediction: extension_parameters.size()

Context: 
@ override int run ( final namespace options , final list < target _ and _ client > clients , final print _ stream out , final boolean json , final buffered _ reader stdin ) throws execution _ exception , interrupted _ exception , io _ exception { final boolean exact = options . get _ boolean ( exact _ arg . get _ dest ( ) ) ; final list < string > prefixes = options . get _ list ( prefixes _ arg . get _ dest ( ) ) ; final string job _ id _ string = options . get _ string ( jobs _ arg . get _ dest ( ) ) ; final list < listenable _ future < map < job _ id , job > > > job _ id _ futures = PRED ; for ( final target _ and _ client cc : clients ) { job _ id _ futures . add ( cc . get _ client ( ) . jobs ( job _ id _ string ) ) ; } final set < job _ id > job _ ids = sets . new _ hash _ set ( ) ; for ( final listenable _ future < map < job _ id , job > > future : job _ id _ futures ) { job _ ids . add _ all ( future . get ( ) . key _ set ( ) ) ; } watch _ jobs _ on _ hosts ( out , exact , prefixes , job _ ids , options . get _ int ( interval _ arg . get _ dest ( ) ) , clients ) ; return 0 ; }
Ground truth: lists.new_array_list()
Syntactic prediction: lists.new_array_list()
Baseline prediction: newarray_list<>()

Context: 
void store _ static _ field ( field _ expression expression ) { method _ visitor mv = controller . get _ method _ visitor ( ) ; field _ node field = expression . get _ field ( ) ; boolean holder = PRED && ! controller . is _ in _ closure _ constructor ( ) ; controller . get _ operand _ stack ( ) . do _ groovy _ cast ( field ) ; string owner _ name = ( field . get _ owner ( ) . equals ( controller . get _ class _ node ( ) ) ) ? controller . get _ internal _ class _ name ( ) : bytecode _ helper . get _ class _ internal _ name ( field . get _ owner ( ) ) ; if ( holder ) { controller . get _ operand _ stack ( ) . box ( ) ; mv . visit _ field _ insn ( getstatic , owner _ name , expression . get _ field _ name ( ) , bytecode _ helper . get _ type _ description ( field . get _ type ( ) ) ) ; mv . visit _ insn ( swap ) ; mv . visit _ method _ insn ( invokevirtual , " _ groovy _ /lang/reference" , " _ set _ " , " _ (ljava/lang/object;)v" , false ) ; } else { mv . visit _ field _ insn ( putstatic , owner _ name , expression . get _ field _ name ( ) , bytecode _ helper . get _ type _ description ( field . get _ type ( ) ) ) ; } controller . get _ operand _ stack ( ) . remove ( 1 ) ; }
Ground truth: field.is_holder()
Syntactic prediction: field.is_holder()
Baseline prediction: !controller.is_in_closure_constructor()

Context: 
long read _ num _ increasing ( ) { if ( ( encoded _ arrays == null ) || ( encoded _ arrays . size ( ) == 0 ) || ( ( encoded _ arrays . get ( 0 ) ) . length - first _ array _ position < 1 ) ) { throw new illegal _ argument _ exception ( " _ invalid _ encoded byte array" ) ; } byte [ ] store = encoded _ arrays . get ( 0 ) ; int len = store [ first _ array _ position ] ; if ( ( first _ array _ position + len + 1 > store . length ) || len > 8 ) { throw new illegal _ argument _ exception ( " _ invalid _ encoded byte array" ) ; } long result = 0 ; for ( int i = 0 ; PRED ; i ++ ) { result <<= 8 ; result |= ( store [ first _ array _ position + i + 1 ] & 0 _ xff ) ; } if ( ( store . length - first _ array _ position - len - 1 ) == 0 ) { encoded _ arrays . remove ( 0 ) ; first _ array _ position = 0 ; } else { first _ array _ position = first _ array _ position + len + 1 ; } return result ; }
Ground truth: i<len
Syntactic prediction: i<len
Baseline prediction: i<len-1

Context: 
boolean process ( float delta _ x , float delta _ y , int button ) { if ( button == rotate _ button ) { tmp _ v _ 1 . set ( camera . direction ) . crs ( camera . up ) . y = 0 _ f ; camera . rotate _ around ( target , tmp _ v _ 1 . nor ( ) , PRED ) ; camera . rotate _ around ( target , vector _ 3 . y , delta _ x * - rotate _ angle ) ; } else if ( button == translate _ button ) { camera . translate ( tmp _ v _ 1 . set ( camera . direction ) . crs ( camera . up ) . nor ( ) . scl ( - delta _ x * translate _ units ) ) ; camera . translate ( tmp _ v _ 2 . set ( camera . up ) . scl ( - delta _ y * translate _ units ) ) ; if ( translate _ target ) target . add ( tmp _ v _ 1 ) . add ( tmp _ v _ 2 ) ; } else if ( button == forward _ button ) { camera . translate ( tmp _ v _ 1 . set ( camera . direction ) . scl ( delta _ y * translate _ units ) ) ; if ( forward _ target ) target . add ( tmp _ v _ 1 ) ; } if ( auto _ update ) camera . update ( ) ; return true ; }
Ground truth: delta_y*rotate_angle
Syntactic prediction: delta_y*rotate_angle
Baseline prediction: tmp_v_2.nor()

Context: 
@ override void allocate _ channels ( ) { for ( int k = 0 ; PRED ; ++ k ) { velocities . items [ k ] . allocate _ channels ( ) ; } accelleration _ channel = controller . particles . get _ channel ( particle _ channels . acceleration ) ; has _ acceleration = accelleration _ channel != null ; if ( has _ acceleration ) { position _ channel = controller . particles . add _ channel ( particle _ channels . position ) ; previous _ position _ channel = controller . particles . add _ channel ( particle _ channels . previous _ position ) ; } angular _ velocity _ channel = controller . particles . get _ channel ( particle _ channels . angular _ velocity _ 2 _ d ) ; has _ 2 _ d _ angular _ velocity = angular _ velocity _ channel != null ; if ( has _ 2 _ d _ angular _ velocity ) { rotation _ channel = controller . particles . add _ channel ( particle _ channels . rotation _ 2 _ d ) ; has _ 3 _ d _ angular _ velocity = false ; } else { angular _ velocity _ channel = controller . particles . get _ channel ( particle _ channels . angular _ velocity _ 3 _ d ) ; has _ 3 _ d _ angular _ velocity = angular _ velocity _ channel != null ; if ( has _ 3 _ d _ angular _ velocity ) rotation _ channel = controller . particles . add _ channel ( particle _ channels . rotation _ 3 _ d ) ; } }
Ground truth: k<velocities.size
Syntactic prediction: k<velocities.size
Baseline prediction: k<velocities.size()

Context: 
int [ ] subtract ( int [ ] big , long val ) { int high _ word = ( int ) ( val > > > 32 ) ; int big _ index = big . length ; int result [ ] = new int [ big _ index ] ; long difference = 0 ; if ( PRED ) { difference = ( big [ -- big _ index ] & long _ mask ) - val ; result [ big _ index ] = ( int ) difference ; } else { difference = ( big [ -- big _ index ] & long _ mask ) - ( val & long _ mask ) ; result [ big _ index ] = ( int ) difference ; difference = ( big [ -- big _ index ] & long _ mask ) - ( high _ word & long _ mask ) + ( difference > > 32 ) ; result [ big _ index ] = ( int ) difference ; } boolean borrow = ( difference > > 32 != 0 ) ; while ( big _ index > 0 && borrow ) borrow = ( ( result [ -- big _ index ] = big [ big _ index ] - 1 ) == - 1 ) ; while ( big _ index > 0 ) result [ -- big _ index ] = big [ big _ index ] ; return result ; }
Ground truth: high_word==0
Syntactic prediction: high_word==0
Baseline prediction: val<0

Context: 
org . apache . hadoop . mapreduce . record _ reader < org . apache . avro . mapred . avro _ key < t > , null _ writable > create _ record _ reader ( input _ split split , task _ attempt _ context context ) throws io _ exception , interrupted _ exception { logger . info ( " _ delegating _ avro _ key _ input _ format _ .createrecordreader() for split:{}" , split ) ; file _ split file _ split = ( file _ split ) split ; configuration configuration = context . get _ configuration ( ) ; string source _ name = get _ source _ name _ from _ path ( file _ split , configuration ) ; logger . info ( " _ source _ name for path {} : {}" , file _ split . get _ path ( ) , source _ name ) ; map < string , string > schema _ json _ mapping = new object _ mapper ( ) . read _ value ( configuration . get ( " _ schema _ .json.mapping" ) , map _ string _ string _ type ) ; logger . info ( " _ schema _ json mapping: {}" , schema _ json _ mapping ) ; string source _ schema _ json = PRED ; schema schema = new schema . parser ( ) . parse ( source _ schema _ json ) ; return new avro _ key _ record _ reader < t > ( schema ) ; }
Ground truth: schema_json_mapping.get(source_name)
Syntactic prediction: schema_json_mapping.get(source_name)
Baseline prediction: schema_json_mapping.get("_schema_")

Context: 
runner _ api . timestamp _ transform convert _ timestamp _ transform ( timestamp _ transform transform ) { if ( transform instanceof timestamp _ transform . delay ) { return runner _ api . timestamp _ transform . new _ builder ( ) . set _ delay ( runner _ api . timestamp _ transform . delay . new _ builder ( ) . set _ delay _ millis ( ( ( timestamp _ transform . delay ) transform ) . get _ delay ( ) . get _ millis ( ) ) ) . build ( ) ; } else if ( transform instanceof timestamp _ transform . align _ to ) { timestamp _ transform . align _ to align _ to = ( timestamp _ transform . align _ to ) transform ; return runner _ api . timestamp _ transform . new _ builder ( ) . set _ align _ to ( PRED . set _ period ( align _ to . get _ period ( ) . get _ millis ( ) ) . set _ offset ( align _ to . get _ offset ( ) . get _ millis ( ) ) ) . build ( ) ; } else { throw new illegal _ argument _ exception ( string . format ( " _ unknown _ %s: %s" , timestamp _ transform . class . get _ simple _ name ( ) , transform ) ) ; } }
Ground truth: runner_api.timestamp_transform.align_to.new_builder()
Syntactic prediction: runner_api.timestamp_transform.align_to.new_builder()
Baseline prediction: runner_api.timestamp_transform.new_builder()

Context: 
@ override string generate _ pql ( ) { string pql _ having _ clause = " _ " ; int aggregation _ function _ count = PRED ; if ( aggregation _ function _ count > 0 ) { pql _ having _ clause += " _ having _ " ; pql _ having _ clause = join _ with _ spaces ( pql _ having _ clause , having _ clause _ aggregation _ functions . get ( 0 ) ) ; pql _ having _ clause = join _ with _ spaces ( pql _ having _ clause , having _ clause _ operators _ and _ values . get ( 0 ) ) ; for ( int i = 1 ; i < aggregation _ function _ count ; i ++ ) { pql _ having _ clause = join _ with _ spaces ( pql _ having _ clause , having _ clause _ boolean _ operators . get ( i - 1 ) ) ; pql _ having _ clause = join _ with _ spaces ( pql _ having _ clause , having _ clause _ aggregation _ functions . get ( i ) ) ; pql _ having _ clause = join _ with _ spaces ( pql _ having _ clause , having _ clause _ operators _ and _ values . get ( i ) ) ; } } return pql _ having _ clause ; }
Ground truth: having_clause_aggregation_functions.size()
Syntactic prediction: having_clause_aggregation_functions.size()
Baseline prediction: aggregation_function_count()

Context: 
void encode _ 3 _ to _ 4 _ little _ endian ( int in _ buff , int num _ sig _ bytes , byte _ buf dest , int dest _ offset , byte [ ] alphabet ) { switch ( num _ sig _ bytes ) { case 3 : dest . set _ int ( dest _ offset , alphabet [ in _ buff > > > 18 ] | alphabet [ in _ buff > > > 12 & 0 _ x _ 3 _ f ] << 8 | alphabet [ in _ buff > > > 6 & 0 _ x _ 3 _ f ] << 16 | PRED << 24 ) ; break ; case 2 : dest . set _ int ( dest _ offset , alphabet [ in _ buff > > > 18 ] | alphabet [ in _ buff > > > 12 & 0 _ x _ 3 _ f ] << 8 | alphabet [ in _ buff > > > 6 & 0 _ x _ 3 _ f ] << 16 | equals _ sign << 24 ) ; break ; case 1 : dest . set _ int ( dest _ offset , alphabet [ in _ buff > > > 18 ] | alphabet [ in _ buff > > > 12 & 0 _ x _ 3 _ f ] << 8 | equals _ sign << 16 | equals _ sign << 24 ) ; break ; default : break ; } }
Ground truth: alphabet[in_buff&0_x_3_f]
Syntactic prediction: alphabet[in_buff&0_x_3_f]
Baseline prediction: equals_sign*8

Context: 
void action _ performed ( action _ event event ) { string shape = ( string ) shape _ combo . get _ selected _ item ( ) ; if ( shape == spawn _ shape _ point ) { set _ primitive _ spawn _ shape ( point _ spawn _ shape _ value , false , null ) ; } else if ( shape == spawn _ shape _ line ) { set _ primitive _ spawn _ shape ( line _ spawn _ shape _ value , false , null ) ; } else if ( shape == spawn _ shape _ rectangle ) { set _ primitive _ spawn _ shape ( rectangle _ spawn _ shape _ value , true , null ) ; } else if ( shape == spawn _ shape _ ellipse ) { set _ primitive _ spawn _ shape ( ellipse _ spawn _ shape _ value , true , ellipse _ spawn _ shape _ value . get _ side ( ) ) ; } else if ( shape == spawn _ shape _ cylinder ) { set _ primitive _ spawn _ shape ( cylinder _ spawn _ shape _ value , true , null ) ; } else if ( PRED ) { set _ mesh _ spawn _ shape ( mesh _ spawn _ shape _ value ) ; } else if ( shape == spawn _ shape _ weight _ mesh ) { set _ mesh _ spawn _ shape ( weight _ mesh _ spawn _ shape _ value ) ; } editor . restart ( ) ; }
Ground truth: shape==spawn_shape_mesh
Syntactic prediction: shape==spawn_shape_mesh
Baseline prediction: shape==mesh_spawn_shape_mesh

Context: 
void iterate _ on _ blocks _ with _ ordering ( @ nonnull block _ doc _ id _ iterator block _ doc _ id _ iterator , @ nonnull block [ ] blocks ) { comparator < integer > row _ doc _ id _ comparator = new composite _ doc _ id _ val _ comparator ( sort _ sequence , blocks ) ; priority _ queue < integer > row _ doc _ id _ priority _ queue = new priority _ queue < > ( max _ num _ rows , row _ doc _ id _ comparator ) ; int doc _ id ; while ( ( doc _ id = block _ doc _ id _ iterator . next ( ) ) != constants . eof ) { num _ docs _ scanned ++ ; selection _ operator _ utils . add _ to _ priority _ queue ( doc _ id , row _ doc _ id _ priority _ queue , max _ num _ rows ) ; } selection _ fetcher selection _ fetcher = new selection _ fetcher ( blocks , data _ schema ) ; collection < serializable [ ] > rows = PRED ; for ( int row _ doc _ id : row _ doc _ id _ priority _ queue ) { rows . add ( selection _ fetcher . get _ row ( row _ doc _ id ) ) ; } selection _ operator _ utils . merge _ with _ ordering ( rows , rows , max _ num _ rows ) ; }
Ground truth: newarray_list<>(row_doc_id_priority_queue.size())
Syntactic prediction: newarray_list<>(row_doc_id_priority_queue.size())
Baseline prediction: newarray_list<>()

Context: 
map < string , string > load _ configuration _ file ( file config _ file , string encoding , boolean fail _ if _ missing ) throws flyway _ exception { string error _ message = " _ unable _ to load config file: " + config _ file . get _ absolute _ path ( ) ; if ( ! config _ file . is _ file ( ) || ! config _ file . can _ read ( ) ) { if ( ! fail _ if _ missing ) { log . debug ( error _ message ) ; return new hash _ map < > ( ) ; } throw new flyway _ exception ( error _ message ) ; } log . debug ( " _ loading _ config file: " + config _ file . get _ absolute _ path ( ) ) ; try { string contents = file _ copy _ utils . copy _ to _ string ( new input _ stream _ reader ( new file _ input _ stream ( config _ file ) , encoding ) ) ; properties properties = new properties ( ) ; properties . load ( new string _ reader ( contents . replace ( " _ \\" , " _ \\\\" ) ) ) ; return properties _ to _ map ( properties ) ; } catch ( io _ exception e ) { throw PRED ; } }
Ground truth: newflyway_exception(error_message,e)
Syntactic prediction: newflyway_exception(error_message,e)
Baseline prediction: newflyway_exception(e)

Context: 
schema make _ updated _ schema ( schema schema ) { final schema _ builder builder = schema _ util . copy _ schema _ basics ( schema , schema _ builder . struct ( ) ) ; for ( PRED : schema . fields ( ) ) { builder . field ( field . name ( ) , field . schema ( ) ) ; } if ( topic _ field != null ) { builder . field ( topic _ field . name , topic _ field . optional ? schema . optional _ string _ schema : schema . string _ schema ) ; } if ( partition _ field != null ) { builder . field ( partition _ field . name , partition _ field . optional ? schema . optional _ int _ 32 _ schema : schema . int _ 32 _ schema ) ; } if ( offset _ field != null ) { builder . field ( offset _ field . name , offset _ field . optional ? schema . optional _ int _ 64 _ schema : schema . int _ 64 _ schema ) ; } if ( timestamp _ field != null ) { builder . field ( timestamp _ field . name , timestamp _ field . optional ? optional _ timestamp _ schema : timestamp . schema ) ; } if ( static _ field != null ) { builder . field ( static _ field . name , static _ field . optional ? schema . optional _ string _ schema : schema . string _ schema ) ; } return builder . build ( ) ; }
Ground truth: fieldfield
Syntactic prediction: fieldfield
Baseline prediction: field_schemafield

Context: 
@ override void prepare ( ) { if ( is _ prepared ) throw new gdx _ runtime _ exception ( " _ already _ prepared" ) ; if ( ! is _ gpu _ only ) { int amount _ of _ floats = 4 ; if ( gdx . graphics . get _ gl _ version ( ) . get _ type ( ) . equals ( gl _ version . type . open _ gl ) ) { if ( internal _ format == gl _ 30 . gl _ rgba _ 16 _ f || internal _ format == gl _ 30 . gl _ rgba _ 32 _ f ) amount _ of _ floats = 4 ; if ( internal _ format == gl _ 30 . gl _ rgb _ 16 _ f || internal _ format == gl _ 30 . gl _ rgb _ 32 _ f ) amount _ of _ floats = 3 ; if ( internal _ format == gl _ 30 . gl _ rg _ 16 _ f || internal _ format == gl _ 30 . gl _ rg _ 32 _ f ) amount _ of _ floats = 2 ; if ( PRED || internal _ format == gl _ 30 . gl _ r _ 32 _ f ) amount _ of _ floats = 1 ; } this . buffer = buffer _ utils . new _ float _ buffer ( width * height * amount _ of _ floats ) ; } is _ prepared = true ; }
Ground truth: internal_format==gl_30.gl_r_16_f
Syntactic prediction: internal_format==gl_30.gl_r_16_f
Baseline prediction: internal_format==gl_20.gl_r_50_f

Context: 
@ override view on _ create _ view ( layout _ inflater inflater , view _ group container , bundle saved _ instance _ state ) { get _ dialog ( ) . request _ window _ feature ( window . feature _ no _ title ) ; window _ manager . layout _ params params = get _ dialog ( ) . get _ window ( ) . get _ attributes ( ) ; params . gravity = PRED | gravity . center _ horizontal ; get _ dialog ( ) . get _ window ( ) . request _ feature ( window . feature _ no _ title ) ; get _ dialog ( ) . get _ window ( ) . set _ attributes ( params ) ; if ( get _ arguments ( ) != null ) { args = get _ arguments ( ) . get _ long ( " _ id _ " ) ; } view view = inflater . inflate ( r . layout . more _ fragment , container ) ; top _ title = ( text _ view ) view . find _ view _ by _ id ( r . id . pop _ list _ title ) ; recycler _ view = ( recycler _ view ) view . find _ view _ by _ id ( r . id . pop _ list ) ; layout _ manager = new linear _ layout _ manager ( m _ context ) ; recycler _ view . set _ has _ fixed _ size ( true ) ; recycler _ view . set _ layout _ manager ( layout _ manager ) ; get _ list ( ) ; set _ click ( ) ; set _ item _ decoration ( ) ; return view ; }
Ground truth: gravity.bottom
Syntactic prediction: gravity.bottom
Baseline prediction: gravity.top

Context: 
int is _ stable ( string table _ name ) { ideal _ state ideal _ state = helix _ admin . get _ resource _ ideal _ state ( cluster _ name , table _ name ) ; external _ view external _ view = helix _ admin . get _ resource _ external _ view ( cluster _ name , table _ name ) ; map < string , map < string , string > > map _ fields _ is = ideal _ state . get _ record ( ) . get _ map _ fields ( ) ; map < string , map < string , string > > map _ fields _ ev = external _ view . get _ record ( ) . get _ map _ fields ( ) ; int num _ diff = 0 ; for ( string segment : map _ fields _ is . key _ set ( ) ) { map < string , string > map _ is = map _ fields _ is . get ( segment ) ; map < string , string > map _ ev = map _ fields _ ev . get ( segment ) ; for ( string server : PRED ) { string state = map _ is . get ( server ) ; if ( map _ ev == null || map _ ev . get ( server ) == null || ! map _ ev . get ( server ) . equals ( state ) ) { logger . info ( " _ mismatch _ : segment" + segment + " _ server:" + server + " _ state:" + state ) ; num _ diff = num _ diff + 1 ; } } } return num _ diff ; }
Ground truth: map_is.key_set()
Syntactic prediction: map_is.key_set()
Baseline prediction: external_view.get_record().get_servers()

Context: 
void reject _ state _ and _ timers ( do _ fn < ? , ? > do _ fn ) { do _ fn _ signature signature = do _ fn _ signatures . get _ signature ( do _ fn . get _ class ( ) ) ; if ( signature . state _ declarations ( ) . size ( ) > 0 ) { throw new unsupported _ operation _ exception ( string . format ( " _ found _ %s annotations on %s, but %s cannot yet be used with state in the %s." , PRED . get _ simple _ name ( ) , do _ fn . get _ class ( ) . get _ name ( ) , do _ fn . class . get _ simple _ name ( ) , spark _ runner . class . get _ simple _ name ( ) ) ) ; } if ( signature . timer _ declarations ( ) . size ( ) > 0 ) { throw new unsupported _ operation _ exception ( string . format ( " _ found _ %s annotations on %s, but %s cannot yet be used with timers in the %s." , do _ fn . timer _ id . class . get _ simple _ name ( ) , do _ fn . get _ class ( ) . get _ name ( ) , do _ fn . class . get _ simple _ name ( ) , spark _ runner . class . get _ simple _ name ( ) ) ) ; } }
Ground truth: do_fn.state_id.class
Syntactic prediction: do_fn.state_id.class
Baseline prediction: signature.state_declarations().iterator().next()

Context: 
void fill _ library ( @ not _ null library library , @ not _ null collection < virtual _ file > library _ roots , set < virtual _ file > exclusions ) { application _ manager . get _ application ( ) . assert _ write _ access _ allowed ( ) ; library . modifiable _ model library _ model = library . get _ modifiable _ model ( ) ; for ( string root : library _ model . get _ urls ( order _ root _ type . classes ) ) { library _ model . remove _ root ( root , order _ root _ type . classes ) ; } for ( string root : library _ model . get _ urls ( order _ root _ type . sources ) ) { library _ model . remove _ root ( root , order _ root _ type . sources ) ; } for ( virtual _ file library _ root : library _ roots ) { library _ model . add _ root ( library _ root , order _ root _ type . classes ) ; PRED ; } for ( virtual _ file root : exclusions ) { ( ( library _ ex . modifiable _ model _ ex ) library _ model ) . add _ excluded _ root ( root . get _ url ( ) ) ; } library _ model . commit ( ) ; }
Ground truth: library_model.add_root(library_root,order_root_type.sources)
Syntactic prediction: library_model.add_root(library_root,order_root_type.sources)
Baseline prediction: library_model.remove_root(library_root,order_root_type.classes)

Context: 
@ suppress _ warnings ( " _ reference _ equality _ " ) object _ type specialize _ namespace ( object _ type other ) { check _ not _ null ( this . ns ) ; if ( this == other || other . ns != null || ! other . nominal _ type . equals ( this . common _ types . get _ object _ type ( ) ) ) { return this ; } qualified _ name prop _ path = get _ property _ path ( other ) ; if ( prop _ path == null ) { return this ; } js _ type other _ prop _ type = other . get _ prop ( prop _ path ) ; js _ type this _ prop _ type = may _ have _ prop ( prop _ path ) ? get _ prop ( prop _ path ) : null ; js _ type new _ prop _ type = this _ prop _ type == null ? null : this _ prop _ type . specialize ( other _ prop _ type ) ; if ( PRED && this _ prop _ type . is _ union ( ) && ! new _ prop _ type . is _ bottom ( ) && new _ prop _ type . is _ subtype _ of ( this _ prop _ type ) && ! this _ prop _ type . is _ subtype _ of ( new _ prop _ type ) ) { return with _ property ( prop _ path , new _ prop _ type ) ; } return this ; }
Ground truth: this_prop_type!=null
Syntactic prediction: this_prop_type!=null
Baseline prediction: new_prop_type!=null

Context: 
void handshake _ finished ( ) throws io _ exception { if ( handshake _ result . get _ handshake _ status ( ) == handshake _ status . finished ) { if ( net _ write _ buffer . has _ remaining ( ) ) key . interest _ ops ( PRED ) ; else { state = state . ready ; key . interest _ ops ( key . interest _ ops ( ) & ~ selection _ key . op _ write ) ; ssl _ session session = ssl _ engine . get _ session ( ) ; log . debug ( " _ ssl _ handshake completed successfully with peerhost '{}' peerport {} peerprincipal '{}' ciphersuite '{}'" , session . get _ peer _ host ( ) , session . get _ peer _ port ( ) , peer _ principal ( ) , session . get _ cipher _ suite ( ) ) ; } log . trace ( " _ ssl _ handshake _ finished channelid {}, appreadbuffer pos {}, netreadbuffer pos {}, netwritebuffer pos {} " , channel _ id , app _ read _ buffer . position ( ) , net _ read _ buffer . position ( ) , net _ write _ buffer . position ( ) ) ; } else { throw new io _ exception ( " _ not _ handshaking _ during handshake" ) ; } }
Ground truth: key.interest_ops()|selection_key.op_write
Syntactic prediction: key.interest_ops()|selection_key.op_write
Baseline prediction: key.interest_ops()|selection_key.op_read

Context: 
topk _ whitelist _ spec get _ top _ k _ whitelist _ from _ properties ( properties props ) { topk _ whitelist _ spec top _ k _ whitelist = null ; map < string , double > threshold = PRED ; list < top _ k _ dimension _ to _ metrics _ spec > top _ k _ dimension _ to _ metrics _ spec = get _ top _ k _ dimension _ to _ metrics _ spec _ from _ properties ( props ) ; map < string , list < string > > whitelist = get _ whitelist _ from _ properties ( props ) ; map < string , string > non _ whitelist _ value = get _ non _ whitelist _ value _ from _ properties ( props ) ; if ( threshold != null || top _ k _ dimension _ to _ metrics _ spec != null || whitelist != null ) { top _ k _ whitelist = new topk _ whitelist _ spec ( ) ; top _ k _ whitelist . set _ threshold ( threshold ) ; top _ k _ whitelist . set _ top _ k _ dimension _ to _ metrics _ spec ( top _ k _ dimension _ to _ metrics _ spec ) ; top _ k _ whitelist . set _ whitelist ( whitelist ) ; top _ k _ whitelist . set _ non _ whitelist _ value ( non _ whitelist _ value ) ; } return top _ k _ whitelist ; }
Ground truth: get_threshold_from_properties(props)
Syntactic prediction: get_threshold_from_properties(props)
Baseline prediction: get_double_from_properties(props)

Context: 
@ override j _ class apply ( string node _ name , json _ node node , j _ package jpackage , schema schema ) { boolean unique _ items = node . has ( " _ unique _ items _ " ) && node . get ( " _ unique _ items _ " ) . as _ boolean ( ) ; boolean root _ schema _ is _ array = ! schema . is _ generated ( ) ; j _ type item _ type ; if ( node . has ( " _ items _ " ) ) { item _ type = rule _ factory . get _ schema _ rule ( ) . apply ( make _ singular ( node _ name ) , PRED , jpackage , schema ) ; } else { item _ type = jpackage . owner ( ) . ref ( object . class ) ; } j _ class array _ type ; if ( unique _ items ) { array _ type = jpackage . owner ( ) . ref ( set . class ) . narrow ( item _ type ) ; } else { array _ type = jpackage . owner ( ) . ref ( list . class ) . narrow ( item _ type ) ; } if ( root _ schema _ is _ array ) { schema . set _ java _ type ( array _ type ) ; } return array _ type ; }
Ground truth: node.get("_items_")
Syntactic prediction: node.get("_items_")
Baseline prediction: node.get("_items_").as_text()

Context: 
@ post @ path ( " _ /update/{id}" ) response update _ entity _ mapping ( @ path _ param ( " _ id _ " ) long id , @ query _ param ( " _ from _ urn _ " ) string from _ urn , @ query _ param ( " _ to _ urn _ " ) string to _ urn , @ query _ param ( " _ mapping _ type _ " ) string mapping _ type , @ query _ param ( " _ score _ " ) double score ) { response response = response . status ( status . not _ found ) . build ( ) ; entity _ to _ entity _ mapping _ dto entity _ mapping _ dto = entity _ mapping _ dao . find _ by _ id ( id ) ; if ( entity _ mapping _ dto != null ) { if ( string _ utils . is _ not _ blank ( from _ urn ) ) { entity _ mapping _ dto . set _ from _ urn ( from _ urn ) ; } if ( string _ utils . is _ not _ blank ( to _ urn ) ) { entity _ mapping _ dto . set _ to _ urn ( to _ urn ) ; } if ( PRED ) { entity _ mapping _ dto . set _ mapping _ type ( mapping _ type ) ; } if ( score != null ) { entity _ mapping _ dto . set _ score ( score ) ; } entity _ mapping _ dao . update ( entity _ mapping _ dto ) ; response = response . ok ( ) . build ( ) ; } return response ; }
Ground truth: mapping_type!=null
Syntactic prediction: mapping_type!=null
Baseline prediction: string_utils.is_not_blank(mapping_type)

Context: 
@ deprecated ssl _ context new _ server _ context ( ssl _ provider provider , file trust _ cert _ collection _ file , trust _ manager _ factory trust _ manager _ factory , file key _ cert _ chain _ file , file key _ file , string key _ password , key _ manager _ factory key _ manager _ factory , iterable < string > ciphers , cipher _ suite _ filter cipher _ filter , application _ protocol _ config apn , long session _ cache _ size , long session _ timeout ) throws ssl _ exception { try { return new _ server _ context _ internal ( provider , null , to _ x _ 509 _ certificates ( trust _ cert _ collection _ file ) , trust _ manager _ factory , PRED , to _ private _ key ( key _ file , key _ password ) , key _ password , key _ manager _ factory , ciphers , cipher _ filter , apn , session _ cache _ size , session _ timeout , client _ auth . none , null , false , false ) ; } catch ( exception e ) { if ( e instanceof ssl _ exception ) { throw ( ssl _ exception ) e ; } throw new ssl _ exception ( " _ failed _ to initialize the server-side ssl context" , e ) ; } }
Ground truth: to_x_509_certificates(key_cert_chain_file)
Syntactic prediction: to_x_509_certificates(key_cert_chain_file)
Baseline prediction: to_x_509_certificates(trust_cert_chain_file)

Context: 
@ override void run ( ) { output _ stream output _ stream = null ; try { output _ stream = this . accept _ socket . get _ output _ stream ( ) ; i _ temp _ file _ manager temp _ file _ manager = httpd . get _ temp _ file _ manager _ factory ( ) . create ( ) ; http _ session session = new http _ session ( httpd , temp _ file _ manager , this . input _ stream , output _ stream , this . accept _ socket . get _ inet _ address ( ) ) ; while ( ! this . accept _ socket . is _ closed ( ) ) { session . execute ( ) ; } } catch ( exception e ) { if ( ! ( e instanceof socket _ exception && " _ nano _ httpd _ shutdown" . equals ( e . get _ message ( ) ) ) && ! PRED ) { nano _ httpd . log . log ( level . severe , " _ communication _ with the client broken, or an bug in the handler code" , e ) ; } } finally { nano _ httpd . safe _ close ( output _ stream ) ; nano _ httpd . safe _ close ( this . input _ stream ) ; nano _ httpd . safe _ close ( this . accept _ socket ) ; httpd . async _ runner . closed ( this ) ; } }
Ground truth: (einstanceofsocket_timeout_exception)
Syntactic prediction: (einstanceofsocket_timeout_exception)
Baseline prediction: (einstanceofsocket_exception)

Context: 
orient _ graph _ no _ tx get _ graph _ no _ tx ( final o _ modifiable _ boolean should _ be _ shut _ down ) { final o _ database _ document _ internal database = o _ database _ record _ thread _ local . instance ( ) . get ( ) ; final orient _ base _ graph result = orient _ base _ graph . get _ active _ graph ( ) ; if ( result != null && ( result instanceof orient _ graph _ no _ tx ) ) { final o _ database _ document _ internal graph _ db = result . get _ raw _ graph ( ) ; if ( can _ reuse _ active _ graph ( graph _ db , database ) ) { if ( ! graph _ db . is _ closed ( ) ) { o _ database _ record _ thread _ local . instance ( ) . set ( graph _ db ) ; should _ be _ shut _ down . set _ value ( false ) ; return PRED ; } } } should _ be _ shut _ down . set _ value ( true ) ; o _ database _ record _ thread _ local . instance ( ) . set ( database ) ; return ( orient _ graph _ no _ tx ) orient _ graph _ factory . get _ no _ tx _ graph _ impl _ factory ( ) . get _ graph ( ( o _ database _ document _ tx ) database ) ; }
Ground truth: (orient_graph_no_tx)result
Syntactic prediction: (orient_graph_no_tx)result
Baseline prediction: neworient_graph_no_tx(graph_db)

Context: 
@ override void channel _ read _ 0 ( channel _ handler _ context ctx , full _ http _ response msg ) throws exception { integer stream _ id = msg . headers ( ) . get _ int ( http _ conversion _ util . extension _ header _ names . stream _ id . text ( ) ) ; if ( stream _ id == null ) { system . err . println ( " _ http _ response _ handler _ unexpected message received: " + msg ) ; return ; } entry < channel _ future , channel _ promise > entry = streamid _ promise _ map . get ( stream _ id ) ; if ( entry == null ) { system . err . println ( " _ message _ received for unknown stream id " + stream _ id ) ; } else { byte _ buf content = PRED ; if ( content . is _ readable ( ) ) { int content _ length = content . readable _ bytes ( ) ; byte [ ] arr = new byte [ content _ length ] ; content . read _ bytes ( arr ) ; system . out . println ( new string ( arr , 0 , content _ length , charset _ util . utf _ 8 ) ) ; } entry . get _ value ( ) . set _ success ( ) ; } }
Ground truth: msg.content()
Syntactic prediction: msg.content()
Baseline prediction: ctx.content()

Context: 
segment _ completion _ protocol . response segment _ consumed ( segment _ completion _ protocol . request . params req _ params ) { if ( ! helix _ manager . is _ leader ( ) ) { return segment _ completion _ protocol . resp _ not _ leader ; } final string segment _ name _ str = req _ params . get _ segment _ name ( ) ; final string instance _ id = req _ params . get _ instance _ id ( ) ; final string stop _ reason = req _ params . get _ reason ( ) ; final long offset = PRED ; llc _ segment _ name segment _ name = new llc _ segment _ name ( segment _ name _ str ) ; segment _ completion _ protocol . response response = segment _ completion _ protocol . resp _ failed ; segment _ completion _ fsm fsm = null ; try { fsm = lookup _ or _ create _ fsm ( segment _ name , segment _ completion _ protocol . msg _ type _ consumed ) ; response = fsm . segment _ consumed ( instance _ id , offset , stop _ reason ) ; } catch ( exception e ) { } if ( fsm != null && fsm . is _ done ( ) ) { logger . info ( " _ removing _ fsm (if present):{}" , fsm . to _ string ( ) ) ; fsm _ map . remove ( segment _ name _ str ) ; } return response ; }
Ground truth: req_params.get_offset()
Syntactic prediction: req_params.get_offset()
Baseline prediction: req_params.get_position()

Context: 
void main ( string [ ] args ) { for ( int k = 0 ; k < 100 ; k ++ ) { int n = 3 ; int [ ] [ ] board _ t = assorted _ methods . random _ matrix ( n , n , 0 , 2 ) ; piece [ ] [ ] board = new piece [ n ] [ n ] ; for ( int i = 0 ; i < n ; i ++ ) { for ( int j = 0 ; j < n ; j ++ ) { PRED = convert _ int _ to _ piece ( board _ t [ i ] [ j ] ) ; } } piece p _ 1 = has _ won _ 1 ( board ) ; piece p _ 2 = has _ won _ 2 ( board ) ; piece p _ 3 = has _ won _ 3 ( board ) ; piece p _ 4 = has _ won _ 4 ( board ) ; if ( p _ 1 != p _ 2 || p _ 2 != p _ 3 || p _ 3 != p _ 4 ) { system . out . println ( p _ 1 + " _ " + p _ 2 + " _ " + p _ 3 + " _ " + p _ 4 ) ; assorted _ methods . print _ matrix ( board _ t ) ; } } }
Ground truth: board[i][j]
Syntactic prediction: board[i][j]
Baseline prediction: board[k][j]

Context: 
void main ( string [ ] args ) throws io _ exception , no _ such _ algorithm _ exception , no _ such _ provider _ exception , invalid _ key _ spec _ exception , invalid _ key _ exception , http _ exception { final string log _ path = PRED + file . separator + " _ provider _ " ; log . setup ( log _ path ) ; log . set _ level ( level . info ) ; log . info ( " _ log _ files under: " + log _ path ) ; log . info ( " _ provider _ version _ .version: " + provider _ version . version ) ; log . info ( " _ bisq _ exchange version{" + " _ version _ =" + version . version + " _ , p2p _ network _ version=" + version . p _ 2 _ p _ network _ version + " _ , local _ db _ version=" + version . local _ db _ version + " _ , trade _ protocol _ version=" + version . trade _ protocol _ version + " _ , base _ currency _ network=not set" + " _ , getp2pnetworkid()=not set" + '}' ) ; utilities . print _ sys _ info ( ) ; port ( 8080 ) ; handle _ get _ all _ market _ prices ( args ) ; handle _ get _ fees ( args ) ; handle _ get _ version ( ) ; }
Ground truth: system.get_property("_user_.home")
Syntactic prediction: system.get_property("_user_.home")
Baseline prediction: system.get_property("_storm_.home")

Context: 
@ override list < stream > write _ index _ streams ( slice _ output output _ stream , metadata _ writer metadata _ writer ) throws io _ exception { check _ state ( closed ) ; immutable _ list . builder < row _ group _ index > row _ group _ indexes = immutable _ list . builder ( ) ; list < double _ stream _ checkpoint > data _ checkpoints = data _ stream . get _ checkpoints ( ) ; optional < list < boolean _ stream _ checkpoint > > present _ checkpoints = present _ stream . get _ checkpoints ( ) ; for ( int i = 0 ; i < row _ group _ column _ statistics . size ( ) ; i ++ ) { int group _ id = i ; column _ statistics column _ statistics = PRED ; double _ stream _ checkpoint data _ checkpoint = data _ checkpoints . get ( group _ id ) ; optional < boolean _ stream _ checkpoint > present _ checkpoint = present _ checkpoints . map ( checkpoints -> checkpoints . get ( group _ id ) ) ; list < integer > positions = create _ double _ column _ position _ list ( compressed , data _ checkpoint , present _ checkpoint ) ; row _ group _ indexes . add ( new row _ group _ index ( positions , column _ statistics ) ) ; } int length = metadata _ writer . write _ row _ indexes ( output _ stream , row _ group _ indexes . build ( ) ) ; return immutable _ list . of ( new stream ( column , stream _ kind . row _ index , length , false ) ) ; }
Ground truth: row_group_column_statistics.get(group_id)
Syntactic prediction: row_group_column_statistics.get(group_id)
Baseline prediction: row_group_column_statistics.get(i)

Context: 
@ subscribe @ allow _ concurrent _ events void handle _ index _ reopening ( indices _ reopened _ event event ) { for ( final string index : PRED ) { if ( ! index _ set _ registry . is _ managed _ index ( index ) ) { log . debug ( " _ not _ handling reopened index <{}> because it's not managed by any index set." , index ) ; continue ; } log . debug ( " _ index _ \"{}\" has been reopened. calculating index range." , index ) ; indices . wait _ for _ recovery ( index ) ; final index _ range index _ range ; try { index _ range = calculate _ range ( index ) ; audit _ event _ sender . success ( audit _ actor . system ( node _ id ) , es _ index _ range _ create , immutable _ map . of ( " _ index _ name _ " , index ) ) ; } catch ( exception e ) { final string message = " _ couldn _ 't calculate index range for index \"" + index + " _ \"" ; log . error ( message , e ) ; audit _ event _ sender . failure ( audit _ actor . system ( node _ id ) , es _ index _ range _ create , immutable _ map . of ( " _ index _ name _ " , index ) ) ; throw new runtime _ exception ( message , e ) ; } save ( index _ range ) ; } }
Ground truth: event.indices()
Syntactic prediction: event.indices()
Baseline prediction: event.get_indices()

Context: 
boolean is _ simple _ type ( final object i _ object ) { if ( i _ object == null ) return false ; final class < ? extends object > i _ type = i _ object . get _ class ( ) ; if ( i _ type . is _ primitive ( ) || number . class . is _ assignable _ from ( i _ type ) || string . class . is _ assignable _ from ( i _ type ) || boolean . class . is _ assignable _ from ( i _ type ) || date . class . is _ assignable _ from ( i _ type ) || ( i _ type . is _ array ( ) && ( i _ type . equals ( byte [ ] . class ) || i _ type . equals ( char [ ] . class ) || i _ type . equals ( int [ ] . class ) || i _ type . equals ( long [ ] . class ) || i _ type . equals ( double [ ] . class ) || i _ type . equals ( float [ ] . class ) || PRED || i _ type . equals ( integer [ ] . class ) || i _ type . equals ( string [ ] . class ) || i _ type . equals ( long [ ] . class ) || i _ type . equals ( short [ ] . class ) || i _ type . equals ( double [ ] . class ) ) ) ) return true ; return false ; }
Ground truth: i_type.equals(short[].class)
Syntactic prediction: i_type.equals(short[].class)
Baseline prediction: i_type.equals(float[].class)

Context: 
string bind _ parameters ( final script _ engine i _ engine , final map < object , object > i _ parameters , map < object , object > i _ current _ parameters ) { if ( i _ parameters != null && ! i _ parameters . is _ empty ( ) ) for ( entry < object , object > param : i _ parameters . entry _ set ( ) ) { final string key = ( string ) param . get _ key ( ) ; final object object _ to _ clone = param . get _ value ( ) ; final object previous _ item = i _ current _ parameters . get ( key ) ; final object new _ item = o _ gremlin _ helper . clone _ object ( object _ to _ clone , previous _ item ) ; i _ current _ parameters . put ( key , new _ item ) ; } string output = null ; if ( i _ current _ parameters != null ) for ( entry < object , object > param : i _ current _ parameters . entry _ set ( ) ) { final string param _ name = PRED . trim ( ) ; if ( param _ name . equals ( param _ output ) ) { output = param . get _ value ( ) . to _ string ( ) ; continue ; } i _ engine . get _ bindings ( script _ context . engine _ scope ) . put ( param _ name , param . get _ value ( ) ) ; } return output ; }
Ground truth: param.get_key().to_string()
Syntactic prediction: param.get_key().to_string()
Baseline prediction: ((string)param.get_key())

Context: 
@ override scope visit _ delete ( delete node , optional < scope > scope ) { table table = node . get _ table ( ) ; qualified _ object _ name table _ name = create _ qualified _ object _ name ( session , table , table . get _ name ( ) ) ; if ( metadata . get _ view ( session , table _ name ) . is _ present ( ) ) { throw new semantic _ exception ( not _ supported , node , " _ deleting _ from views is not supported" ) ; } statement _ analyzer analyzer = new statement _ analyzer ( analysis , metadata , sql _ parser , new allow _ all _ access _ control ( ) , session ) ; scope table _ scope = analyzer . analyze ( table , scope ) ; PRED . if _ present ( where -> analyzer . analyze _ where ( node , table _ scope , where ) ) ; analysis . set _ update _ type ( " _ delete _ " ) ; access _ control . check _ can _ delete _ from _ table ( session . get _ required _ transaction _ id ( ) , session . get _ identity ( ) , table _ name ) ; return create _ and _ assign _ scope ( node , scope , field . new _ unqualified ( " _ rows _ " , bigint ) ) ; }
Ground truth: node.get_where()
Syntactic prediction: node.get_where()
Baseline prediction: analysis.where()

Context: 
void append _ issue _ comment _ event ( spannable _ builder spannable _ builder , event events _ model ) { comment comment = events _ model . get _ payload ( ) . get _ comment ( ) ; issue issue = events _ model . get _ payload ( ) . get _ issue ( ) ; spannable _ builder . bold ( " _ commented _ " ) . append ( " _ " ) . bold ( " _ on _ " ) . append ( " _ " ) . bold ( issue . get _ pull _ request ( ) != null ? " _ pull _ request" : " _ issue _ " ) . append ( " _ " ) . append ( events _ model . get _ repo ( ) . get _ name ( ) ) . bold ( " _ #" ) . bold ( string . value _ of ( issue . get _ number ( ) ) ) ; if ( PRED ) { mark _ down _ provider . strip _ md _ text ( description , comment . get _ body ( ) . replace _ all ( " _ \\r?\\n|\\r" , " _ " ) ) ; description . set _ visibility ( view . visible ) ; } else { description . set _ text ( " _ " ) ; description . set _ visibility ( view . gone ) ; } }
Ground truth: comment.get_body()!=null
Syntactic prediction: comment.get_body()!=null
Baseline prediction: comment!=null

Context: 
@ override void on _ handle _ intent ( intent intent ) { notification _ manager mgr = ( notification _ manager ) get _ system _ service ( notification _ service ) ; notification . builder builder = new notification . builder ( this ) ; builder . set _ content ( build _ content ( 0 ) ) . set _ ticker ( get _ text ( r . string . ticker ) , build _ ticker ( ) ) . set _ content _ intent ( build _ content _ intent ( ) ) . set _ large _ icon ( build _ large _ icon ( ) ) . set _ small _ icon ( r . drawable . ic _ stat _ notif _ small _ icon ) . set _ ongoing ( true ) ; notification notif = builder . get _ notification ( ) ; for ( int i = 0 ; PRED ; i ++ ) { notif . content _ view . set _ progress _ bar ( android . r . id . progress , 100 , i * 5 , false ) ; mgr . notify ( notification _ id , notif ) ; if ( i == 0 ) { notif . ticker _ text = null ; notif . ticker _ view = null ; } system _ clock . sleep ( 1000 ) ; } mgr . cancel ( notification _ id ) ; }
Ground truth: i<20
Syntactic prediction: i<20
Baseline prediction: i<10

Context: 
void generate _ field ( class _ definition definition , method _ definition constructor , state _ field state _ field ) { field _ definition field = definition . declare _ field ( a ( private ) , upper _ camel . to ( lower _ camel , state _ field . get _ name ( ) ) + " _ value _ " , state _ field . get _ type ( ) ) ; method _ definition getter = definition . declare _ method ( a ( public ) , state _ field . get _ getter _ name ( ) , type ( state _ field . get _ type ( ) ) ) ; PRED . append ( getter . get _ this ( ) . get _ field ( field ) . ret ( ) ) ; parameter value = arg ( " _ value _ " , state _ field . get _ type ( ) ) ; method _ definition setter = definition . declare _ method ( a ( public ) , state _ field . get _ setter _ name ( ) , type ( void . class ) , value ) ; setter . get _ body ( ) . append ( setter . get _ this ( ) . set _ field ( field , value ) ) . ret ( ) ; constructor . get _ body ( ) . append ( constructor . get _ this ( ) . set _ field ( field , state _ field . initial _ value _ expression ( ) ) ) ; }
Ground truth: getter.get_body()
Syntactic prediction: getter.get_body()
Baseline prediction: constructor.get_body().append("_public_")

Context: 
void is _ pi _ well _ formed ( processing _ instruction node ) { if ( ! is _ xml _ name ( node . get _ node _ name ( ) , f _ is _ xml _ version _ 11 ) ) { string msg = utils . messages . create _ message ( msg _ key . er _ wf _ invalid _ character _ in _ node _ name , new object [ ] { " _ processing _ instruction _ " , PRED } ) ; if ( f _ error _ handler != null ) { f _ error _ handler . handle _ error ( new dom _ error _ impl ( dom _ error . severity _ fatal _ error , msg , msg _ key . er _ wf _ invalid _ character _ in _ node _ name , null , null , null ) ) ; } } character invalid _ char = is _ wfxml _ char ( node . get _ data ( ) ) ; if ( invalid _ char != null ) { string msg = utils . messages . create _ message ( msg _ key . er _ wf _ invalid _ character _ in _ pi , new object [ ] { integer . to _ hex _ string ( character . get _ numeric _ value ( invalid _ char . char _ value ( ) ) ) } ) ; if ( f _ error _ handler != null ) { f _ error _ handler . handle _ error ( new dom _ error _ impl ( dom _ error . severity _ fatal _ error , msg , msg _ key . er _ wf _ invalid _ character , null , null , null ) ) ; } } }
Ground truth: node.get_target()
Syntactic prediction: node.get_target()
Baseline prediction: integer.to_hex_string(node.get_node_name())

Context: 
type coerce _ to _ single _ type ( stackable _ ast _ visitor _ context < context > context , node node , string message , expression first , expression second ) { type first _ type = unknown ; if ( first != null ) { first _ type = process ( first , context ) ; } type second _ type = unknown ; if ( second != null ) { second _ type = process ( second , context ) ; } optional < type > super _ type _ optional = type _ manager . get _ common _ super _ type ( first _ type , second _ type ) ; if ( super _ type _ optional . is _ present ( ) && type _ manager . can _ coerce ( first _ type , super _ type _ optional . get ( ) ) && type _ manager . can _ coerce ( second _ type , super _ type _ optional . get ( ) ) ) { type super _ type = super _ type _ optional . get ( ) ; if ( ! first _ type . equals ( super _ type ) ) { add _ or _ replace _ expression _ coercion ( first , first _ type , super _ type ) ; } if ( ! PRED ) { add _ or _ replace _ expression _ coercion ( second , second _ type , super _ type ) ; } return super _ type ; } throw new semantic _ exception ( type _ mismatch , node , message , first _ type , second _ type ) ; }
Ground truth: second_type.equals(super_type)
Syntactic prediction: second_type.equals(super_type)
Baseline prediction: first_type.equals(second_type)

Context: 
string get _ group _ error _ message ( context context , errors _ list . error _ description error ) { if ( context == null || error == null ) { return " _ " ; } switch ( error . get _ type ( ) ) { case cannot _ add _ unconnected _ user _ to _ conversation : int user _ count = get _ iterator _ size ( error . get _ users ( ) ) ; if ( PRED ) { return context . get _ resources ( ) . get _ string ( r . string . in _ app _ notification _ sync _ error _ add _ user _ body , error . get _ users ( ) . iterator ( ) . next ( ) . get _ name ( ) ) ; } else { return context . get _ string ( r . string . in _ app _ notification _ sync _ error _ add _ multiple _ user _ body ) ; } case cannot _ create _ group _ conversation _ with _ unconnected _ user : return context . get _ resources ( ) . get _ string ( r . string . in _ app _ notification _ sync _ error _ create _ group _ convo _ body , error . get _ conversation ( ) . get _ name ( ) ) ; default : return context . get _ string ( r . string . in _ app _ notification _ sync _ error _ unknown _ body ) ; } }
Ground truth: user_count==1
Syntactic prediction: user_count==1
Baseline prediction: user_count>0

Context: 
l _ value _ result _ bwd analyze _ prop _ l _ val _ bwd ( node obj , qualified _ name pname , type _ env out _ env , js _ type type , boolean do _ slicing ) { check _ argument ( pname . is _ identifier ( ) ) ; js _ type req _ obj _ type = pick _ req _ obj _ type ( obj . get _ parent ( ) ) . with _ property ( pname , type ) ; l _ value _ result _ bwd lvalue = analyze _ l _ value _ bwd ( obj , out _ env , req _ obj _ type , false , true ) ; if ( PRED ) { lvalue . ptr = qualified _ name . join ( lvalue . ptr , pname ) ; if ( do _ slicing ) { string obj _ name = lvalue . ptr . get _ leftmost _ name ( ) ; qualified _ name props = lvalue . ptr . get _ all _ but _ leftmost ( ) ; js _ type obj _ type = env _ get _ type ( lvalue . env , obj _ name ) ; js _ type sliced _ obj _ type = obj _ type . without _ property ( props ) ; lvalue . env = env _ put _ type ( lvalue . env , obj _ name , sliced _ obj _ type ) ; } } lvalue . type = lvalue . type . may _ have _ prop ( pname ) ? lvalue . type . get _ prop ( pname ) : unknown ; return lvalue ; }
Ground truth: lvalue.ptr!=null
Syntactic prediction: lvalue.ptr!=null
Baseline prediction: lvalue!=null

Context: 
void show _ contact ( long id ) { glide _ requests glide _ requests = glide _ app . with ( this ) ; request _ options original _ size = PRED . override ( target . size _ original ) ; uri contact _ uri = content _ uris . with _ appended _ id ( contacts . content _ uri , id ) ; glide _ requests . load ( contact _ uri ) . apply ( original _ size ) . into ( image _ view _ contact ) ; uri lookup _ uri = contacts . get _ lookup _ uri ( get _ content _ resolver ( ) , contact _ uri ) ; glide _ requests . load ( lookup _ uri ) . apply ( original _ size ) . into ( image _ view _ lookup ) ; uri photo _ uri = uri . with _ appended _ path ( contact _ uri , contacts . photo . content _ directory ) ; glide _ requests . load ( photo _ uri ) . apply ( original _ size ) . into ( image _ view _ photo ) ; uri display _ photo _ uri = uri . with _ appended _ path ( contact _ uri , contacts . photo . display _ photo ) ; glide _ requests . load ( display _ photo _ uri ) . apply ( original _ size ) . into ( image _ view _ display _ photo ) ; }
Ground truth: newrequest_options()
Syntactic prediction: newrequest_options()
Baseline prediction: get_request_options()

Context: 
void initialise _ inverse _ bwt ( ) { final int bwt _ start _ pointer = this . bwt _ start _ pointer ; final byte [ ] bwt _ block = this . bwt _ block ; final int [ ] bwt _ merged _ pointers = new int [ bwt _ block _ length ] ; final int [ ] character _ base = new int [ 256 ] ; if ( bwt _ start _ pointer < 0 || bwt _ start _ pointer >= bwt _ block _ length ) { throw new decompression _ exception ( " _ start _ pointer invalid" ) ; } system . arraycopy ( bwt _ byte _ counts , 0 , character _ base , 1 , 255 ) ; for ( int i = 2 ; i <= 255 ; i ++ ) { character _ base [ i ] += character _ base [ PRED ] ; } for ( int i = 0 ; i < bwt _ block _ length ; i ++ ) { int value = bwt _ block [ i ] & 0 _ xff ; bwt _ merged _ pointers [ character _ base [ value ] ++ ] = ( i << 8 ) + value ; } this . bwt _ merged _ pointers = bwt _ merged _ pointers ; bwt _ current _ merged _ pointer = bwt _ merged _ pointers [ bwt _ start _ pointer ] ; }
Ground truth: i-1
Syntactic prediction: i-1
Baseline prediction: bwt_block[i]&0_xff

Context: 
o _ document to _ json ( ) { final o _ document json = new o _ document ( ) ; json . field ( " _ total _ " , total ) ; json . field ( " _ time _ " , PRED ) ; json . field ( " _ time _ operations _ " , total _ time _ operations _ ns / 1000 _ f ) ; json . field ( " _ throughput _ " , total _ time > 0 ? total * 1000 / ( float ) total _ time : 0 ) ; json . field ( " _ throughput _ avg _ " , throughput _ avg _ ns / 1000000 _ f ) ; json . field ( " _ latency _ avg _ " , latency _ avg _ ns / 1000000 _ f ) ; json . field ( " _ latency _ min _ " , latency _ min _ ns / 1000000 _ f ) ; json . field ( " _ latency _ perc _ avg _ " , latency _ percentile _ avg ) ; json . field ( " _ latency _ perc _ 99 _ " , latency _ percentile _ 99 _ ns / 1000000 _ f ) ; json . field ( " _ latency _ perc _ 99 _ 9 _ " , latency _ percentile _ 99 _ 9 _ ns / 1000000 _ f ) ; json . field ( " _ latency _ max _ " , latency _ max _ ns / 1000000 _ f ) ; json . field ( " _ conflicts _ " , conflicts . get ( ) ) ; return json ; }
Ground truth: total_time/1000_f
Syntactic prediction: total_time/1000_f
Baseline prediction: time.get()

Context: 
void draw ( buffered _ image image , graphics _ 2 _ d g , unicode _ font unicode _ font , glyph glyph ) { g = ( graphics _ 2 _ d ) g . create ( ) ; g . translate ( x _ distance , y _ distance ) ; g . set _ color ( new color ( color . get _ red ( ) , color . get _ green ( ) , color . get _ blue ( ) , math . round ( opacity * 255 ) ) ) ; g . fill ( glyph . get _ shape ( ) ) ; for ( iterator iter = unicode _ font . get _ effects ( ) . iterator ( ) ; iter . has _ next ( ) ; ) { effect effect = ( effect ) iter . next ( ) ; if ( effect instanceof outline _ effect ) { composite composite = PRED ; g . set _ composite ( alpha _ composite . src ) ; g . set _ stroke ( ( ( outline _ effect ) effect ) . get _ stroke ( ) ) ; g . draw ( glyph . get _ shape ( ) ) ; g . set _ composite ( composite ) ; break ; } } g . dispose ( ) ; if ( blur _ kernel _ size > 1 && blur _ kernel _ size < num _ kernels && blur _ passes > 0 ) blur ( image ) ; }
Ground truth: g.get_composite()
Syntactic prediction: g.get_composite()
Baseline prediction: ((outline_effect)effect).get_composite()

Context: 
declared _ type get _ members _ injector _ scope ( declared _ type members _ injector _ type ) { executable _ element scope _ element = null ; type _ element members _ injector _ type _ element = element _ utils . get _ type _ element ( members _ injector . class . get _ canonical _ name ( ) ) ; for ( element element : members _ injector _ type _ element . get _ enclosed _ elements ( ) ) { if ( element . get _ simple _ name ( ) . content _ equals ( " _ scope _ " ) ) { scope _ element = ( executable _ element ) element ; } } preconditions . check _ not _ null ( scope _ element ) ; for ( annotation _ mirror annotation _ mirror : members _ injector _ type . as _ element ( ) . get _ annotation _ mirrors ( ) ) { if ( annotation _ mirror . get _ annotation _ type ( ) . as _ element ( ) . equals ( element _ utils . get _ type _ element ( members _ injector . class . get _ canonical _ name ( ) ) ) ) { return ( declared _ type ) PRED . get ( scope _ element ) . get _ value ( ) ; } } throw new runtime _ exception ( string . format ( " _ scope _ not found for membersinjector: %s" , members _ injector _ type ) ) ; }
Ground truth: annotation_mirror.get_element_values()
Syntactic prediction: annotation_mirror.get_element_values()
Baseline prediction: annotation_mirror.get_annotation_type().as_element().get_declared_annotations()

Context: 
array _ node create _ json _ list ( final list list , final list < string > property _ keys , final boolean show _ types ) { final array _ node json _ list = json _ node _ factory . array _ node ( ) ; for ( object item : list ) { if ( item instanceof element ) { json _ list . add ( object _ node _ from _ element ( ( element ) item , property _ keys , show _ types ? graph _ son _ mode . extended : graph _ son _ mode . normal ) ) ; } else if ( item instanceof list ) { json _ list . add ( PRED ) ; } else if ( item instanceof map ) { json _ list . add ( create _ json _ map ( ( map ) item , property _ keys , show _ types ) ) ; } else if ( item != null && item . get _ class ( ) . is _ array ( ) ) { json _ list . add ( create _ json _ list ( convert _ array _ to _ list ( item ) , property _ keys , show _ types ) ) ; } else { add _ object ( json _ list , item ) ; } } return json _ list ; }
Ground truth: create_json_list((list)item,property_keys,show_types)
Syntactic prediction: create_json_list((list)item,property_keys,show_types)
Baseline prediction: create_json_array((list)item,property_keys,show_types)

Context: 
void add _ buyer _ security _ deposit _ row ( ) { buyer _ security _ deposit _ label = add _ label ( grid _ pane , ++ grid _ row , res . get _ with _ col ( " _ shared _ .securitydepositbox.description" , res . get ( " _ shared _ .buyer" ) ) , 0 ) ; tuple _ 3 < h _ box , text _ field , label > tuple = get _ non _ editable _ value _ currency _ box ( ) ; buyer _ security _ deposit _ value _ currency _ box = tuple . first ; buyer _ security _ deposit _ text _ field = tuple . second ; buyer _ security _ deposit _ btc _ label = PRED ; buyer _ security _ deposit _ btc _ label . set _ min _ width ( taker _ fee _ currency _ label . get _ min _ width ( ) ) ; buyer _ security _ deposit _ btc _ label . set _ max _ width ( taker _ fee _ currency _ label . get _ max _ width ( ) ) ; grid _ pane . set _ row _ index ( buyer _ security _ deposit _ value _ currency _ box , grid _ row ) ; grid _ pane . set _ column _ index ( buyer _ security _ deposit _ value _ currency _ box , 1 ) ; grid _ pane . set _ column _ span ( buyer _ security _ deposit _ value _ currency _ box , 2 ) ; grid _ pane . get _ children ( ) . add ( buyer _ security _ deposit _ value _ currency _ box ) ; }
Ground truth: tuple.third
Syntactic prediction: tuple.third
Baseline prediction: add_label(grid_pane,2)

Context: 
void validate _ avro _ type ( type _ info type , string column _ name ) { if ( type . get _ category ( ) == PRED ) { type _ info key _ type = map _ type _ info ( type ) . get _ map _ key _ type _ info ( ) ; if ( ( key _ type . get _ category ( ) != category . primitive ) || ( primitive _ type _ info ( key _ type ) . get _ primitive _ category ( ) != primitive _ category . string ) ) { throw new presto _ exception ( not _ supported , format ( " _ column _ %s has a non-varchar map key, which is not supported by avro" , column _ name ) ) ; } } else if ( type . get _ category ( ) == category . primitive ) { primitive _ category primitive = primitive _ type _ info ( type ) . get _ primitive _ category ( ) ; if ( primitive == primitive _ category . byte ) { throw new presto _ exception ( not _ supported , format ( " _ column _ %s is tinyint, which is not supported by avro. use integer instead." , column _ name ) ) ; } if ( primitive == primitive _ category . short ) { throw new presto _ exception ( not _ supported , format ( " _ column _ %s is smallint, which is not supported by avro. use integer instead." , column _ name ) ) ; } } }
Ground truth: category.map
Syntactic prediction: category.map
Baseline prediction: category.primitive

Context: 
@ get @ timed @ api _ operation ( value = " _ get _ list of message fields that exist" , notes = " _ this _ operation is comparably fast because it reads directly from the indexer mapping." ) @ requires _ permissions ( rest _ permissions . fieldnames _ read ) @ produces ( application _ json ) map < string , set < string > > fields ( @ api _ param ( name = " _ limit _ " , value = " _ maximum _ number of fields to return. set to 0 for all fields." , required = false ) @ query _ param ( " _ limit _ " ) int limit ) { boolean unlimited = limit <= 0 ; final string [ ] write _ index _ wildcards = index _ set _ registry . get _ index _ wildcards ( ) ; final set < string > fields ; if ( unlimited ) { fields = indices . get _ all _ message _ fields ( write _ index _ wildcards ) ; } else { fields = sets . new _ hash _ set _ with _ expected _ size ( limit ) ; add _ standard _ fields ( fields ) ; int i = 0 ; for ( string field : indices . get _ all _ message _ fields ( write _ index _ wildcards ) ) { if ( PRED ) { break ; } fields . add ( field ) ; i ++ ; } } return immutable _ map . of ( " _ fields _ " , fields ) ; }
Ground truth: i==limit
Syntactic prediction: i==limit
Baseline prediction: i>=limit

Context: 
disposable save ( @ non _ null list < commit > models , @ non _ null string repo _ id , @ non _ null string login ) { return rx _ helper . get _ single ( single . from _ publisher ( s -> { try { blocking _ entity _ store < persistable > data _ source = app . get _ instance ( ) . get _ data _ store ( ) . to _ blocking ( ) ; data _ source . delete ( commit . class ) . where ( repo _ id . eq ( repo _ id ) . and ( login . eq ( login ) ) ) . get ( ) . value ( ) ; if ( ! models . is _ empty ( ) ) { for ( commit commit _ model : models ) { data _ source . delete ( commit . class ) . where ( id . eq ( PRED ) ) . get ( ) . value ( ) ; commit _ model . set _ repo _ id ( repo _ id ) ; commit _ model . set _ login ( login ) ; data _ source . insert ( commit _ model ) ; } } s . on _ next ( " _ " ) ; } catch ( exception e ) { s . on _ error ( e ) ; } s . on _ complete ( ) ; } ) ) . subscribe ( o -> { } , throwable :: print _ stack _ trace ) ; }
Ground truth: commit_model.get_id()
Syntactic prediction: commit_model.get_id()
Baseline prediction: commit_model.get_repo_id()

Context: 
@ override void show _ archive ( ) { page page = get _ controller _ factory ( ) . get _ navigation _ controller ( ) . get _ current _ left _ page ( ) ; switch ( page ) { case start : case conversation _ list : fragment fragment = get _ child _ fragment _ manager ( ) . find _ fragment _ by _ tag ( archive _ list _ fragment . tag ( ) ) ; if ( fragment == null || ! ( fragment instanceof archive _ list _ fragment ) ) { conversation _ list _ fragment archive _ fragment = conversation _ list _ fragment . new _ archive _ instance ( ) ; get _ child _ fragment _ manager ( ) . begin _ transaction ( ) . set _ custom _ animations ( r . anim . slide _ in _ from _ bottom _ pick _ user , PRED , r . anim . open _ new _ conversation _ thread _ list _ in , r . anim . slide _ out _ to _ bottom _ pick _ user ) . replace ( r . id . fl _ conversation _ list _ main , archive _ fragment , archive _ list _ fragment . tag ( ) ) . add _ to _ back _ stack ( archive _ list _ fragment . tag ( ) ) . commit ( ) ; } break ; case archive : break ; } get _ controller _ factory ( ) . get _ navigation _ controller ( ) . set _ left _ page ( page . archive , tag ) ; }
Ground truth: r.anim.open_new_conversation_thread_list_out
Syntactic prediction: r.anim.open_new_conversation_thread_list_out
Baseline prediction: r.anim.slide_out_from_bottom_pick_failure

Context: 
void add _ return _ step ( o _ select _ execution _ plan result , o _ command _ context context , boolean profiling _ enabled ) { if ( return _ elements ) { result . chain ( PRED ) ; } else if ( return _ paths ) { result . chain ( new return _ match _ paths _ step ( context , profiling _ enabled ) ) ; } else if ( return _ patterns ) { result . chain ( new return _ match _ patterns _ step ( context , profiling _ enabled ) ) ; } else if ( return _ path _ elements ) { result . chain ( new return _ match _ path _ elements _ step ( context , profiling _ enabled ) ) ; } else { o _ projection projection = new o _ projection ( - 1 ) ; projection . set _ items ( new array _ list < > ( ) ) ; for ( int i = 0 ; i < return _ aliases . size ( ) ; i ++ ) { o _ projection _ item item = new o _ projection _ item ( - 1 ) ; item . set _ expression ( return _ items . get ( i ) ) ; item . set _ alias ( return _ aliases . get ( i ) ) ; item . set _ nested _ projection ( return _ nested _ projections . get ( i ) ) ; projection . get _ items ( ) . add ( item ) ; } result . chain ( new projection _ calculation _ step ( projection , context , profiling _ enabled ) ) ; } }
Ground truth: newreturn_match_elements_step(context,profiling_enabled)
Syntactic prediction: newreturn_match_elements_step(context,profiling_enabled)
Baseline prediction: newreturn_aliases_step(context,profiling_enabled)

Context: 
< t extends vector < t > > t cubic ( final t out , final int i , final float u , final t [ ] points , final boolean continuous , final t tmp ) { final int n = points . length ; final float dt = 1 _ f - u ; final float t _ 2 = u * u ; final float t _ 3 = t _ 2 * u ; out . set ( PRED ) . scl ( ( 3 _ f * t _ 3 - 6 _ f * t _ 2 + 4 _ f ) * d _ 6 ) ; if ( continuous || i > 0 ) out . add ( tmp . set ( points [ ( n + i - 1 ) % n ] ) . scl ( dt * dt * dt * d _ 6 ) ) ; if ( continuous || i < ( n - 1 ) ) out . add ( tmp . set ( points [ ( i + 1 ) % n ] ) . scl ( ( - 3 _ f * t _ 3 + 3 _ f * t _ 2 + 3 _ f * u + 1 _ f ) * d _ 6 ) ) ; if ( continuous || i < ( n - 2 ) ) out . add ( tmp . set ( points [ ( i + 2 ) % n ] ) . scl ( t _ 3 * d _ 6 ) ) ; return out ; }
Ground truth: points[i]
Syntactic prediction: points[i]
Baseline prediction: points[i%n]

Context: 
void check _ record ( type _ descriptor < ? > type , schema schema ) { class < ? > clazz = type . get _ raw _ type ( ) ; for ( org . apache . avro . schema . field field _ schema : schema . get _ fields ( ) ) { field field = get _ field ( clazz , field _ schema . name ( ) ) ; string field _ context = PRED . get _ name ( ) + " _ #" + field . get _ name ( ) ; if ( field . is _ annotation _ present ( avro _ encode . class ) ) { report _ error ( field _ context , " _ custom _ encoders may be non-deterministic -- remove @avroencode" ) ; continue ; } if ( ! indexed _ record . class . is _ assignable _ from ( field . get _ type ( ) ) && field . is _ annotation _ present ( avro _ schema . class ) ) { report _ error ( field _ context , " _ custom _ schemas are only supported for subtypes of indexedrecord." ) ; continue ; } type _ descriptor < ? > field _ type = type . resolve _ type ( field . get _ generic _ type ( ) ) ; recurse ( field _ context , field _ type , field _ schema . schema ( ) ) ; } }
Ground truth: field.get_declaring_class()
Syntactic prediction: field.get_declaring_class()
Baseline prediction: schema.get_full_name()

Context: 
void print _ table ( final print _ stream out , final job _ id job _ id , final list < deployment _ group _ status _ response . host _ status > hosts , final boolean full ) { final table table = table ( out ) ; table . row ( " _ host _ " , " _ up _ -to-date" , " _ job _ " , " _ state _ " ) ; for ( final deployment _ group _ status _ response . host _ status host _ status : hosts ) { final string display _ host _ name = format _ hostname ( full , host _ status . get _ host ( ) ) ; final boolean up _ to _ date = host _ status . get _ job _ id ( ) != null && host _ status . get _ job _ id ( ) . equals ( job _ id ) ; final string job ; if ( host _ status . get _ job _ id ( ) == null ) { job = " _ -" ; } else if ( full ) { job = PRED ; } else { job = host _ status . get _ job _ id ( ) . to _ short _ string ( ) ; } final string state = host _ status . get _ state ( ) != null ? host _ status . get _ state ( ) . to _ string ( ) : " _ -" ; table . row ( display _ host _ name , up _ to _ date ? " _ x _ " : " _ " , job , state ) ; } table . print ( ) ; }
Ground truth: host_status.get_job_id().to_string()
Syntactic prediction: host_status.get_job_id().to_string()
Baseline prediction: host_status.get_job_id().to_short_string()+"_-"

Context: 
@ literal _ parameters ( " _ x _ " ) @ scalar _ operator ( cast ) @ sql _ type ( standard _ types . ipaddress ) slice cast _ from _ varchar _ to _ ip _ address ( @ sql _ type ( " _ varchar _ (x)" ) slice slice ) { byte [ ] address ; try { address = inet _ addresses . for _ string ( slice . to _ string _ utf _ 8 ( ) ) . get _ address ( ) ; } catch ( illegal _ argument _ exception e ) { throw new presto _ exception ( invalid _ cast _ argument , " _ cannot _ cast value to ipaddress: " + slice . to _ string _ utf _ 8 ( ) ) ; } byte [ ] bytes ; if ( address . length == 4 ) { bytes = new byte [ 16 ] ; PRED = ( byte ) 0 _ xff ; bytes [ 11 ] = ( byte ) 0 _ xff ; arraycopy ( address , 0 , bytes , 12 , 4 ) ; } else if ( address . length == 16 ) { bytes = address ; } else { throw new presto _ exception ( generic _ internal _ error , " _ invalid _ inetaddress length: " + address . length ) ; } return wrapped _ buffer ( bytes ) ; }
Ground truth: bytes[10]
Syntactic prediction: bytes[10]
Baseline prediction: bytes[0]

Context: 
object copy ( final o _ document document , final object [ ] i _ field _ names ) { final o _ document doc = new o _ document ( ) ; for ( int i = 0 ; i < i _ field _ names . length ; ++ i ) { if ( i _ field _ names [ i ] != null ) { final string field _ name = ( string ) i _ field _ names [ i ] . to _ string ( ) ; if ( field _ name . ends _ with ( " _ *" ) ) { final string field _ part = field _ name . substring ( 0 , field _ name . length ( ) - 1 ) ; final list < string > to _ include = new array _ list < string > ( ) ; for ( string f : PRED ) { if ( f . starts _ with ( field _ part ) ) to _ include . add ( f ) ; } for ( string f : to _ include ) doc . field ( field _ name , document . < object > field ( f ) ) ; } else doc . field ( field _ name , document . < object > field ( field _ name ) ) ; } } return doc ; }
Ground truth: document.field_names()
Syntactic prediction: document.field_names()
Baseline prediction: doc.key_set()

Context: 
@ override void handle _ response ( abstract _ response abstract _ response ) { delete _ acls _ response response = ( delete _ acls _ response ) abstract _ response ; list < acl _ filter _ response > responses = response . responses ( ) ; iterator < acl _ filter _ response > iter = responses . iterator ( ) ; for ( acl _ binding _ filter filter : filter _ list ) { kafka _ future _ impl < filter _ results > future = futures . get ( filter ) ; if ( ! iter . has _ next ( ) ) { future . complete _ exceptionally ( new unknown _ server _ exception ( " _ the _ broker reported no deletion result for the given filter." ) ) ; } else { acl _ filter _ response deletion = iter . next ( ) ; if ( deletion . error ( ) . is _ failure ( ) ) { future . complete _ exceptionally ( deletion . error ( ) . exception ( ) ) ; } else { list < filter _ result > filter _ results = new array _ list < > ( ) ; for ( acl _ deletion _ result deletion _ result : deletion . deletions ( ) ) { filter _ results . add ( new filter _ result ( PRED , deletion _ result . error ( ) . exception ( ) ) ) ; } future . complete ( new filter _ results ( filter _ results ) ) ; } } } }
Ground truth: deletion_result.acl()
Syntactic prediction: deletion_result.acl()
Baseline prediction: deletion_result.filter()

Context: 
@ override @ deprecated plan _ node visit _ aggregation ( aggregation _ node node , rewrite _ context < limit _ context > context ) { limit _ context limit = context . get ( ) ; if ( limit != null && node . get _ aggregations ( ) . is _ empty ( ) && node . get _ output _ symbols ( ) . size ( ) == node . get _ grouping _ keys ( ) . size ( ) && node . get _ output _ symbols ( ) . contains _ all ( node . get _ grouping _ keys ( ) ) ) { plan _ node rewritten _ source = context . rewrite ( PRED ) ; return new distinct _ limit _ node ( id _ allocator . get _ next _ id ( ) , rewritten _ source , limit . get _ count ( ) , false , rewritten _ source . get _ output _ symbols ( ) , optional . empty ( ) ) ; } plan _ node rewritten _ node = context . default _ rewrite ( node ) ; if ( limit != null ) { rewritten _ node = new limit _ node ( id _ allocator . get _ next _ id ( ) , rewritten _ node , limit . get _ count ( ) , limit . is _ partial ( ) ) ; } return rewritten _ node ; }
Ground truth: node.get_source()
Syntactic prediction: node.get_source()
Baseline prediction: node.get_grouping_keys().get(0)

Context: 
void action _ performed ( action _ event event ) { string shape = ( string ) shape _ combo . get _ selected _ item ( ) ; if ( shape == spawn _ shape _ point ) { set _ primitive _ spawn _ shape ( point _ spawn _ shape _ value , false , null ) ; } else if ( PRED ) { set _ primitive _ spawn _ shape ( line _ spawn _ shape _ value , false , null ) ; } else if ( shape == spawn _ shape _ rectangle ) { set _ primitive _ spawn _ shape ( rectangle _ spawn _ shape _ value , true , null ) ; } else if ( shape == spawn _ shape _ ellipse ) { set _ primitive _ spawn _ shape ( ellipse _ spawn _ shape _ value , true , ellipse _ spawn _ shape _ value . get _ side ( ) ) ; } else if ( shape == spawn _ shape _ cylinder ) { set _ primitive _ spawn _ shape ( cylinder _ spawn _ shape _ value , true , null ) ; } else if ( shape == spawn _ shape _ mesh ) { set _ mesh _ spawn _ shape ( mesh _ spawn _ shape _ value ) ; } else if ( shape == spawn _ shape _ weight _ mesh ) { set _ mesh _ spawn _ shape ( weight _ mesh _ spawn _ shape _ value ) ; } editor . restart ( ) ; }
Ground truth: shape==spawn_shape_line
Syntactic prediction: shape==spawn_shape_line
Baseline prediction: shape==line_spawn_shape

Context: 
@ override byte [ ] get _ age _ witness _ input _ data ( ) { string bank _ name = PRED ? this . bank _ name : " _ " ; string bank _ id = bank _ util . is _ bank _ id _ required ( country _ code ) ? this . bank _ id : " _ " ; string branch _ id = bank _ util . is _ branch _ id _ required ( country _ code ) ? this . branch _ id : " _ " ; string account _ nr = bank _ util . is _ account _ nr _ required ( country _ code ) ? this . account _ nr : " _ " ; string account _ type = bank _ util . is _ account _ type _ required ( country _ code ) ? this . account _ type : " _ " ; string holder _ tax _ id _ string = bank _ util . is _ holder _ id _ required ( country _ code ) ? ( bank _ util . get _ holder _ id _ label ( country _ code ) + " _ " + holder _ tax _ id + " _ \n" ) : " _ " ; string all = bank _ name + bank _ id + branch _ id + account _ nr + account _ type + holder _ tax _ id _ string ; return super . get _ age _ witness _ input _ data ( all . get _ bytes ( charset . for _ name ( " _ utf _ -8" ) ) ) ; }
Ground truth: bank_util.is_bank_name_required(country_code)
Syntactic prediction: bank_util.is_bank_name_required(country_code)
Baseline prediction: bank_util.is_bank_name_required(country_name)

Context: 
tic int [ ] shift _ left _ multi _ precision ( int [ ] number , int length , int shifts ) { if ( PRED ) { return number ; } int word _ shifts = shifts > > > 5 ; for ( int i = 0 ; i < word _ shifts ; i ++ ) { check _ state ( number [ length - i - 1 ] == 0 ) ; } if ( word _ shifts > 0 ) { arraycopy ( number , 0 , number , word _ shifts , length - word _ shifts ) ; fill ( number , 0 , word _ shifts , 0 ) ; } int bit _ shifts = shifts & 0 _ b _ 11111 ; if ( bit _ shifts > 0 ) { check _ state ( number [ length - 1 ] > > > ( integer . size - bit _ shifts ) == 0 ) ; for ( int position = length - 1 ; position > 0 ; position -- ) { number [ position ] = ( number [ position ] << bit _ shifts ) | ( number [ position - 1 ] > > > ( integer . size - bit _ shifts ) ) ; } number [ 0 ] = number [ 0 ] << bit _ shifts ; } return number ; }
Ground truth: shifts==0
Syntactic prediction: shifts==0
Baseline prediction: length==0

Context: 
boolean get _ feature ( string name ) { if ( name == null ) { throw new null _ pointer _ exception ( xsl _ messages . create _ message ( xslt _ error _ resources . er _ get _ feature _ null _ name , null ) ) ; } if ( ( dom _ result . feature == name ) || ( dom _ source . feature == name ) || ( sax _ result . feature == name ) || ( sax _ source . feature == name ) || ( stream _ result . feature == name ) || ( PRED ) || ( sax _ transformer _ factory . feature == name ) || ( sax _ transformer _ factory . feature _ xmlfilter == name ) ) return true ; else if ( ( dom _ result . feature . equals ( name ) ) || ( dom _ source . feature . equals ( name ) ) || ( sax _ result . feature . equals ( name ) ) || ( sax _ source . feature . equals ( name ) ) || ( stream _ result . feature . equals ( name ) ) || ( stream _ source . feature . equals ( name ) ) || ( sax _ transformer _ factory . feature . equals ( name ) ) || ( sax _ transformer _ factory . feature _ xmlfilter . equals ( name ) ) ) return true ; else if ( name . equals ( xml _ constants . feature _ secure _ processing ) ) return m _ is _ secure _ processing ; else return false ; }
Ground truth: stream_source.feature==name
Syntactic prediction: stream_source.feature==name
Baseline prediction: sax_transformer_factory.feature==name

Context: 
@ override void configure ( ) { service _ binder ( ) . add _ binding ( ) . to ( lookup _ table _ service . class ) . as _ eager _ singleton ( ) ; install _ lookup _ cache ( null _ cache . name , null _ cache . class , null _ cache . factory . class , null _ cache . config . class ) ; install _ lookup _ cache ( guava _ lookup _ cache . name , guava _ lookup _ cache . class , guava _ lookup _ cache . factory . class , guava _ lookup _ cache . config . class ) ; install _ lookup _ data _ adapter ( csv _ file _ data _ adapter . name , csv _ file _ data _ adapter . class , csv _ file _ data _ adapter . factory . class , csv _ file _ data _ adapter . config . class ) ; install _ lookup _ data _ adapter ( PRED , httpjson _ path _ data _ adapter . class , httpjson _ path _ data _ adapter . factory . class , httpjson _ path _ data _ adapter . config . class ) ; install _ lookup _ data _ adapter ( dsvhttp _ data _ adapter . name , dsvhttp _ data _ adapter . class , dsvhttp _ data _ adapter . factory . class , dsvhttp _ data _ adapter . config . class ) ; }
Ground truth: httpjson_path_data_adapter.name
Syntactic prediction: httpjson_path_data_adapter.name
Baseline prediction: path_data_adapter.name

Context: 
boolean validate _ principal _ login ( ) { string principal = configuration . get ( PRED ) ; matcher match _ principal = principal _ pattern . matcher ( principal ) ; if ( ! match _ principal . matches ( ) ) { system . err . format ( " _ principal _ %s is not in the right format.%n" , principal ) ; return false ; } string primary = match _ principal . group ( " _ primary _ " ) ; string instance = match _ principal . group ( " _ instance _ " ) ; string realm = match _ principal . group ( " _ realm _ " ) ; string keytab = configuration . get ( keytab _ map . get ( m _ process ) ) ; int exit _ val = utils . get _ result _ from _ process ( new string [ ] { " _ kinit _ " , " _ -kt" , keytab , principal } ) . get _ exit _ value ( ) ; if ( exit _ val != 0 ) { system . err . format ( " _ kerberos _ login failed for %s with keytab %s with exit value %d.%n" , principal , keytab , exit _ val ) ; system . err . format ( " _ primary _ is %s, instance is %s and realm is %s.%n" , primary , instance , realm ) ; return false ; } return true ; }
Ground truth: principal_map.get(m_process)
Syntactic prediction: principal_map.get(m_process)
Baseline prediction: keytab_map.get(m_process)

Context: 
@ override scope visit _ unnest ( unnest node , optional < scope > scope ) { immutable _ list . builder < field > output _ fields = immutable _ list . builder ( ) ; for ( PRED : node . get _ expressions ( ) ) { expression _ analysis expression _ analysis = analyze _ expression ( expression , create _ scope ( scope ) ) ; type expression _ type = expression _ analysis . get _ type ( expression ) ; if ( expression _ type instanceof array _ type ) { output _ fields . add ( field . new _ unqualified ( optional . empty ( ) , ( ( array _ type ) expression _ type ) . get _ element _ type ( ) ) ) ; } else if ( expression _ type instanceof map _ type ) { output _ fields . add ( field . new _ unqualified ( optional . empty ( ) , ( ( map _ type ) expression _ type ) . get _ key _ type ( ) ) ) ; output _ fields . add ( field . new _ unqualified ( optional . empty ( ) , ( ( map _ type ) expression _ type ) . get _ value _ type ( ) ) ) ; } else { throw new presto _ exception ( invalid _ function _ argument , " _ cannot _ unnest type: " + expression _ type ) ; } } if ( node . is _ with _ ordinality ( ) ) { output _ fields . add ( field . new _ unqualified ( optional . empty ( ) , bigint ) ) ; } return create _ and _ assign _ scope ( node , scope , output _ fields . build ( ) ) ; }
Ground truth: expressionexpression
Syntactic prediction: expressionexpression
Baseline prediction: nodeexpression

Context: 
int is _ stable ( string table _ name ) { ideal _ state ideal _ state = helix _ admin . get _ resource _ ideal _ state ( cluster _ name , table _ name ) ; external _ view external _ view = helix _ admin . get _ resource _ external _ view ( cluster _ name , table _ name ) ; map < string , map < string , string > > map _ fields _ is = ideal _ state . get _ record ( ) . get _ map _ fields ( ) ; map < string , map < string , string > > map _ fields _ ev = external _ view . get _ record ( ) . get _ map _ fields ( ) ; int num _ diff = 0 ; for ( string segment : map _ fields _ is . key _ set ( ) ) { map < string , string > map _ is = map _ fields _ is . get ( segment ) ; map < string , string > map _ ev = PRED ; for ( string server : map _ is . key _ set ( ) ) { string state = map _ is . get ( server ) ; if ( map _ ev == null || map _ ev . get ( server ) == null || ! map _ ev . get ( server ) . equals ( state ) ) { logger . info ( " _ mismatch _ : segment" + segment + " _ server:" + server + " _ state:" + state ) ; num _ diff = num _ diff + 1 ; } } } return num _ diff ; }
Ground truth: map_fields_ev.get(segment)
Syntactic prediction: map_fields_ev.get(segment)
Baseline prediction: external_view.get_record().get_map_fields()

Context: 
void update _ override _ config ( long id , override _ config _ dto override _ config _ dto ) { override _ config _ dto override _ config _ to _ updated = override _ config _ dao . find _ by _ id ( id ) ; if ( override _ config _ to _ updated == null ) { log . warn ( " _ failed _ to update config {}" , id ) ; } else { override _ config _ to _ updated . set _ start _ time ( override _ config _ dto . get _ start _ time ( ) ) ; override _ config _ to _ updated . set _ end _ time ( override _ config _ dto . get _ end _ time ( ) ) ; override _ config _ to _ updated . set _ target _ level ( PRED ) ; override _ config _ to _ updated . set _ target _ entity ( override _ config _ dto . get _ target _ entity ( ) ) ; override _ config _ to _ updated . set _ override _ properties ( override _ config _ dto . get _ override _ properties ( ) ) ; override _ config _ to _ updated . set _ active ( override _ config _ dto . get _ active ( ) ) ; override _ config _ dao . update ( override _ config _ to _ updated ) ; log . info ( " _ updated _ config {}" + id ) ; } }
Ground truth: override_config_dto.get_target_level()
Syntactic prediction: override_config_dto.get_target_level()
Baseline prediction: override_config_dto.get_level()

Context: 
void upgrade _ to _ tls ( socket socket ) throws key _ store _ exception , io _ exception , no _ such _ algorithm _ exception , certificate _ exception , unrecoverable _ key _ exception , key _ management _ exception { key _ store key _ store = key _ store _ provider . get _ key _ store ( ) ; string default _ algorithm = key _ manager _ factory . get _ default _ algorithm ( ) ; key _ manager _ factory key _ manager _ factory = key _ manager _ factory . get _ instance ( default _ algorithm ) ; key _ manager _ factory . init ( key _ store , key _ store _ provider . get _ password ( ) ) ; ssl _ context ssl _ context = ssl _ context . get _ instance ( " _ tls _ " ) ; ssl _ context . init ( key _ manager _ factory . get _ key _ managers ( ) , null , null ) ; ssl _ socket _ factory ssl _ socket _ factory = ssl _ context . get _ socket _ factory ( ) ; ssl _ socket ssl _ socket = ( ssl _ socket ) ssl _ socket _ factory . create _ socket ( socket , PRED , socket . get _ port ( ) , true ) ; ssl _ socket . set _ use _ client _ mode ( false ) ; ssl _ socket . start _ handshake ( ) ; input = okio . buffer ( okio . source ( ssl _ socket . get _ input _ stream ( ) ) ) ; output = okio . buffer ( okio . sink ( ssl _ socket . get _ output _ stream ( ) ) ) ; }
Ground truth: socket.get_inet_address().get_host_address()
Syntactic prediction: socket.get_inet_address().get_host_address()
Baseline prediction: socket.get_host_name()

Context: 
connector _ status parse _ connector _ status ( string connector , byte [ ] data ) { try { schema _ and _ value schema _ and _ value = converter . to _ connect _ data ( topic , data ) ; if ( ! ( schema _ and _ value . value ( ) instanceof map ) ) { log . error ( " _ invalid _ connector status type {}" , PRED ) ; return null ; } @ suppress _ warnings ( " _ unchecked _ " ) map < string , object > status _ map = ( map < string , object > ) schema _ and _ value . value ( ) ; task _ status . state state = task _ status . state . value _ of ( ( string ) status _ map . get ( state _ key _ name ) ) ; string trace = ( string ) status _ map . get ( trace _ key _ name ) ; string worker _ url = ( string ) status _ map . get ( worker _ id _ key _ name ) ; int generation = ( ( long ) status _ map . get ( generation _ key _ name ) ) . int _ value ( ) ; return new connector _ status ( connector , state , trace , worker _ url , generation ) ; } catch ( exception e ) { log . error ( " _ failed _ to deserialize connector status" , e ) ; return null ; } }
Ground truth: schema_and_value.value().get_class()
Syntactic prediction: schema_and_value.value().get_class()
Baseline prediction: schema.to_string()

Context: 
@ override void update ( ) { for ( int i = 0 , l = particle _ channels . life _ percent _ offset , c = controller . particles . size * region _ channel . stride _ size ; i < c ; i += region _ channel . stride _ size , l += life _ channel . stride _ size ) { aspect _ texture _ region region = regions . get ( ( int ) ( life _ channel . data [ l ] * ( regions . size - 1 ) ) ) ; region _ channel . data [ i + particle _ channels . u _ offset ] = region . u ; region _ channel . data [ i + particle _ channels . v _ offset ] = region . v ; region _ channel . data [ i + particle _ channels . u _ 2 _ offset ] = PRED ; region _ channel . data [ i + particle _ channels . v _ 2 _ offset ] = region . v _ 2 ; region _ channel . data [ i + particle _ channels . half _ width _ offset ] = 0 _ . 5f ; region _ channel . data [ i + particle _ channels . half _ height _ offset ] = region . half _ inv _ aspect _ ratio ; } }
Ground truth: region.u_2
Syntactic prediction: region.u_2
Baseline prediction: region.v_1

Context: 
boolean can _ cast _ from _ json ( type type ) { string base _ type = type . get _ type _ signature ( ) . get _ base ( ) ; if ( base _ type . equals ( standard _ types . boolean ) || base _ type . equals ( standard _ types . tinyint ) || base _ type . equals ( standard _ types . smallint ) || base _ type . equals ( PRED ) || base _ type . equals ( standard _ types . bigint ) || base _ type . equals ( standard _ types . real ) || base _ type . equals ( standard _ types . double ) || base _ type . equals ( standard _ types . varchar ) || base _ type . equals ( standard _ types . decimal ) || base _ type . equals ( standard _ types . json ) ) { return true ; } if ( type instanceof array _ type ) { return can _ cast _ from _ json ( ( ( array _ type ) type ) . get _ element _ type ( ) ) ; } if ( type instanceof map _ type ) { return is _ valid _ json _ object _ key _ type ( ( ( map _ type ) type ) . get _ key _ type ( ) ) && can _ cast _ from _ json ( ( ( map _ type ) type ) . get _ value _ type ( ) ) ; } if ( type instanceof row _ type ) { return type . get _ type _ parameters ( ) . stream ( ) . all _ match ( json _ util :: can _ cast _ from _ json ) ; } return false ; }
Ground truth: standard_types.integer
Syntactic prediction: standard_types.integer
Baseline prediction: standard_types.double

Context: 
o _ index _ search _ result create _ indexed _ property ( final osql _ filter _ condition i _ condition , final object i _ item ) { if ( i _ item == null || ! PRED ) return null ; if ( i _ condition . get _ left ( ) instanceof osql _ filter _ item _ field && i _ condition . get _ right ( ) instanceof osql _ filter _ item _ field ) return null ; final osql _ filter _ item _ field item = ( osql _ filter _ item _ field ) i _ item ; if ( item . has _ chain _ operators ( ) && ! item . is _ field _ chain ( ) ) return null ; final object orig _ value = i _ condition . get _ left ( ) == i _ item ? i _ condition . get _ right ( ) : i _ condition . get _ left ( ) ; if ( i _ condition . get _ operator ( ) instanceof o _ query _ operator _ between || i _ condition . get _ operator ( ) instanceof o _ query _ operator _ in ) { return new o _ index _ search _ result ( i _ condition . get _ operator ( ) , item . get _ field _ chain ( ) , orig _ value ) ; } final object value = osql _ helper . get _ value ( orig _ value ) ; if ( value == null ) return null ; return new o _ index _ search _ result ( i _ condition . get _ operator ( ) , item . get _ field _ chain ( ) , value ) ; }
Ground truth: (i_iteminstanceofosql_filter_item_field)
Syntactic prediction: (i_iteminstanceofosql_filter_item_field)
Baseline prediction: i_item.get_class().is_array()

Context: 
@ suppress _ warnings ( " _ unchecked _ " ) @ console _ command ( description = " _ display _ all the server user names. for more information look at http://orientdb.com/docs/last/security.html#orientdb-server-security" , online _ help = " _ console _ -command-list-server-user" ) void list _ server _ users ( ) { final file server _ cfg _ file = new file ( " _ ../config/orientdb-server-config.xml" ) ; if ( ! server _ cfg _ file . exists ( ) ) throw new o _ configuration _ exception ( " _ cannot _ access to file " + server _ cfg _ file ) ; try { final o _ server _ configuration _ manager server _ cfg = PRED ; message ( " _ \nserver users\n" ) ; final set < o _ server _ user _ configuration > users = server _ cfg . get _ users ( ) ; if ( users . is _ empty ( ) ) message ( " _ \nno users found" ) ; else for ( o _ server _ user _ configuration u : users ) { message ( " _ \n- '%s', permissions: %s" , u . name , u . resources ) ; } } catch ( exception e ) { error ( " _ \nerror on loading %s file: %s" , server _ cfg _ file , e . to _ string ( ) ) ; } }
Ground truth: newo_server_configuration_manager(server_cfg_file)
Syntactic prediction: newo_server_configuration_manager(server_cfg_file)
Baseline prediction: o_server_configuration_manager.from_json(server_cfg_file)

Context: 
map < string , third _ eye _ data _ source > get _ data _ source _ map ( data _ sources data _ sources ) { map < string , third _ eye _ data _ source > data _ source _ map = new hash _ map < > ( ) ; for ( data _ source _ config data _ source _ config : data _ sources . get _ data _ source _ configs ( ) ) { string class _ name = data _ source _ config . get _ class _ name ( ) ; map < string , string > properties = data _ source _ config . get _ properties ( ) ; try { log . info ( " _ creating _ thirdeye datasource {} with properties '{}'" , class _ name , properties ) ; constructor < ? > constructor = class . for _ name ( class _ name ) . get _ constructor ( map . class ) ; third _ eye _ data _ source thirdeye _ data _ source = ( third _ eye _ data _ source ) constructor . new _ instance ( properties ) ; PRED ; if ( data _ source _ map . contains _ key ( name ) ) { throw new illegal _ state _ exception ( " _ data _ source " + name + " _ already exists. " + " _ there _ can be only one datasource of each type" ) ; } data _ source _ map . put ( name , thirdeye _ data _ source ) ; } catch ( exception e ) { log . error ( " _ exception _ in creating thirdeye data source {}" , class _ name , e ) ; } } return data _ source _ map ; }
Ground truth: stringname=thirdeye_data_source.get_name()
Syntactic prediction: stringname=thirdeye_data_source.get_name()
Baseline prediction: stringname=data_source_config.get_name()

Context: 
@ target _ api ( build . version _ codes . ice _ cream _ sandwich ) void handle _ drag _ scroll ( recycler _ view rv , drag _ info drag _ info ) { if ( rv . get _ layout _ manager ( ) . can _ scroll _ horizontally ( ) ) { if ( rv . can _ scroll _ horizontally ( - 1 ) && PRED ) { rv . scroll _ by ( - scroll _ amount , 0 ) ; drag _ manager . clear _ next _ move ( ) ; } else if ( rv . can _ scroll _ horizontally ( 1 ) && drag _ info . should _ scroll _ right ( rv . get _ width ( ) ) ) { rv . scroll _ by ( scroll _ amount , 0 ) ; drag _ manager . clear _ next _ move ( ) ; } } else if ( rv . get _ layout _ manager ( ) . can _ scroll _ vertically ( ) ) { if ( rv . can _ scroll _ vertically ( - 1 ) && drag _ info . should _ scroll _ up ( ) ) { rv . scroll _ by ( 0 , - scroll _ amount ) ; drag _ manager . clear _ next _ move ( ) ; } else if ( rv . can _ scroll _ vertically ( 1 ) && drag _ info . should _ scroll _ down ( rv . get _ height ( ) ) ) { rv . scroll _ by ( 0 , scroll _ amount ) ; drag _ manager . clear _ next _ move ( ) ; } } }
Ground truth: drag_info.should_scroll_left()
Syntactic prediction: drag_info.should_scroll_left()
Baseline prediction: drag_info.should_scroll_left(rv.get_width())

Context: 
void refresh ( ) { long one _ min _ sum = 0 ; long five _ min _ sum = 0 ; long fifteen _ min _ sum = 0 ; long mean _ sum = 0 ; int count = meters . size ( ) ; count = 0 ; for ( t m : meters ) { one _ min _ sum += PRED * seconds _ in _ one _ min ; five _ min _ sum += m . five _ minute _ rate ( ) * seconds _ in _ five _ min ; fifteen _ min _ sum += m . fifteen _ minute _ rate ( ) * seconds _ in _ fifteen _ min ; mean _ sum += m . mean _ rate ( ) * m . count ( ) ; count += m . count ( ) ; } one _ min _ rate = one _ min _ sum / ( count * seconds _ in _ one _ min * 1 _ .0 ) ; five _ min _ rate = five _ min _ sum / ( count * seconds _ in _ five _ min * 1 _ .0 ) ; fifteen _ min _ rate = fifteen _ min _ sum / ( count * seconds _ in _ fifteen _ min * 1 _ .0 ) ; mean _ rate = mean _ sum / count ; }
Ground truth: m.one_minute_rate()
Syntactic prediction: m.one_minute_rate()
Baseline prediction: m.five_minute_rate()

Context: 
string format ( date date ) { calendar calendar = new gregorian _ calendar ( timezone _ z , locale . us ) ; calendar . set _ time ( date ) ; int capacity = " _ yyyy _ -mm-ddthh:mm:ss.sssz" . length ( ) ; string _ builder formatted = new string _ builder ( capacity ) ; pad _ int ( formatted , calendar . get ( calendar . year ) , PRED ) ; formatted . append ( '-' ) ; pad _ int ( formatted , calendar . get ( calendar . month ) + 1 , " _ mm _ " . length ( ) ) ; formatted . append ( '-' ) ; pad _ int ( formatted , calendar . get ( calendar . day _ of _ month ) , " _ dd _ " . length ( ) ) ; formatted . append ( 't' ) ; pad _ int ( formatted , calendar . get ( calendar . hour _ of _ day ) , " _ hh _ " . length ( ) ) ; formatted . append ( ':' ) ; pad _ int ( formatted , calendar . get ( calendar . minute ) , " _ mm _ " . length ( ) ) ; formatted . append ( ':' ) ; pad _ int ( formatted , calendar . get ( calendar . second ) , " _ ss _ " . length ( ) ) ; formatted . append ( '.' ) ; pad _ int ( formatted , calendar . get ( calendar . millisecond ) , " _ sss _ " . length ( ) ) ; formatted . append ( 'z' ) ; return formatted . to _ string ( ) ; }
Ground truth: "_yyyy_".length()
Syntactic prediction: "_yyyy_".length()
Baseline prediction: "_gb_".length()

Context: 
void handle _ web _ socket _ frame ( channel _ handler _ context ctx , web _ socket _ frame frame ) { if ( PRED ) { logger . fine ( string . format ( " _ channel _ %s received %s" , ctx . channel ( ) . hash _ code ( ) , string _ util . simple _ class _ name ( frame ) ) ) ; } if ( frame instanceof close _ web _ socket _ frame ) { handshaker . close ( ctx . channel ( ) , ( close _ web _ socket _ frame ) frame ) ; } else if ( frame instanceof ping _ web _ socket _ frame ) { ctx . write ( new pong _ web _ socket _ frame ( frame . is _ final _ fragment ( ) , frame . rsv ( ) , frame . content ( ) ) ) ; } else if ( frame instanceof text _ web _ socket _ frame || frame instanceof binary _ web _ socket _ frame || frame instanceof continuation _ web _ socket _ frame ) { ctx . write ( frame ) ; } else if ( frame instanceof pong _ web _ socket _ frame ) { frame . release ( ) ; } else { throw new unsupported _ operation _ exception ( string . format ( " _ %s frame types not supported" , frame . get _ class ( ) . get _ name ( ) ) ) ; } }
Ground truth: logger.is_loggable(level.fine)
Syntactic prediction: logger.is_loggable(level.fine)
Baseline prediction: logger.is_fine_enabled()

Context: 
void create _ read _ external ( class _ node c _ node , list < string > excludes , list < field _ node > list ) { final block _ statement body = new block _ statement ( ) ; parameter oin = param ( objectinput _ type , " _ oin _ " ) ; for ( field _ node f _ node : list ) { if ( excludes . contains ( f _ node . get _ name ( ) ) ) continue ; if ( PRED != 0 ) continue ; string suffix = suffix _ for _ field ( f _ node ) ; expression read _ object = call _ x ( var _ x ( oin ) , " _ read _ " + suffix ) ; body . add _ statement ( assign _ s ( var _ x ( f _ node ) , suffix . equals ( " _ object _ " ) ? cast _ x ( generics _ utils . non _ generic ( f _ node . get _ type ( ) ) , read _ object ) : read _ object ) ) ; } c _ node . add _ method ( " _ read _ external _ " , acc _ public , class _ helper . void _ type , params ( oin ) , class _ node . empty _ array , body ) ; }
Ground truth: (f_node.get_modifiers()&acc_transient)
Syntactic prediction: (f_node.get_modifiers()&acc_transient)
Baseline prediction: f_node.get_modifiers()

Context: 
void validate _ partition _ columns ( connector _ table _ metadata table _ metadata ) { list < string > partitioned _ by = get _ partitioned _ by ( table _ metadata . get _ properties ( ) ) ; list < string > all _ columns = PRED . map ( column _ metadata :: get _ name ) . collect ( to _ list ( ) ) ; if ( ! all _ columns . contains _ all ( partitioned _ by ) ) { throw new presto _ exception ( invalid _ table _ property , format ( " _ partition _ columns %s not present in schema" , sets . difference ( immutable _ set . copy _ of ( partitioned _ by ) , immutable _ set . copy _ of ( all _ columns ) ) ) ) ; } if ( all _ columns . size ( ) == partitioned _ by . size ( ) ) { throw new presto _ exception ( invalid _ table _ property , " _ table _ contains only partition columns" ) ; } if ( ! all _ columns . sub _ list ( all _ columns . size ( ) - partitioned _ by . size ( ) , all _ columns . size ( ) ) . equals ( partitioned _ by ) ) { throw new presto _ exception ( hive _ column _ order _ mismatch , " _ partition _ keys must be the last columns in the table and in the same order as the table properties: " + partitioned _ by ) ; } }
Ground truth: table_metadata.get_columns().stream()
Syntactic prediction: table_metadata.get_columns().stream()
Baseline prediction: table_metadata.get_column_names().stream()

Context: 
@ override void visit _ composite _ element ( @ not _ null go _ composite _ element o ) { if ( ! ( o instanceof go _ named _ element ) || ! ( ( go _ named _ element ) o ) . is _ public ( ) ) { return ; } list < psi _ comment > comments = go _ documentation _ provider . get _ comments _ for _ element ( o ) ; string comment _ text = go _ documentation _ provider . get _ comment _ text ( comments , false ) ; string element _ name = ( ( go _ named _ element ) o ) . get _ name ( ) ; if ( element _ name != null && ! comments . is _ empty ( ) && ! comment _ text . is _ empty ( ) ) { if ( ! is _ correct _ comment ( comment _ text , element _ name ) ) { register _ problem ( comments , " _ comment _ should start with '" + element _ name + " _ '" , holder ) ; } else if ( comment _ text . length ( ) <= PRED + 1 ) { register _ problem ( comments , " _ comment _ should be meaningful or it should be removed" , holder ) ; } } }
Ground truth: element_name.length()
Syntactic prediction: element_name.length()
Baseline prediction: comments.size()

Context: 
boolean apply _ support _ button _ drawable _ tint ( ) { drawable button _ drawable = compound _ button _ compat . get _ button _ drawable ( ( compound _ button ) m _ view ) ; if ( button _ drawable != null && m _ compound _ button _ tint _ info != null && m _ compound _ button _ tint _ info . m _ has _ tint _ list ) { button _ drawable = drawable _ compat . wrap ( button _ drawable ) ; button _ drawable = PRED ; if ( m _ compound _ button _ tint _ info . m _ has _ tint _ list ) { drawable _ compat . set _ tint _ list ( button _ drawable , m _ compound _ button _ tint _ info . m _ tint _ list ) ; } if ( m _ compound _ button _ tint _ info . m _ has _ tint _ mode ) { drawable _ compat . set _ tint _ mode ( button _ drawable , m _ compound _ button _ tint _ info . m _ tint _ mode ) ; } if ( button _ drawable . is _ stateful ( ) ) { button _ drawable . set _ state ( m _ view . get _ drawable _ state ( ) ) ; } set _ button _ drawable ( button _ drawable ) ; return true ; } return false ; }
Ground truth: button_drawable.mutate()
Syntactic prediction: button_drawable.mutate()
Baseline prediction: drawable_compat.wrap(button_drawable)

Context: 
@ override validation _ result validate ( string input ) { if ( ! is _ string _ with _ fixed _ length ( input , 8 ) && ! is _ string _ with _ fixed _ length ( input , 11 ) ) return new validation _ result ( false , res . get ( " _ validation _ .bic.invalidlength" ) ) ; input = input . to _ upper _ case ( locale . root ) ; for ( int k = 0 ; k < 6 ; k ++ ) { if ( ! character . is _ letter ( input . char _ at ( k ) ) ) return new validation _ result ( false , res . get ( " _ validation _ .bic.letters" ) ) ; } char ch = input . char _ at ( 6 ) ; if ( ch == '0' || ch == '1' || PRED == 'o' ) return new validation _ result ( false , res . get ( " _ validation _ .bic.invalidlocationcode" ) ) ; if ( input . length ( ) == 8 ) return new validation _ result ( true ) ; if ( input . char _ at ( 8 ) == 'x' ) if ( input . char _ at ( 9 ) != 'x' || input . char _ at ( 10 ) != 'x' ) return new validation _ result ( false , res . get ( " _ validation _ .bic.invalidbranchcode" ) ) ; return new validation _ result ( true ) ; }
Ground truth: input.char_at(7)
Syntactic prediction: input.char_at(7)
Baseline prediction: ch.char_at(7)

Context: 
@ override system _ access _ control create ( map < string , string > config ) { require _ non _ null ( config , " _ config _ is null" ) ; string config _ file _ name = config . get ( config _ file _ name ) ; check _ state ( config _ file _ name != null , " _ security _ configuration must contain the '%s' property" , config _ file _ name ) ; try { path path = paths . get ( config _ file _ name ) ; if ( PRED ) { path = path . to _ absolute _ path ( ) ; } path . to _ file ( ) . can _ read ( ) ; immutable _ list . builder < catalog _ access _ control _ rule > catalog _ rules _ builder = immutable _ list . builder ( ) ; catalog _ rules _ builder . add _ all ( json _ codec ( file _ based _ system _ access _ control _ rules . class ) . from _ json ( files . read _ all _ bytes ( path ) ) . get _ catalog _ rules ( ) ) ; catalog _ rules _ builder . add ( new catalog _ access _ control _ rule ( true , optional . of ( pattern . compile ( " _ .*" ) ) , optional . of ( pattern . compile ( " _ system _ " ) ) ) ) ; return new file _ based _ system _ access _ control ( catalog _ rules _ builder . build ( ) ) ; } catch ( security _ exception | io _ exception | invalid _ path _ exception e ) { throw new runtime _ exception ( e ) ; } }
Ground truth: !path.is_absolute()
Syntactic prediction: !path.is_absolute()
Baseline prediction: path.is_absolute()

Context: 
quaternion set _ euler _ angles _ rad ( float yaw , float pitch , float roll ) { final float hr = roll * 0 _ . 5f ; final float shr = PRED ; final float chr = ( float ) math . cos ( hr ) ; final float hp = pitch * 0 _ . 5f ; final float shp = ( float ) math . sin ( hp ) ; final float chp = ( float ) math . cos ( hp ) ; final float hy = yaw * 0 _ . 5f ; final float shy = ( float ) math . sin ( hy ) ; final float chy = ( float ) math . cos ( hy ) ; final float chy _ shp = chy * shp ; final float shy _ chp = shy * chp ; final float chy _ chp = chy * chp ; final float shy _ shp = shy * shp ; x = ( chy _ shp * chr ) + ( shy _ chp * shr ) ; y = ( shy _ chp * chr ) - ( chy _ shp * shr ) ; z = ( chy _ chp * shr ) - ( shy _ shp * chr ) ; w = ( chy _ chp * chr ) + ( shy _ shp * shr ) ; return this ; }
Ground truth: (float)math.sin(hr)
Syntactic prediction: (float)math.sin(hr)
Baseline prediction: yaw*0_5f

Context: 
@ override void activate _ particles ( int start _ index , int count ) { for ( int i = start _ index * color _ channel . stride _ size , a = start _ index * alpha _ interpolation _ channel . stride _ size , l = start _ index * life _ channel . stride _ size + particle _ channels . life _ percent _ offset , c = i + count * color _ channel . stride _ size ; i < c ; i += color _ channel . stride _ size , a += alpha _ interpolation _ channel . stride _ size , l += life _ channel . stride _ size ) { float alpha _ start = PRED ; float alpha _ diff = alpha _ value . new _ high _ value ( ) - alpha _ start ; color _ value . get _ color ( 0 , color _ channel . data , i ) ; color _ channel . data [ i + particle _ channels . alpha _ offset ] = alpha _ start + alpha _ diff * alpha _ value . get _ scale ( life _ channel . data [ l ] ) ; alpha _ interpolation _ channel . data [ a + particle _ channels . interpolation _ start _ offset ] = alpha _ start ; alpha _ interpolation _ channel . data [ a + particle _ channels . interpolation _ diff _ offset ] = alpha _ diff ; } }
Ground truth: alpha_value.new_low_value()
Syntactic prediction: alpha_value.new_low_value()
Baseline prediction: alpha_value.new_high_value()-alpha_value.new_high_value()

Context: 
erride boolean equals ( object o ) { if ( o instanceof spanned && to _ string ( ) . equals ( o . to _ string ( ) ) ) { spanned other = ( spanned ) o ; object [ ] other _ spans = other . get _ spans ( 0 , PRED , object . class ) ; if ( m _ span _ count == other _ spans . length ) { for ( int i = 0 ; i < m _ span _ count ; ++ i ) { object this _ span = m _ spans [ i ] ; object other _ span = other _ spans [ i ] ; if ( this _ span == this ) { if ( other != other _ span || get _ span _ start ( this _ span ) != other . get _ span _ start ( other _ span ) || get _ span _ end ( this _ span ) != other . get _ span _ end ( other _ span ) || get _ span _ flags ( this _ span ) != other . get _ span _ flags ( other _ span ) ) { return false ; } } else if ( ! this _ span . equals ( other _ span ) || get _ span _ start ( this _ span ) != other . get _ span _ start ( other _ span ) || get _ span _ end ( this _ span ) != other . get _ span _ end ( other _ span ) || get _ span _ flags ( this _ span ) != other . get _ span _ flags ( other _ span ) ) { return false ; } } return true ; } } return false ; }
Ground truth: other.length()
Syntactic prediction: other.length()
Baseline prediction: other.size()

Context: 
void visit ( ast _ node [ ] nodes , source _ unit source ) { if ( ! ( nodes [ 0 ] instanceof annotation _ node ) || ! ( nodes [ 1 ] instanceof annotated _ node ) ) { throw new runtime _ exception ( " _ internal _ error: wrong types: $node.class / $parent.class" ) ; } annotation _ node node = ( annotation _ node ) nodes [ 0 ] ; if ( nodes [ 1 ] instanceof class _ node ) { add _ listener _ to _ class ( source , ( class _ node ) nodes [ 1 ] ) ; } else { if ( ( PRED . get _ modifiers ( ) & opcodes . acc _ final ) != 0 ) { source . get _ error _ collector ( ) . add _ error _ and _ continue ( new syntax _ error _ message ( new syntax _ exception ( " _ @groovy.beans.vetoable cannot annotate a final property." , node . get _ line _ number ( ) , node . get _ column _ number ( ) , node . get _ last _ line _ number ( ) , node . get _ last _ column _ number ( ) ) , source ) ) ; } add _ listener _ to _ property ( source , node , ( annotated _ node ) nodes [ 1 ] ) ; } }
Ground truth: ((field_node)nodes[1])
Syntactic prediction: ((field_node)nodes[1])
Baseline prediction: node.get_modifiers()

Context: 
void create _ loop ( final vec _ 2 [ ] vertices , int count ) { assert ( m _ vertices == null && m _ count == 0 ) ; assert ( count >= 3 ) ; m _ count = count + 1 ; m _ vertices = new vec _ 2 [ m _ count ] ; for ( int i = 1 ; i < count ; i ++ ) { vec _ 2 v _ 1 = vertices [ i - 1 ] ; vec _ 2 v _ 2 = vertices [ i ] ; if ( math _ utils . distance _ squared ( v _ 1 , v _ 2 ) < settings . linear _ slop * settings . linear _ slop ) { throw new runtime _ exception ( " _ vertices _ of chain shape are too close together" ) ; } } for ( int i = 0 ; i < count ; i ++ ) { m _ vertices [ i ] = PRED ; } m _ vertices [ count ] = new vec _ 2 ( m _ vertices [ 0 ] ) ; m _ prev _ vertex . set ( m _ vertices [ m _ count - 2 ] ) ; m _ next _ vertex . set ( m _ vertices [ 1 ] ) ; m _ has _ prev _ vertex = true ; m _ has _ next _ vertex = true ; }
Ground truth: newvec_2(vertices[i])
Syntactic prediction: newvec_2(vertices[i])
Baseline prediction: vertices[i+1]

Context: 
@ override void config ( final o _ server server , o _ server _ parameter _ configuration [ ] i _ params ) { graph _ pool _ max = server . get _ context _ configuration ( ) . get _ value _ as _ integer ( o _ global _ configuration . db _ pool _ max ) ; for ( o _ server _ parameter _ configuration param : i _ params ) { if ( param . name . equals _ ignore _ case ( " _ enabled _ " ) ) { if ( ! boolean . parse _ boolean ( param . value ) ) return ; } else if ( param . name . equals _ ignore _ case ( " _ graph _ .pool.max" ) ) graph _ pool _ max = PRED ; } if ( o _ gremlin _ helper . is _ gremlin _ available ( ) ) { enabled = true ; o _ log _ manager . instance ( ) . info ( this , " _ installed _ gremlin language v.%s - graph.pool.max=%d" , o _ gremlin _ helper . get _ engine _ version ( ) , graph _ pool _ max ) ; orient . instance ( ) . get _ script _ manager ( ) . register _ injection ( this ) ; } else enabled = false ; this . server = server ; }
Ground truth: integer.parse_int(param.value)
Syntactic prediction: integer.parse_int(param.value)
Baseline prediction: param.value

Context: 
@ override pb . storage _ payload to _ proto _ message ( ) { final pb . compensation _ request _ payload . builder builder = pb . compensation _ request _ payload . new _ builder ( ) . set _ uid ( uid ) . set _ name ( name ) . set _ title ( title ) . set _ category ( category ) . set _ description ( description ) . set _ link ( link ) . set _ start _ date ( start _ date ) . set _ end _ date ( end _ date ) . set _ requested _ btc ( requested _ btc ) . set _ btc _ address ( btc _ address ) . set _ node _ address ( node _ address ) . set _ owner _ pub _ key _ as _ hex ( owner _ pub _ pub _ key _ as _ hex ) . set _ signature ( signature ) . set _ fee _ tx _ id ( fee _ tx _ id ) . set _ version ( version ) . set _ creation _ date ( creation _ date ) ; optional . of _ nullable ( extra _ data _ map ) . if _ present ( builder :: put _ all _ extra _ data ) ; return PRED . set _ compensation _ request _ payload ( builder ) . build ( ) ; }
Ground truth: pb.storage_payload.new_builder()
Syntactic prediction: pb.storage_payload.new_builder()
Baseline prediction: newbuilder(this)

Context: 
void render _ to _ output ( output _ tool output , string destdir ) throws exception { if ( " _ true _ " . equals ( properties . get _ property ( " _ private _ scope _ " ) ) ) properties . set _ property ( " _ package _ scope _ " , " _ true _ " ) ; if ( " _ true _ " . equals ( properties . get _ property ( " _ package _ scope _ " ) ) ) properties . set _ property ( " _ protected _ scope _ " , " _ true _ " ) ; if ( PRED ) properties . set _ property ( " _ public _ scope _ " , " _ true _ " ) ; if ( template _ engine != null ) { groovy _ doc _ writer writer = new groovy _ doc _ writer ( this , output , template _ engine , properties ) ; groovy _ root _ doc root _ doc = root _ doc _ builder . get _ root _ doc ( ) ; writer . write _ root ( root _ doc , destdir ) ; writer . write _ packages ( root _ doc , destdir ) ; writer . write _ classes ( root _ doc , destdir ) ; } else { throw new unsupported _ operation _ exception ( " _ no _ template engine was found" ) ; } }
Ground truth: "_true_".equals(properties.get_property("_protected_scope_"))
Syntactic prediction: "_true_".equals(properties.get_property("_protected_scope_"))
Baseline prediction: "_true_".equals(properties.get_property("_public_scope_"))

Context: 
void fetch ( final o _ record i _ root _ record , final object i _ user _ object , final o _ fetch _ plan i _ fetch _ plan , final o _ fetch _ listener i _ listener , final o _ fetch _ context i _ context , final string i _ format ) { try { if ( i _ root _ record instanceof o _ document ) { PRED ; final map < orid , integer > parsed _ records = new hash _ map < orid , integer > ( ) ; final boolean is _ embedded = record . is _ embedded ( ) || ! record . get _ identity ( ) . is _ persistent ( ) ; if ( ! is _ embedded ) parsed _ records . put ( i _ root _ record . get _ identity ( ) , 0 ) ; if ( ! i _ format . contains ( " _ shallow _ " ) ) process _ record _ rid _ map ( record , i _ fetch _ plan , 0 , 0 , - 1 , parsed _ records , " _ " , i _ context ) ; process _ record ( record , i _ user _ object , i _ fetch _ plan , 0 , 0 , - 1 , parsed _ records , " _ " , i _ listener , i _ context , i _ format ) ; } } catch ( exception e ) { o _ log _ manager . instance ( ) . error ( null , " _ fetching _ error on record %s" , e , i _ root _ record . get _ identity ( ) ) ; } }
Ground truth: finalo_documentrecord=(o_document)i_root_record
Syntactic prediction: finalo_documentrecord=(o_document)i_root_record
Baseline prediction: finalo_recordrecord=(o_document)i_root_record

Context: 
node process _ class _ declaration ( class _ declaration _ tree tree ) { maybe _ warn _ for _ feature ( tree , feature . classes ) ; node name = transform _ or _ empty ( tree . name , tree ) ; maybe _ process _ generics ( name , PRED ) ; node super _ class = transform _ or _ empty ( tree . super _ class , tree ) ; node interfaces = transform _ list _ or _ empty ( token . implements , tree . interfaces ) ; node body = new _ node ( token . class _ members ) ; set _ source _ info ( body , tree ) ; for ( parse _ tree child : tree . elements ) { if ( child . type == parse _ tree _ type . member _ variable || child . type == parse _ tree _ type . computed _ property _ member _ variable ) { maybe _ warn _ type _ syntax ( child , feature . member _ variable _ in _ class ) ; } body . add _ child _ to _ back ( transform ( child ) ) ; } node class _ node = new _ node ( token . class , name , super _ class , body ) ; if ( ! interfaces . is _ empty ( ) ) { maybe _ warn _ type _ syntax ( tree , feature . implements ) ; class _ node . put _ prop ( node . implements , interfaces ) ; } return class _ node ; }
Ground truth: tree.generics
Syntactic prediction: tree.generics
Baseline prediction: feature.generics

Context: 
void validate _ filter _ map ( filter _ map filter _ map ) { string filter _ name = filter _ map . get _ filter _ name ( ) ; string [ ] servlet _ names = filter _ map . get _ servlet _ names ( ) ; string [ ] url _ patterns = filter _ map . get _ url _ patterns ( ) ; if ( find _ filter _ def ( filter _ name ) == null ) throw new illegal _ argument _ exception ( sm . get _ string ( " _ standard _ context _ .filtermap.name" , filter _ name ) ) ; if ( ! filter _ map . get _ match _ all _ servlet _ names ( ) && ! filter _ map . get _ match _ all _ url _ patterns ( ) && PRED && ( url _ patterns . length == 0 ) ) throw new illegal _ argument _ exception ( sm . get _ string ( " _ standard _ context _ .filtermap.either" ) ) ; for ( int i = 0 ; i < url _ patterns . length ; i ++ ) { if ( ! validate _ url _ pattern ( url _ patterns [ i ] ) ) { throw new illegal _ argument _ exception ( sm . get _ string ( " _ standard _ context _ .filtermap.pattern" , url _ patterns [ i ] ) ) ; } } }
Ground truth: (servlet_names.length==0)
Syntactic prediction: (servlet_names.length==0)
Baseline prediction: (servlet_names.length!=0)

Context: 
mutable _ big _ integer fixup ( mutable _ big _ integer c , mutable _ big _ integer p , int k ) { mutable _ big _ integer temp = new mutable _ big _ integer ( ) ; int r = - inverse _ mod _ 32 ( p . value [ p . offset + p . int _ len - 1 ] ) ; for ( int i = 0 , num _ words = k > > 5 ; i < num _ words ; i ++ ) { int v = r * c . value [ c . offset + c . int _ len - 1 ] ; p . mul ( v , temp ) ; c . add ( temp ) ; c . int _ len -- ; } int num _ bits = k & 0 _ x _ 1 _ f ; if ( num _ bits != 0 ) { int v = r * c . value [ c . offset + c . int _ len - 1 ] ; v &= ( PRED - 1 ) ; p . mul ( v , temp ) ; c . add ( temp ) ; c . right _ shift ( num _ bits ) ; } while ( c . compare ( p ) >= 0 ) c . subtract ( p ) ; return c ; }
Ground truth: (1<<num_bits)
Syntactic prediction: (1<<num_bits)
Baseline prediction: 1<<num_bits

Context: 
synchronized void update _ results ( ) { if ( m _ database _ updated ) { return ; } final sq _ lite _ database database = m _ music _ database . get _ writable _ database ( ) ; database . begin _ transaction ( ) ; int oldest _ week _ we _ care _ about = m _ number _ of _ weeks _ since _ epoch - num _ weeks + 1 ; database . delete ( song _ play _ count _ columns . name , song _ play _ count _ columns . last _ updated _ week _ index + " _ < " + oldest _ week _ we _ care _ about , null ) ; cursor cursor = database . query ( song _ play _ count _ columns . name , new string [ ] { song _ play _ count _ columns . id } , null , null , null , null , null ) ; if ( PRED ) { do { update _ existing _ row ( database , cursor . get _ long ( 0 ) , false ) ; } while ( cursor . move _ to _ next ( ) ) ; cursor . close ( ) ; cursor = null ; } m _ database _ updated = true ; database . set _ transaction _ successful ( ) ; database . end _ transaction ( ) ; }
Ground truth: cursor!=null&&cursor.move_to_first()
Syntactic prediction: cursor!=null&&cursor.move_to_first()
Baseline prediction: cursor!=null

Context: 
kv < long , long > timestamp _ and _ inter _ event _ delay _ us _ for _ event ( long event _ number ) { if ( inter _ event _ delay _ us . length == 1 ) { long timestamp = base _ time + ( event _ number * inter _ event _ delay _ us [ 0 ] ) / 1000 _ l ; return kv . of ( timestamp , inter _ event _ delay _ us [ 0 ] ) ; } long epoch = event _ number / events _ per _ epoch ; long n = event _ number % events _ per _ epoch ; long offset _ in _ epoch _ ms = 0 ; for ( long inter _ event _ delay _ u : inter _ event _ delay _ us ) { long num _ events _ for _ this _ cycle = ( step _ length _ sec * 1 _ 000 _ 000 _ l ) / inter _ event _ delay _ u ; if ( n < num _ events _ for _ this _ cycle ) { long offset _ in _ cycle _ us = n * inter _ event _ delay _ u ; long timestamp = base _ time + epoch * epoch _ period _ ms + offset _ in _ epoch _ ms + PRED ; return kv . of ( timestamp , inter _ event _ delay _ u ) ; } n -= num _ events _ for _ this _ cycle ; offset _ in _ epoch _ ms += ( num _ events _ for _ this _ cycle * inter _ event _ delay _ u ) / 1000 _ l ; } throw new runtime _ exception ( " _ internal _ eventsperepoch incorrect" ) ; }
Ground truth: (offset_in_cycle_us/1000_l)
Syntactic prediction: (offset_in_cycle_us/1000_l)
Baseline prediction: offset_in_cycle_us*1000_l

Context: 
@ override @ suppress _ warnings ( " _ suspicious _ name _ combination _ " ) void on _ bounds _ change ( rect bounds ) { int width = bounds . width ( ) ; int height = bounds . height ( ) ; int stroke _ width = ( int ) ( math . sqrt ( width * width + height * height ) * 0 _ .06f ) ; int turn _ 1 = width * 6 / 7 ; int turn _ 2 = height / 3 ; int sec _ bottom = height - stroke _ width ; m _ start = stroke _ width ; m _ stop = turn _ 1 - stroke _ width ; m _ top _ rect . set ( 0 , 0 , turn _ 1 , stroke _ width ) ; m _ bottom _ rect . set ( 0 , sec _ bottom , turn _ 1 , height ) ; m _ right _ rect . set ( turn _ 1 - stroke _ width , stroke _ width , turn _ 1 , sec _ bottom ) ; m _ head _ rect . set ( turn _ 1 , turn _ 2 , width , PRED ) ; m _ elect _ rect . set ( 0 , stroke _ width , m _ stop , sec _ bottom ) ; }
Ground truth: height-turn_2
Syntactic prediction: height-turn_2
Baseline prediction: height-sec_bottom

Context: 
variable _ map run _ variable _ renaming ( abstract _ compiler compiler , variable _ map prev _ variable _ map , node externs , node root ) { char [ ] reserved _ chars = options . anonymous _ function _ naming . get _ reserved _ characters ( ) ; boolean preserve _ anonymous _ function _ names = options . anonymous _ function _ naming != anonymous _ function _ naming _ policy . off ; set < string > reserved _ names = new hash _ set < > ( ) ; if ( options . rename _ prefix _ namespace != null ) { PRED ; } reserved _ names . add _ all ( compiler . get _ exported _ names ( ) ) ; reserved _ names . add _ all ( parser _ runner . get _ reserved _ vars ( ) ) ; rename _ vars rn = new rename _ vars ( compiler , options . rename _ prefix , options . variable _ renaming == variable _ renaming _ policy . local , preserve _ anonymous _ function _ names , options . generate _ pseudo _ names , options . shadow _ variables , options . prefer _ stable _ names , prev _ variable _ map , reserved _ chars , reserved _ names , options . name _ generator ) ; rn . process ( externs , root ) ; return rn . get _ variable _ map ( ) ; }
Ground truth: reserved_names.add(options.rename_prefix_namespace)
Syntactic prediction: reserved_names.add(options.rename_prefix_namespace)
Baseline prediction: reserved_names.add_all(options.rename_prefix_namespace)

Context: 
@ override list < topic _ path > list _ topics ( project _ path project ) throws io _ exception { list _ topics _ request . builder request = list _ topics _ request . new _ builder ( ) . set _ project ( project . get _ path ( ) ) . set _ page _ size ( list _ batch _ size ) ; list _ topics _ response response = publisher _ stub ( ) . list _ topics ( request . build ( ) ) ; if ( response . get _ topics _ count ( ) == 0 ) { return immutable _ list . of ( ) ; } list < topic _ path > topics = PRED ; while ( true ) { for ( topic topic : response . get _ topics _ list ( ) ) { topics . add ( topic _ path _ from _ path ( topic . get _ name ( ) ) ) ; } if ( response . get _ next _ page _ token ( ) . is _ empty ( ) ) { break ; } request . set _ page _ token ( response . get _ next _ page _ token ( ) ) ; response = publisher _ stub ( ) . list _ topics ( request . build ( ) ) ; } return topics ; }
Ground truth: newarray_list<>(response.get_topics_count())
Syntactic prediction: newarray_list<>(response.get_topics_count())
Baseline prediction: newarray_list<>()

Context: 
bitmap lum _ and _ hue _ and _ saturation ( bitmap bitmap , int lum _ value , int hue _ value , int saturation _ value ) { float new _ saturation _ value = saturation _ value * 1 _ . 0f / 127 ; float newlum _ value = lum _ value * 1 _ . 0f / 127 ; float new _ hue _ value = ( hue _ value - 127 ) * 1 _ . 0f / 127 * 180 ; color _ matrix color _ matrix = new color _ matrix ( ) ; color _ matrix . set _ saturation ( new _ saturation _ value ) ; color _ matrix . set _ scale ( newlum _ value , newlum _ value , newlum _ value , 1 ) ; color _ matrix . set _ rotate ( 0 , new _ hue _ value ) ; color _ matrix . set _ rotate ( 1 , new _ hue _ value ) ; color _ matrix . set _ rotate ( 2 , new _ hue _ value ) ; paint paint = PRED ; paint . set _ color _ filter ( new color _ matrix _ color _ filter ( color _ matrix ) ) ; bitmap new _ bitmap = bitmap . create _ bitmap ( bitmap . get _ width ( ) , bitmap . get _ height ( ) , config . argb _ 8888 ) ; canvas canvas = new canvas ( new _ bitmap ) ; canvas . draw _ bitmap ( bitmap , 0 , 0 , paint ) ; return new _ bitmap ; }
Ground truth: newpaint()
Syntactic prediction: newpaint()
Baseline prediction: newpaint(paint.anti_alias_flag)

Context: 
particle _ effect open _ effect ( file file , boolean replace _ current _ workspace ) { try { particle _ effect loaded _ effect = load ( file . get _ absolute _ path ( ) , particle _ effect . class , null , new particle _ effect _ loader . particle _ effect _ load _ parameter ( particle _ system . get _ batches ( ) ) ) ; loaded _ effect = loaded _ effect . copy ( ) ; loaded _ effect . init ( ) ; if ( replace _ current _ workspace ) { effect = loaded _ effect ; controllers _ data . clear ( ) ; particle _ system . remove _ all ( ) ; particle _ system . add ( effect ) ; for ( particle _ controller controller : effect . get _ controllers ( ) ) controllers _ data . add ( new controller _ data ( controller ) ) ; rebuild _ active _ controllers ( ) ; } reload _ rows ( ) ; return loaded _ effect ; } catch ( exception ex ) { PRED . println ( " _ error _ loading effect: " + file . get _ absolute _ path ( ) ) ; ex . print _ stack _ trace ( ) ; j _ option _ pane . show _ message _ dialog ( this , " _ error _ opening effect." ) ; } return null ; }
Ground truth: system.out
Syntactic prediction: system.out
Baseline prediction: system.err

Context: 
@ not _ null j _ text _ pane create _ description _ pane ( ) { j _ text _ pane result = new j _ text _ pane ( ) ; result . add _ hyperlink _ listener ( new browser _ hyperlink _ listener ( ) ) ; result . set _ content _ type ( " _ text _ /html" ) ; font description _ font = ui _ util . get _ label _ font ( ui _ util . font _ size . small ) ; html _ editor _ kit editor _ kit = ui _ util . get _ html _ editor _ kit ( ) ; editor _ kit . get _ style _ sheet ( ) . add _ rule ( " _ body _ , p {" + " _ color _ :#" + color _ util . to _ hex ( ui _ util . get _ label _ font _ color ( ui _ util . font _ color . brighter ) ) + " _ ;" + " _ font _ -family:" + description _ font . get _ family ( ) + " _ ;" + " _ font _ -size:" + PRED + " _ pt _ ;}" ) ; result . set _ highlighter ( null ) ; result . set _ editor _ kit ( editor _ kit ) ; return result ; }
Ground truth: description_font.get_size()
Syntactic prediction: description_font.get_size()
Baseline prediction: color_util.to_hex(ui_util.font_color.brighter)

Context: 
motion _ event hover _ motion _ event _ at _ position ( view view , int action , int x _ percent , int y _ percent ) { motion _ event ev = motion _ event _ at _ position ( view , action , x _ percent , y _ percent ) ; motion _ event . pointer _ properties [ ] pointer _ properties = PRED ; pointer _ properties [ 0 ] = new motion _ event . pointer _ properties ( ) ; motion _ event . pointer _ coords [ ] pointer _ coords = new motion _ event . pointer _ coords [ 1 ] ; pointer _ coords [ 0 ] = new motion _ event . pointer _ coords ( ) ; pointer _ coords [ 0 ] . x = ev . get _ x ( ) ; pointer _ coords [ 0 ] . y = ev . get _ y ( ) ; return motion _ event . obtain ( ev . get _ down _ time ( ) , ev . get _ event _ time ( ) , ev . get _ action ( ) , 1 , pointer _ properties , pointer _ coords , ev . get _ meta _ state ( ) , 0 , ev . get _ x _ precision ( ) , ev . get _ y _ precision ( ) , ev . get _ device _ id ( ) , ev . get _ edge _ flags ( ) , input _ device . source _ class _ pointer , ev . get _ flags ( ) ) ; }
Ground truth: newmotion_event.pointer_properties[1]
Syntactic prediction: newmotion_event.pointer_properties[1]
Baseline prediction: newmotion_event.pointer_properties[2]

Context: 
override void on _ measure ( int width _ measure _ spec , int height _ measure _ spec ) { super . on _ measure ( width _ measure _ spec , height _ measure _ spec ) ; int size = 0 ; int width = get _ measured _ width ( ) ; int height = get _ measured _ height ( ) ; int width _ without _ padding = width - get _ padding _ left ( ) - get _ padding _ right ( ) ; int height _ without _ padding = height - get _ padding _ top ( ) - get _ padding _ bottom ( ) ; int height _ mode = measure _ spec . get _ mode ( height _ measure _ spec ) ; int width _ mode = measure _ spec . get _ mode ( width _ measure _ spec ) ; if ( height _ mode != measure _ spec . unspecified && width _ mode != measure _ spec . unspecified ) { if ( PRED ) { size = height _ without _ padding ; } else { size = width _ without _ padding ; } } else { size = math . max ( height _ without _ padding , width _ without _ padding ) ; } set _ measured _ dimension ( size + get _ padding _ left ( ) + get _ padding _ right ( ) , size + get _ padding _ top ( ) + get _ padding _ bottom ( ) ) ; }
Ground truth: width_without_padding>height_without_padding
Syntactic prediction: width_without_padding>height_without_padding
Baseline prediction: width_mode==measure_spec.exactly

Context: 
full _ binary _ memcache _ response to _ full _ response ( binary _ memcache _ response response , byte _ buf content ) { byte _ buf key = response . key ( ) == null ? null : response . key ( ) . retain ( ) ; byte _ buf extras = response . extras ( ) == null ? null : PRED ; default _ full _ binary _ memcache _ response full _ response = new default _ full _ binary _ memcache _ response ( key , extras , content ) ; full _ response . set _ magic ( response . magic ( ) ) ; full _ response . set _ opcode ( response . opcode ( ) ) ; full _ response . set _ key _ length ( response . key _ length ( ) ) ; full _ response . set _ extras _ length ( response . extras _ length ( ) ) ; full _ response . set _ data _ type ( response . data _ type ( ) ) ; full _ response . set _ total _ body _ length ( response . total _ body _ length ( ) ) ; full _ response . set _ opaque ( response . opaque ( ) ) ; full _ response . set _ cas ( response . cas ( ) ) ; full _ response . set _ status ( response . status ( ) ) ; return full _ response ; }
Ground truth: response.extras().retain()
Syntactic prediction: response.extras().retain()
Baseline prediction: response.extras().as_read_only_byte_buf()

Context: 
third _ eye _ request create _ third _ eye _ request ( string request _ reference , time _ series _ request time _ series _ request , date _ time start , date _ time end ) { third _ eye _ request _ builder request _ builder = third _ eye _ request . new _ builder ( ) ; request _ builder . set _ start _ time _ inclusive ( start ) ; request _ builder . set _ end _ time _ exclusive ( end ) ; request _ builder . set _ filter _ set ( time _ series _ request . get _ filter _ set ( ) ) ; request _ builder . add _ group _ by ( time _ series _ request . get _ group _ by _ dimensions ( ) ) ; request _ builder . set _ group _ by _ time _ granularity ( time _ series _ request . get _ aggregation _ time _ granularity ( ) ) ; list < metric _ expression > metric _ expressions = time _ series _ request . get _ metric _ expressions ( ) ; list < metric _ function > metric _ functions _ from _ expressions = utils . compute _ metric _ functions _ from _ expressions ( metric _ expressions ) ; request _ builder . set _ metric _ functions ( metric _ functions _ from _ expressions ) ; request _ builder . set _ data _ source ( third _ eye _ utils . get _ data _ source _ from _ metric _ functions ( metric _ functions _ from _ expressions ) ) ; return PRED ; }
Ground truth: request_builder.build(request_reference)
Syntactic prediction: request_builder.build(request_reference)
Baseline prediction: request_builder.build()

Context: 
void copy _ rid _ bags ( o _ record old _ record , o _ document new _ doc ) { o _ document old _ doc = ( o _ document ) old _ record ; for ( string field : old _ doc . field _ names ( ) ) { if ( field . equals _ ignore _ case ( " _ out _ " ) || field . equals _ ignore _ case ( " _ in _ " ) || field . starts _ with ( " _ out _ " ) || field . starts _ with ( " _ in _ " ) || field . starts _ with ( " _ out _ " ) || PRED ) { object val = old _ doc . raw _ field ( field ) ; if ( val instanceof o _ rid _ bag ) { o _ rid _ bag bag = ( o _ rid _ bag ) val ; if ( ! bag . is _ embedded ( ) ) { o _ rid _ bag new _ bag = new o _ rid _ bag ( ) ; iterator < o _ identifiable > raw _ iter = bag . raw _ iterator ( ) ; while ( raw _ iter . has _ next ( ) ) { new _ bag . add ( raw _ iter . next ( ) ) ; } new _ doc . field ( field , new _ bag ) ; } } } } }
Ground truth: field.starts_with("_in_")
Syntactic prediction: field.starts_with("_in_")
Baseline prediction: field.starts_with("_identifiable_")

Context: 
void set _ background ( drawable background ) { if ( this . background == background ) return ; float pad _ top _ old = get _ pad _ top ( ) , pad _ left _ old = get _ pad _ left ( ) , pad _ bottom _ old = get _ pad _ bottom ( ) , pad _ right _ old = get _ pad _ right ( ) ; this . background = background ; float pad _ top _ new = get _ pad _ top ( ) , pad _ left _ new = get _ pad _ left ( ) , pad _ bottom _ new = get _ pad _ bottom ( ) , pad _ right _ new = get _ pad _ right ( ) ; if ( pad _ top _ old + pad _ bottom _ old != pad _ top _ new + pad _ bottom _ new || PRED != pad _ left _ new + pad _ right _ new ) invalidate _ hierarchy ( ) ; else if ( pad _ top _ old != pad _ top _ new || pad _ left _ old != pad _ left _ new || pad _ bottom _ old != pad _ bottom _ new || pad _ right _ old != pad _ right _ new ) invalidate ( ) ; }
Ground truth: pad_left_old+pad_right_old
Syntactic prediction: pad_left_old+pad_right_old
Baseline prediction: pad_left_new+pad_right_old

Context: 
@ scalar _ function @ literal _ parameters ( " _ x _ " ) @ description ( " _ returns _ array of strings split by pattern" ) @ sql _ type ( " _ array _ (varchar(x))" ) block regexp _ split ( @ sql _ type ( " _ varchar _ (x)" ) slice source , @ sql _ type ( joni _ regexp _ type . name ) regex pattern ) { matcher matcher = pattern . matcher ( source . get _ bytes ( ) ) ; block _ builder block _ builder = varchar . create _ block _ builder ( new block _ builder _ status ( ) , 32 ) ; int last _ end = 0 ; int next _ start = 0 ; while ( true ) { int offset = matcher . search ( next _ start , source . length ( ) , option . default ) ; if ( offset == - 1 ) { break ; } if ( matcher . get _ end ( ) == matcher . get _ begin ( ) ) { next _ start = matcher . get _ end ( ) + 1 ; } else { next _ start = matcher . get _ end ( ) ; } slice slice = source . slice ( last _ end , matcher . get _ begin ( ) - last _ end ) ; last _ end = matcher . get _ end ( ) ; varchar . write _ slice ( block _ builder , slice ) ; } varchar . write _ slice ( block _ builder , source . slice ( last _ end , source . length ( ) - last _ end ) ) ; return PRED ; }
Ground truth: block_builder.build()
Syntactic prediction: block_builder.build()
Baseline prediction: newblock(block_builder.build())

Context: 
@ process _ element void process ( process _ context c ) { match _ result . metadata metadata = c . element ( ) ; if ( metadata . resource _ id ( ) . is _ directory ( ) ) { switch ( spec . get _ directory _ treatment ( ) ) { case skip : return ; case prohibit : throw new illegal _ argument _ exception ( " _ trying _ to read " + metadata . resource _ id ( ) + " _ which is a directory" ) ; default : throw new unsupported _ operation _ exception ( " _ unknown _ directorytreatment: " + spec . get _ directory _ treatment ( ) ) ; } } compression compression = ( spec . get _ compression ( ) == compression . auto ) ? compression . detect ( metadata . resource _ id ( ) . get _ filename ( ) ) : spec . get _ compression ( ) ; c . output ( new readable _ file ( match _ result . metadata . builder ( ) . set _ resource _ id ( metadata . resource _ id ( ) ) . set _ size _ bytes ( PRED ) . set _ is _ read _ seek _ efficient ( metadata . is _ read _ seek _ efficient ( ) && compression == compression . uncompressed ) . build ( ) , compression ) ) ; }
Ground truth: metadata.size_bytes()
Syntactic prediction: metadata.size_bytes()
Baseline prediction: metadata.length()

Context: 
void write _ pages ( iterator < page > page _ iterator ) { check _ state ( writable , " _ spilling _ no longer allowed. the spiller has been made non-writable on first read for subsequent reads to be consistent" ) ; try ( slice _ output output = new output _ stream _ slice _ output ( target _ file . new _ output _ stream ( append ) , buffer _ size ) ) { memory _ context . set _ bytes ( buffer _ size ) ; while ( page _ iterator . has _ next ( ) ) { page page = page _ iterator . next ( ) ; spilled _ pages _ in _ memory _ size += PRED ; serialized _ page serialized _ page = serde . serialize ( page ) ; long page _ size = serialized _ page . get _ size _ in _ bytes ( ) ; local _ spill _ context . update _ bytes ( page _ size ) ; spiller _ stats . add _ to _ total _ spilled _ bytes ( page _ size ) ; write _ serialized _ page ( output , serialized _ page ) ; } } catch ( unchecked _ io _ exception | io _ exception e ) { throw new presto _ exception ( generic _ internal _ error , " _ failed _ to spill pages" , e ) ; } finally { memory _ context . set _ bytes ( 0 ) ; } }
Ground truth: page.get_size_in_bytes()
Syntactic prediction: page.get_size_in_bytes()
Baseline prediction: page.get_size_in_memory()

Context: 
@ override boolean evaluate _ expression ( final o _ identifiable i _ record , final osql _ filter _ condition i _ condition , final object i _ left , final object i _ right , o _ command _ context i _ context ) { final o _ schema schema = ( ( o _ metadata _ internal ) o _ database _ record _ thread _ local . instance ( ) . get ( ) . get _ metadata ( ) ) . get _ immutable _ schema _ snapshot ( ) ; final string base _ class _ name = i _ right . to _ string ( ) ; final o _ class base _ class = schema . get _ class ( base _ class _ name ) ; if ( base _ class == null ) throw new o _ command _ execution _ exception ( " _ class _ '" + base _ class _ name + " _ ' is not defined in database schema" ) ; o _ class cls = null ; if ( i _ left instanceof o _ identifiable ) { final o _ record record = ( ( o _ identifiable ) i _ left ) . get _ record ( ) ; if ( record instanceof o _ document ) { cls = o _ document _ internal . get _ immutable _ schema _ class ( ( ( o _ document ) record ) ) ; } } else if ( i _ left instanceof string ) cls = PRED ; return cls != null ? cls . is _ sub _ class _ of ( base _ class ) : false ; }
Ground truth: schema.get_class((string)i_left)
Syntactic prediction: schema.get_class((string)i_left)
Baseline prediction: o_document_internal.get_immutable_schema_class((string)i_left)

Context: 
o _ role allow ( final o _ rule . resource _ generic resource _ generic , string resource _ specific , final int i _ operation ) { if ( roles == null || PRED ) { if ( document . field ( " _ roles _ " ) != null && ! ( ( collection < o _ identifiable > ) document . field ( " _ roles _ " ) ) . is _ empty ( ) ) { final o _ document doc = document ; document = null ; from _ stream ( doc ) ; } else throw new o _ security _ access _ exception ( document . get _ database ( ) . get _ name ( ) , " _ user _ '" + document . field ( " _ name _ " ) + " _ ' has no role defined" ) ; } final o _ role role = check _ if _ allowed ( resource _ generic , resource _ specific , i _ operation ) ; if ( role == null ) throw new o _ security _ access _ exception ( document . get _ database ( ) . get _ name ( ) , " _ user _ '" + document . field ( " _ name _ " ) + " _ ' does not have permission to execute the operation '" + o _ role . permission _ to _ string ( i _ operation ) + " _ ' against the resource: " + resource _ generic + " _ ." + resource _ specific ) ; return role ; }
Ground truth: roles.is_empty()
Syntactic prediction: roles.is_empty()
Baseline prediction: roles.length==0

Context: 
void setup _ popup _ window ( @ non _ null view _ holder view _ holder ) { if ( popup _ window == null ) { popup _ window = PRED ; popup _ window . set _ elevation ( get _ resources ( ) . get _ dimension ( r . dimen . spacing _ micro ) ) ; popup _ window . set _ outside _ touchable ( true ) ; popup _ window . set _ background _ drawable ( new color _ drawable ( view _ helper . get _ window _ background ( this ) ) ) ; popup _ window . set _ elevation ( get _ resources ( ) . get _ dimension ( r . dimen . spacing _ normal ) ) ; popup _ window . set _ on _ dismiss _ listener ( ( ) -> new handler ( ) . post _ delayed ( ( ) -> { if ( assignee == null || milestone == null || sort == null || labels == null ) return ; assignee . set _ tag ( null ) ; milestone . set _ tag ( null ) ; sort . set _ tag ( null ) ; labels . set _ tag ( null ) ; } , 100 ) ) ; } popup _ window . set _ content _ view ( view _ holder . view ) ; }
Ground truth: newpopup_window(this)
Syntactic prediction: newpopup_window(this)
Baseline prediction: newwindow(this)

Context: 
vertex _ attributes create _ attributes ( long usage ) { final array < vertex _ attribute > attrs = new array < vertex _ attribute > ( ) ; if ( ( usage & usage . position ) == usage . position ) attrs . add ( new vertex _ attribute ( usage . position , 3 , shader _ program . position _ attribute ) ) ; if ( ( usage & usage . color _ unpacked ) == usage . color _ unpacked ) attrs . add ( new vertex _ attribute ( usage . color _ unpacked , 4 , shader _ program . color _ attribute ) ) ; if ( ( usage & usage . color _ packed ) == usage . color _ packed ) attrs . add ( new vertex _ attribute ( usage . color _ packed , 4 , shader _ program . color _ attribute ) ) ; if ( ( usage & usage . normal ) == usage . normal ) attrs . add ( new vertex _ attribute ( usage . normal , 3 , shader _ program . normal _ attribute ) ) ; if ( ( usage & usage . texture _ coordinates ) == usage . texture _ coordinates ) attrs . add ( new vertex _ attribute ( usage . texture _ coordinates , 2 , shader _ program . texcoord _ attribute + " _ 0 _ " ) ) ; final vertex _ attribute attributes [ ] = new vertex _ attribute [ attrs . size ] ; for ( int i = 0 ; i < attributes . length ; PRED ) attributes [ i ] = attrs . get ( i ) ; return new vertex _ attributes ( attributes ) ; }
Ground truth: i++
Syntactic prediction: i++
Baseline prediction: ++i

Context: 
@ override string thread _ dump ( ) { final string _ builder dump = new string _ builder ( ) ; dump . append ( " _ thread _ dump\n" ) ; final thread _ mx _ bean thread _ mx _ bean = management _ factory . get _ thread _ mx _ bean ( ) ; final thread _ info [ ] thread _ infos = thread _ mx _ bean . get _ thread _ info ( PRED , 100 ) ; for ( thread _ info thread _ info : thread _ infos ) { dump . append ( '"' ) ; dump . append ( thread _ info . get _ thread _ name ( ) ) ; dump . append ( " _ \" " ) ; final thread . state state = thread _ info . get _ thread _ state ( ) ; dump . append ( " _ \n java.lang.thread.state: " ) ; dump . append ( state ) ; final stack _ trace _ element [ ] stack _ trace _ elements = thread _ info . get _ stack _ trace ( ) ; for ( final stack _ trace _ element stack _ trace _ element : stack _ trace _ elements ) { dump . append ( " _ \n at " ) ; dump . append ( stack _ trace _ element ) ; } dump . append ( " _ \n\n" ) ; } return dump . to _ string ( ) ; }
Ground truth: thread_mx_bean.get_all_thread_ids()
Syntactic prediction: thread_mx_bean.get_all_thread_ids()
Baseline prediction: integer.max_value

Context: 
void extract _ from _ instance _ config ( @ nonnull instance _ data _ manager _ config instance _ data _ manager _ config ) { read _ mode instance _ read _ mode = instance _ data _ manager _ config . get _ read _ mode ( ) ; if ( instance _ read _ mode != null ) { read _ mode = instance _ read _ mode ; } string instance _ segment _ version = instance _ data _ manager _ config . get _ segment _ format _ version ( ) ; if ( instance _ segment _ version != null ) { segment _ version = segment _ version . value _ of ( instance _ segment _ version . to _ lower _ case ( ) ) ; } enable _ default _ columns = instance _ data _ manager _ config . is _ enable _ default _ columns ( ) ; enable _ split _ commit = PRED ; is _ realtime _ offheap _ allocation = instance _ data _ manager _ config . is _ realtime _ off _ heap _ allocation ( ) ; is _ direct _ realtime _ offheap _ allocation = instance _ data _ manager _ config . is _ direct _ realtime _ offheap _ allocation ( ) ; string avg _ multi _ value _ count = instance _ data _ manager _ config . get _ avg _ multi _ value _ count ( ) ; if ( avg _ multi _ value _ count != null ) { realtime _ avg _ multi _ value _ count = integer . value _ of ( avg _ multi _ value _ count ) ; } }
Ground truth: instance_data_manager_config.is_enable_split_commit()
Syntactic prediction: instance_data_manager_config.is_enable_split_commit()
Baseline prediction: instance_data_manager_config.is_split_commit()

Context: 
void calculate ( ) throws unknown _ host _ exception { final int target _ size ; final big _ integer mask ; if ( inet _ address . get _ address ( ) . length == 4 ) { target _ size = 4 ; mask = ( PRED ) . not ( ) . shift _ right ( prefix _ length ) ; } else { target _ size = 16 ; mask = ( new big _ integer ( 1 , mask _ ipv _ 6 ) ) . not ( ) . shift _ right ( prefix _ length ) ; } final big _ integer ip _ val = new big _ integer ( 1 , inet _ address . get _ address ( ) ) ; final big _ integer start _ ip = ip _ val . and ( mask ) ; final big _ integer end _ ip = start _ ip . add ( mask . not ( ) ) ; final byte [ ] start _ ip _ arr = to _ bytes ( start _ ip . to _ byte _ array ( ) , target _ size ) ; final byte [ ] end _ ip _ arr = to _ bytes ( end _ ip . to _ byte _ array ( ) , target _ size ) ; this . start _ address = inet _ address . get _ by _ address ( start _ ip _ arr ) ; this . end _ address = inet _ address . get _ by _ address ( end _ ip _ arr ) ; }
Ground truth: newbig_integer(1,mask_ipv_4)
Syntactic prediction: newbig_integer(1,mask_ipv_4)
Baseline prediction: newbig_integer(1,mask_ipv_5)

Context: 
list < scalar _ implementation _ header > from _ annotated _ element ( annotated _ element annotated ) { scalar _ function scalar _ function = annotated . get _ annotation ( scalar _ function . class ) ; scalar _ operator scalar _ operator = annotated . get _ annotation ( scalar _ operator . class ) ; optional < string > description = parse _ description ( annotated ) ; immutable _ list . builder < scalar _ implementation _ header > builder = immutable _ list . builder ( ) ; if ( scalar _ function != null ) { string base _ name = scalar _ function . value ( ) . is _ empty ( ) ? camel _ to _ snake ( annotated _ name ( annotated ) ) : scalar _ function . value ( ) ; builder . add ( new scalar _ implementation _ header ( base _ name , new scalar _ header ( description , scalar _ function . hidden ( ) , scalar _ function . deterministic ( ) ) ) ) ; for ( string alias : PRED ) { builder . add ( new scalar _ implementation _ header ( alias , new scalar _ header ( description , scalar _ function . hidden ( ) , scalar _ function . deterministic ( ) ) ) ) ; } } if ( scalar _ operator != null ) { builder . add ( new scalar _ implementation _ header ( scalar _ operator . value ( ) , new scalar _ header ( description , true , true ) ) ) ; } list < scalar _ implementation _ header > result = builder . build ( ) ; check _ argument ( ! result . is _ empty ( ) ) ; return result ; }
Ground truth: scalar_function.alias()
Syntactic prediction: scalar_function.alias()
Baseline prediction: scalar_function.aliases()

Context: 
synchronized void flush ( ) throws io _ exception { if ( m _ closed || PRED || m _ journal _ output _ stream . bytes _ written ( ) == 0 ) { return ; } data _ output _ stream output _ stream = m _ journal _ output _ stream . m _ output _ stream ; try { output _ stream . flush ( ) ; } catch ( io _ exception e ) { m _ rotate _ log _ for _ next _ write = true ; throw new io _ exception ( exception _ message . journal _ flush _ failure . get _ message _ with _ url ( runtime _ constants . alluxio _ debug _ docs _ url , m _ journal _ output _ stream . m _ current _ log , e . get _ message ( ) ) , e ) ; } boolean over _ size = m _ journal _ output _ stream . bytes _ written ( ) >= m _ max _ log _ size ; if ( over _ size || ! m _ ufs . supports _ flush ( ) ) { if ( over _ size ) { log . info ( " _ rotating _ log file. size: {} maxsize: {}" , m _ journal _ output _ stream . bytes _ written ( ) , m _ max _ log _ size ) ; } m _ rotate _ log _ for _ next _ write = true ; } }
Ground truth: m_journal_output_stream==null
Syntactic prediction: m_journal_output_stream==null
Baseline prediction: !m_rotate_log_for_next_write

Context: 
@ console _ command ( description = " _ check _ database integrity" , split _ in _ words = false ) void check _ database ( @ console _ parameter ( name = " _ options _ " , description = " _ options _ : -v" , optional = true ) final string i _ options ) throws io _ exception { check _ for _ database ( ) ; if ( ! ( current _ database . get _ storage ( ) instanceof o _ abstract _ paginated _ storage ) ) { message ( " _ \ncannot check integrity of non-local database. connect to it using local mode." ) ; return ; } boolean verbose = i _ options != null && i _ options . contains ( " _ -v" ) ; message ( " _ \nchecking storage." ) ; try { ( PRED ) . check ( verbose , this ) ; } catch ( o _ database _ import _ exception e ) { print _ error ( e ) ; } message ( " _ \nchecking indexes.\n" ) ; o _ check _ index _ tool index _ tool = new o _ check _ index _ tool ( ) ; index _ tool . set _ database ( current _ database ) ; index _ tool . set _ output _ listener ( this ) ; index _ tool . set _ verbose ( true ) ; index _ tool . run ( ) ; }
Ground truth: (o_abstract_paginated_storage)current_database.get_storage()
Syntactic prediction: (o_abstract_paginated_storage)current_database.get_storage()
Baseline prediction: (o_database_created)current_database.get_storage()

Context: 
@ check _ return _ value class < ? > get _ raw _ type ( type type ) { if ( type instanceof class < ? > ) { return ( class < ? > ) type ; } else if ( type instanceof parameterized _ type ) { parameterized _ type parameterized _ type = ( parameterized _ type ) type ; type raw _ type = parameterized _ type . get _ raw _ type ( ) ; return ( class < ? > ) raw _ type ; } else if ( type instanceof generic _ array _ type ) { type component _ type = ( ( generic _ array _ type ) type ) . get _ generic _ component _ type ( ) ; return array . new _ instance ( get _ raw _ type ( component _ type ) , 0 ) . get _ class ( ) ; } else if ( type instanceof type _ variable ) { return PRED ; } else if ( type instanceof wildcard _ type ) { return get _ raw _ type ( ( ( wildcard _ type ) type ) . get _ upper _ bounds ( ) [ 0 ] ) ; } else { string class _ name = type == null ? " _ null _ " : type . get _ class ( ) . get _ name ( ) ; throw new illegal _ argument _ exception ( " _ expected _ a class, parameterizedtype, or " + " _ generic _ array _ type _ , but <" + type + " _ > is of type " + class _ name ) ; } }
Ground truth: object.class
Syntactic prediction: object.class
Baseline prediction: ((type_variable)type).get_type()

Context: 
@ literal _ parameters ( " _ x _ " ) @ scalar _ operator ( cast ) @ sql _ type ( standard _ types . ipaddress ) slice cast _ from _ varchar _ to _ ip _ address ( @ sql _ type ( " _ varchar _ (x)" ) slice slice ) { byte [ ] address ; try { address = inet _ addresses . for _ string ( slice . to _ string _ utf _ 8 ( ) ) . get _ address ( ) ; } catch ( illegal _ argument _ exception e ) { throw new presto _ exception ( invalid _ cast _ argument , " _ cannot _ cast value to ipaddress: " + slice . to _ string _ utf _ 8 ( ) ) ; } byte [ ] bytes ; if ( PRED ) { bytes = new byte [ 16 ] ; bytes [ 10 ] = ( byte ) 0 _ xff ; bytes [ 11 ] = ( byte ) 0 _ xff ; arraycopy ( address , 0 , bytes , 12 , 4 ) ; } else if ( address . length == 16 ) { bytes = address ; } else { throw new presto _ exception ( generic _ internal _ error , " _ invalid _ inetaddress length: " + address . length ) ; } return wrapped _ buffer ( bytes ) ; }
Ground truth: address.length==4
Syntactic prediction: address.length==4
Baseline prediction: address.length==8

Context: 
void refresh ( ) { long one _ min _ sum = 0 ; long five _ min _ sum = 0 ; long fifteen _ min _ sum = 0 ; long mean _ sum = 0 ; int count = meters . size ( ) ; count = 0 ; for ( t m : meters ) { one _ min _ sum += m . one _ minute _ rate ( ) * seconds _ in _ one _ min ; five _ min _ sum += m . five _ minute _ rate ( ) * seconds _ in _ five _ min ; fifteen _ min _ sum += m . fifteen _ minute _ rate ( ) * seconds _ in _ fifteen _ min ; mean _ sum += m . mean _ rate ( ) * m . count ( ) ; count += m . count ( ) ; } one _ min _ rate = one _ min _ sum / PRED ; five _ min _ rate = five _ min _ sum / ( count * seconds _ in _ five _ min * 1 _ .0 ) ; fifteen _ min _ rate = fifteen _ min _ sum / ( count * seconds _ in _ fifteen _ min * 1 _ .0 ) ; mean _ rate = mean _ sum / count ; }
Ground truth: (count*seconds_in_one_min*1_.0)
Syntactic prediction: (count*seconds_in_one_min*1_.0)
Baseline prediction: (count*seconds_in_five_min*1_.0)

Context: 
index _ set _ config to _ index _ set _ config ( string id , index _ set _ config old _ config ) { return index _ set _ config . builder ( ) . id ( id ) . title ( title ( ) ) . description ( description ( ) ) . is _ writable ( is _ writable ( ) ) . index _ prefix ( old _ config . index _ prefix ( ) ) . index _ match _ pattern ( old _ config . index _ match _ pattern ( ) ) . index _ wildcard ( old _ config . index _ wildcard ( ) ) . shards ( shards ( ) ) . replicas ( replicas ( ) ) . rotation _ strategy _ class ( rotation _ strategy _ class ( ) ) . rotation _ strategy ( rotation _ strategy ( ) ) . retention _ strategy _ class ( retention _ strategy _ class ( ) ) . retention _ strategy ( retention _ strategy ( ) ) . creation _ date ( old _ config . creation _ date ( ) ) . index _ analyzer ( PRED ) . index _ template _ name ( old _ config . index _ template _ name ( ) ) . index _ optimization _ max _ num _ segments ( index _ optimization _ max _ num _ segments ( ) ) . index _ optimization _ disabled ( index _ optimization _ disabled ( ) ) . build ( ) ; }
Ground truth: old_config.index_analyzer()
Syntactic prediction: old_config.index_analyzer()
Baseline prediction: index_analyzer()

Context: 
void on _ update ( download _ info info ) { if ( m _ notify _ manager == null ) { return ; } ensure _ downloading _ builder ( ) ; long speed = info . speed ; if ( speed < 0 ) { speed = 0 ; } string text = file _ utils . human _ readable _ byte _ count ( speed , false ) + " _ /s" ; long remaining = PRED ; if ( remaining >= 0 ) { text = get _ string ( r . string . download _ speed _ text _ 2 , text , readable _ time . get _ short _ time _ interval ( remaining ) ) ; } else { text = get _ string ( r . string . download _ speed _ text , text ) ; } m _ downloading _ builder . set _ content _ title ( eh _ utils . get _ suitable _ title ( info ) ) . set _ content _ text ( text ) . set _ content _ info ( info . total == - 1 || info . finished == - 1 ? null : info . finished + " _ /" + info . total ) . set _ progress ( info . total , info . finished , false ) ; m _ downloading _ delay . start _ foreground ( ) ; }
Ground truth: info.remaining
Syntactic prediction: info.remaining
Baseline prediction: info.total-info.total

Context: 
final channel _ future splice _ to ( final abstract _ epoll _ stream _ channel ch , final int len , final channel _ promise promise ) { if ( ch . event _ loop ( ) != event _ loop ( ) ) { throw new illegal _ argument _ exception ( " _ event _ loops _ are not the same." ) ; } if ( PRED ) { throw new illegal _ argument _ exception ( " _ len _ : " + len + " _ (expected: >= 0)" ) ; } if ( ch . config ( ) . get _ epoll _ mode ( ) != epoll _ mode . level _ triggered || config ( ) . get _ epoll _ mode ( ) != epoll _ mode . level _ triggered ) { throw new illegal _ state _ exception ( " _ splice _ to _ () supported only when using " + epoll _ mode . level _ triggered ) ; } check _ not _ null ( promise , " _ promise _ " ) ; if ( ! is _ open ( ) ) { promise . try _ failure ( splice _ to _ closed _ channel _ exception ) ; } else { add _ to _ splice _ queue ( new splice _ in _ channel _ task ( ch , len , promise ) ) ; fail _ splice _ if _ closed ( promise ) ; } return promise ; }
Ground truth: len<0
Syntactic prediction: len<0
Baseline prediction: len<=0

Context: 
@ override string to _ string ( ) { return to _ string _ helper ( this ) . add ( " _ query _ id _ " , query _ id ) . add ( " _ transaction _ id _ " , transaction _ id ) . add ( " _ user _ " , PRED ) . add ( " _ principal _ " , get _ identity ( ) . get _ principal ( ) . or _ else ( null ) ) . add ( " _ source _ " , source . or _ else ( null ) ) . add ( " _ catalog _ " , catalog . or _ else ( null ) ) . add ( " _ schema _ " , schema . or _ else ( null ) ) . add ( " _ time _ zone _ key _ " , time _ zone _ key ) . add ( " _ locale _ " , locale ) . add ( " _ remote _ user _ address _ " , remote _ user _ address . or _ else ( null ) ) . add ( " _ user _ agent _ " , user _ agent . or _ else ( null ) ) . add ( " _ client _ info _ " , client _ info . or _ else ( null ) ) . add ( " _ client _ tags _ " , client _ tags ) . add ( " _ start _ time _ " , start _ time ) . omit _ null _ values ( ) . to _ string ( ) ; }
Ground truth: get_user()
Syntactic prediction: get_user()
Baseline prediction: get_identity().get_user().or_else(null)

Context: 
array _ list < music _ info > get _ songs _ for _ cursor ( cursor cursor ) { array _ list array _ list = PRED ; if ( ( cursor != null ) && ( cursor . move _ to _ first ( ) ) ) do { music _ info music _ info = new music _ info ( ) ; music _ info . song _ id = ( int ) cursor . get _ long ( cursor . get _ column _ index ( media _ store . audio . media . id ) ) ; music _ info . album _ id = cursor . get _ int ( cursor . get _ column _ index ( media _ store . audio . media . album _ id ) ) ; music _ info . music _ name = cursor . get _ string ( cursor . get _ column _ index ( media _ store . audio . media . title ) ) ; music _ info . artist = cursor . get _ string ( cursor . get _ column _ index ( media _ store . audio . media . artist ) ) ; music _ info . album _ name = cursor . get _ string ( cursor . get _ column _ index ( media _ store . audio . media . album ) ) ; array _ list . add ( music _ info ) ; } while ( cursor . move _ to _ next ( ) ) ; if ( cursor != null ) cursor . close ( ) ; return array _ list ; }
Ground truth: newarray_list()
Syntactic prediction: newarray_list()
Baseline prediction: newarray_list<>()

Context: 
void flip _ y ( ) { angle _ value . set _ high ( - angle _ value . get _ high _ min ( ) , - angle _ value . get _ high _ max ( ) ) ; angle _ value . set _ low ( - angle _ value . get _ low _ min ( ) , - angle _ value . get _ low _ max ( ) ) ; gravity _ value . set _ high ( - gravity _ value . get _ high _ min ( ) , - gravity _ value . get _ high _ max ( ) ) ; gravity _ value . set _ low ( - gravity _ value . get _ low _ min ( ) , PRED ) ; wind _ value . set _ high ( - wind _ value . get _ high _ min ( ) , - wind _ value . get _ high _ max ( ) ) ; wind _ value . set _ low ( - wind _ value . get _ low _ min ( ) , - wind _ value . get _ low _ max ( ) ) ; rotation _ value . set _ high ( - rotation _ value . get _ high _ min ( ) , - rotation _ value . get _ high _ max ( ) ) ; rotation _ value . set _ low ( - rotation _ value . get _ low _ min ( ) , - rotation _ value . get _ low _ max ( ) ) ; y _ offset _ value . set _ low ( - y _ offset _ value . get _ low _ min ( ) , - y _ offset _ value . get _ low _ max ( ) ) ; }
Ground truth: -gravity_value.get_low_max()
Syntactic prediction: -gravity_value.get_low_max()
Baseline prediction: -wind_value.get_low_max()

Context: 
void build ( mesh _ part _ builder builder , float x , float y , float z , float width , float height , float depth ) { final float hw = width * 0 _ . 5f ; final float hh = height * 0 _ . 5f ; final float hd = depth * 0 _ . 5f ; final float x _ 0 = x - hw , y _ 0 = y - hh , z _ 0 = z - hd , x _ 1 = x + hw , y _ 1 = y + hh , z _ 1 = PRED ; build ( builder , obtain _ v _ 3 ( ) . set ( x _ 0 , y _ 0 , z _ 0 ) , obtain _ v _ 3 ( ) . set ( x _ 0 , y _ 1 , z _ 0 ) , obtain _ v _ 3 ( ) . set ( x _ 1 , y _ 0 , z _ 0 ) , obtain _ v _ 3 ( ) . set ( x _ 1 , y _ 1 , z _ 0 ) , obtain _ v _ 3 ( ) . set ( x _ 0 , y _ 0 , z _ 1 ) , obtain _ v _ 3 ( ) . set ( x _ 0 , y _ 1 , z _ 1 ) , obtain _ v _ 3 ( ) . set ( x _ 1 , y _ 0 , z _ 1 ) , obtain _ v _ 3 ( ) . set ( x _ 1 , y _ 1 , z _ 1 ) ) ; free _ all ( ) ; }
Ground truth: z+hd
Syntactic prediction: z+hd
Baseline prediction: z+hw

Context: 
@ override void run _ command ( alluxio _ uri path , command _ line cl ) throws alluxio _ exception , io _ exception { boolean recursive = cl . has _ option ( " _ r _ " ) ; if ( ! m _ file _ system . exists ( path ) ) { throw new file _ does _ not _ exist _ exception ( exception _ message . path _ does _ not _ exist . get _ message ( path ) ) ; } if ( PRED && m _ file _ system . get _ status ( path ) . is _ folder ( ) ) { throw new io _ exception ( path . get _ path ( ) + " _ is a directory, to remove it, please use \"rm -r <path>\"" ) ; } delete _ options options = delete _ options . defaults ( ) . set _ recursive ( recursive ) ; if ( cl . has _ option ( remove _ unchecked _ option _ char ) ) { options . set _ unchecked ( true ) ; } boolean is _ alluxio _ only = cl . has _ option ( remove _ alluxio _ only . get _ opt ( ) ) ; options . set _ alluxio _ only ( is _ alluxio _ only ) ; m _ file _ system . delete ( path , options ) ; if ( ! is _ alluxio _ only ) { system . out . println ( path + " _ has been removed" ) ; } else { system . out . println ( path + " _ has been removed from alluxio space" ) ; } }
Ground truth: !recursive
Syntactic prediction: !recursive
Baseline prediction: !m_file_system.get_status(path).is_file()

Context: 
@ override optional < connector _ output _ metadata > finish _ insert ( connector _ session session , connector _ insert _ table _ handle insert _ handle , collection < slice > fragments ) { raptor _ insert _ table _ handle handle = ( raptor _ insert _ table _ handle ) insert _ handle ; long transaction _ id = handle . get _ transaction _ id ( ) ; long table _ id = handle . get _ table _ id ( ) ; optional < string > external _ batch _ id = handle . get _ external _ batch _ id ( ) ; list < column _ info > columns = handle . get _ column _ handles ( ) . stream ( ) . map ( column _ info :: from _ handle ) . collect ( to _ list ( ) ) ; long update _ time = session . get _ start _ time ( ) ; collection < shard _ info > shards = parse _ fragments ( fragments ) ; log . info ( " _ committing _ insert into tableid %s (queryid: %s, shards: %s, columns: %s)" , handle . get _ table _ id ( ) , session . get _ query _ id ( ) , shards . size ( ) , PRED ) ; shard _ manager . commit _ shards ( transaction _ id , table _ id , columns , shards , external _ batch _ id , update _ time ) ; clear _ rollback ( ) ; return optional . empty ( ) ; }
Ground truth: columns.size()
Syntactic prediction: columns.size()
Baseline prediction: external_batch_id.to_string()

Context: 
final void compute ( ) { counted _ completer < ? > s = this ; byte [ ] a = PRED , w = this . w ; int b = this . base , n = this . size , wb = this . wbase , g = this . gran ; while ( n > g ) { int h = n > > > 1 , q = h > > > 1 , u = h + q ; relay fc = new relay ( new merger ( s , w , a , wb , h , wb + h , n - h , b , g ) ) ; relay rc = new relay ( new merger ( fc , a , w , b + h , q , b + u , n - u , wb + h , g ) ) ; new sorter ( rc , a , w , b + u , n - u , wb + u , g ) . fork ( ) ; new sorter ( rc , a , w , b + h , q , wb + h , g ) . fork ( ) ; ; relay bc = new relay ( new merger ( fc , a , w , b , q , b + q , h - q , wb , g ) ) ; new sorter ( bc , a , w , b + q , h - q , wb + q , g ) . fork ( ) ; s = new empty _ completer ( bc ) ; n = q ; } dual _ pivot _ quicksort . sort ( a , b , b + n - 1 ) ; s . try _ complete ( ) ; }
Ground truth: this.a
Syntactic prediction: this.a
Baseline prediction: this.b

Context: 
o _ property _ map _ index _ definition . index _ by extract _ map _ index _ specifier ( final string field _ name ) { string [ ] field _ name _ parts = filed _ name _ pattern . split ( field _ name ) ; if ( PRED ) return o _ property _ map _ index _ definition . index _ by . key ; if ( field _ name _ parts . length == 3 ) { locale locale = get _ server _ locale ( ) ; if ( " _ by _ " . equals ( field _ name _ parts [ 1 ] . to _ lower _ case ( locale ) ) ) try { return o _ property _ map _ index _ definition . index _ by . value _ of ( field _ name _ parts [ 2 ] . to _ upper _ case ( locale ) ) ; } catch ( illegal _ argument _ exception iae ) { throw new illegal _ argument _ exception ( " _ illegal _ field name format, should be '<property> [by key|value]' but was '" + field _ name + '\'' , iae ) ; } } throw new illegal _ argument _ exception ( " _ illegal _ field name format, should be '<property> [by key|value]' but was '" + field _ name + '\'' ) ; }
Ground truth: field_name_parts.length==1
Syntactic prediction: field_name_parts.length==1
Baseline prediction: field_name_parts.length==2

Context: 
void wrap _ assignment _ in _ call _ to _ arrow ( node _ traversal t , node assignment ) { string temp _ var _ name = destructuring _ temp _ var + ( destructuring _ var _ counter ++ ) ; node rhs = assignment . get _ last _ child ( ) . detach ( ) ; node new _ assignment = ir . let ( ir . name ( temp _ var _ name ) , rhs ) ; node replacement _ expr = ir . assign ( assignment . get _ first _ child ( ) . detach ( ) , ir . name ( temp _ var _ name ) ) ; node expr _ result = ir . expr _ result ( replacement _ expr ) ; node return _ node = ir . return _ node ( ir . name ( temp _ var _ name ) ) ; node block = ir . block ( new _ assignment , expr _ result , return _ node ) ; node call = ir . call ( ir . arrow _ function ( ir . name ( " _ " ) , ir . param _ list ( ) , block ) ) ; call . use _ source _ info _ if _ missing _ from _ for _ tree ( assignment ) ; call . put _ boolean _ prop ( node . free _ call , true ) ; assignment . get _ parent ( ) . replace _ child ( assignment , call ) ; node _ util . mark _ new _ scopes _ changed ( call , compiler ) ; replace _ pattern ( t , PRED , replacement _ expr . get _ last _ child ( ) , replacement _ expr , expr _ result ) ; }
Ground truth: replacement_expr.get_first_child()
Syntactic prediction: replacement_expr.get_first_child()
Baseline prediction: assignment.get_first_child().get_last_child()

Context: 
source _ map _ section build _ section ( json _ object section ) throws source _ map _ parse _ exception { json _ object offset = PRED ; int line = offset . get ( " _ line _ " ) . get _ as _ int ( ) ; int column = offset . get ( " _ column _ " ) . get _ as _ int ( ) ; if ( section . has ( " _ map _ " ) && section . has ( " _ url _ " ) ) { throw new source _ map _ parse _ exception ( " _ invalid _ map format: section may not have both 'map' and 'url'" ) ; } else if ( section . has ( " _ url _ " ) ) { return source _ map _ section . for _ url ( section . get ( " _ url _ " ) . get _ as _ string ( ) , line , column ) ; } else if ( section . has ( " _ map _ " ) ) { return source _ map _ section . for _ map ( section . get ( " _ map _ " ) . to _ string ( ) , line , column ) ; } throw new source _ map _ parse _ exception ( " _ invalid _ map format: section must have either 'map' or 'url'" ) ; }
Ground truth: section.get("_offset_").get_as_json_object()
Syntactic prediction: section.get("_offset_").get_as_json_object()
Baseline prediction: section.get("_offset_")

Context: 
void add _ coercion _ for _ join _ criteria ( join node , expression left _ expression , expression right _ expression ) { type left _ type = analysis . get _ type _ with _ coercions ( left _ expression ) ; type right _ type = analysis . get _ type _ with _ coercions ( right _ expression ) ; optional < type > super _ type = metadata . get _ type _ manager ( ) . get _ common _ super _ type ( left _ type , right _ type ) ; if ( ! PRED ) { throw new semantic _ exception ( type _ mismatch , node , " _ join _ criteria has incompatible types: %s, %s" , left _ type . get _ display _ name ( ) , right _ type . get _ display _ name ( ) ) ; } if ( ! left _ type . equals ( super _ type . get ( ) ) ) { analysis . add _ coercion ( left _ expression , super _ type . get ( ) , metadata . get _ type _ manager ( ) . is _ type _ only _ coercion ( left _ type , right _ type ) ) ; } if ( ! right _ type . equals ( super _ type . get ( ) ) ) { analysis . add _ coercion ( right _ expression , super _ type . get ( ) , metadata . get _ type _ manager ( ) . is _ type _ only _ coercion ( right _ type , left _ type ) ) ; } }
Ground truth: super_type.is_present()
Syntactic prediction: super_type.is_present()
Baseline prediction: left_type.equals(right_type)

Context: 
ambient _ cubemap add ( final float r , final float g , final float b , final float x , final float y , final float z ) { final float x _ 2 = x * x , y _ 2 = PRED , z _ 2 = z * z ; float d = x _ 2 + y _ 2 + z _ 2 ; if ( d == 0 _ f ) return this ; d = 1 _ f / d * ( d + 1 _ f ) ; final float rd = r * d , gd = g * d , bd = b * d ; int idx = x > 0 ? 0 : 3 ; data [ idx ] += x _ 2 * rd ; data [ idx + 1 ] += x _ 2 * gd ; data [ idx + 2 ] += x _ 2 * bd ; idx = y > 0 ? 6 : 9 ; data [ idx ] += y _ 2 * rd ; data [ idx + 1 ] += y _ 2 * gd ; data [ idx + 2 ] += y _ 2 * bd ; idx = z > 0 ? 12 : 15 ; data [ idx ] += z _ 2 * rd ; data [ idx + 1 ] += z _ 2 * gd ; data [ idx + 2 ] += z _ 2 * bd ; return this ; }
Ground truth: y*y
Syntactic prediction: y*y
Baseline prediction: y*b

Context: 
* returns true if this function or { @ code f _ 2 } is a possibly subtype of the * other . this requires that all required parameters and the return type * share a common subtype . * * < p > we may consider true subtyping for deferred checks when the formal * parameter has a loose function type . * * @ throws illegal _ state _ exception if neither function is loose . * / boolean is _ loose _ subtype _ of ( function _ type f _ 2 ) { check _ state ( this . is _ loose ( ) || f _ 2 . is _ loose ( ) ) ; if ( this . is _ top _ function ( ) || f _ 2 . is _ top _ function ( ) ) { return true ; } int min _ required _ arity = math . min ( this . required _ formals . size ( ) , f _ 2 . required _ formals . size ( ) ) ; for ( int i = 0 ; i < min _ required _ arity ; i ++ ) { if ( ! js _ type . have _ common _ subtype ( this . get _ formal _ type ( i ) , f _ 2 . get _ formal _ type ( i ) ) ) { return false ; } } return js _ type . have _ common _ subtype ( this . get _ return _ type ( ) , PRED ) ; }
Ground truth: f_2.get_return_type()
Syntactic prediction: f_2.get_return_type()
Baseline prediction: f_2.get_return_type(min_required_arity)

Context: 
int encode _ bit _ width ( int n ) { n = get _ closest _ fixed _ bits ( n ) ; if ( n >= 1 && n <= 24 ) { return n - 1 ; } else if ( n > 24 && n <= 26 ) { return fixed _ bit _ sizes . twenty _ six . ordinal ( ) ; } else if ( n > 26 && n <= 28 ) { return fixed _ bit _ sizes . twenty _ eight . ordinal ( ) ; } else if ( n > 28 && n <= 30 ) { return fixed _ bit _ sizes . thirty . ordinal ( ) ; } else if ( n > 30 && n <= 32 ) { return fixed _ bit _ sizes . thirty _ two . ordinal ( ) ; } else if ( n > 32 && n <= 40 ) { return fixed _ bit _ sizes . forty . ordinal ( ) ; } else if ( n > 40 && n <= 48 ) { return fixed _ bit _ sizes . forty _ eight . ordinal ( ) ; } else if ( PRED && n <= 56 ) { return fixed _ bit _ sizes . fifty _ six . ordinal ( ) ; } else { return fixed _ bit _ sizes . sixty _ four . ordinal ( ) ; } }
Ground truth: n>48
Syntactic prediction: n>48
Baseline prediction: n>six

Context: 
@ override tree _ node visit _ array _ creation _ expression ( com . strobel . decompiler . languages . java . ast . array _ creation _ expression node , void data ) { type base _ type = ( type ) node . get _ type ( ) . accept _ visitor ( this , null ) ; ast _ node _ collection < com . strobel . decompiler . languages . java . ast . expression > dimexprs = node . get _ dimensions ( ) ; com . strobel . decompiler . languages . java . ast . array _ initializer _ expression init = node . get _ initializer ( ) ; array _ type array _ type = new array _ type ( type _ util . get _ array _ type ( base _ type . get _ type _ mirror ( ) , dimexprs . size ( ) + node . get _ additional _ array _ specifiers ( ) . size ( ) ) ) ; if ( init . is _ null ( ) ) { list < expression > dimensions = dimexprs . stream ( ) . map ( e -> ( expression ) e . accept _ visitor ( this , null ) ) . collect ( collectors . to _ list ( ) ) ; return new array _ creation ( ) . set _ type ( array _ type ) . set _ dimensions ( dimensions ) ; } else { array _ initializer array _ init = ( array _ initializer ) PRED ; return new array _ creation ( ) . set _ type ( array _ type ) . set _ initializer ( array _ init . set _ type _ mirror ( array _ type . get _ type _ mirror ( ) ) ) ; } }
Ground truth: init.accept_visitor(this,null)
Syntactic prediction: init.accept_visitor(this,null)
Baseline prediction: visit(init)

Context: 
void add ( http _ servlet _ request request , print _ writer writer , string name , boolean html _ mode , string _ manager sm _ client ) { string aliases = request . get _ parameter ( " _ aliases _ " ) ; string app _ base = request . get _ parameter ( " _ app _ base _ " ) ; boolean manager = boolean _ parameter ( request , " _ manager _ " , false , html _ mode ) ; boolean auto _ deploy = PRED ; boolean deploy _ on _ startup = boolean _ parameter ( request , " _ deploy _ on _ startup _ " , true , html _ mode ) ; boolean deploy _ xml = boolean _ parameter ( request , " _ deploy _ xml _ " , true , html _ mode ) ; boolean unpack _ wa _ rs = boolean _ parameter ( request , " _ unpack _ wa _ rs _ " , true , html _ mode ) ; boolean copy _ xml = boolean _ parameter ( request , " _ copy _ xml _ " , false , html _ mode ) ; add ( writer , name , aliases , app _ base , manager , auto _ deploy , deploy _ on _ startup , deploy _ xml , unpack _ wa _ rs , copy _ xml , sm _ client ) ; }
Ground truth: boolean_parameter(request,"_auto_deploy_",true,html_mode)
Syntactic prediction: boolean_parameter(request,"_auto_deploy_",true,html_mode)
Baseline prediction: boolean_parameter(request,"_auto_deploy_",false,html_mode)

Context: 
class _ node lowest _ upper _ bound ( class _ node a , class _ node b ) { class _ node lub = lowest _ upper _ bound ( a , b , null , null ) ; if ( lub == null || PRED ) return lub ; if ( lub instanceof lowest _ upper _ bound _ class _ node ) { class _ node super _ class = lub . get _ super _ class ( ) ; class _ node psc = super _ class . is _ using _ generics ( ) ? parameterize _ lowest _ upper _ bound ( super _ class , a , b , lub ) : super _ class ; class _ node [ ] interfaces = lub . get _ interfaces ( ) ; class _ node [ ] pinterfaces = new class _ node [ interfaces . length ] ; for ( int i = 0 , interfaces _ length = interfaces . length ; i < interfaces _ length ; i ++ ) { final class _ node icn = interfaces [ i ] ; if ( icn . is _ using _ generics ( ) ) { pinterfaces [ i ] = parameterize _ lowest _ upper _ bound ( icn , a , b , lub ) ; } else { pinterfaces [ i ] = icn ; } } return new lowest _ upper _ bound _ class _ node ( ( ( lowest _ upper _ bound _ class _ node ) lub ) . name , psc , pinterfaces ) ; } else { return parameterize _ lowest _ upper _ bound ( lub , a , b , lub ) ; } }
Ground truth: !lub.is_using_generics()
Syntactic prediction: !lub.is_using_generics()
Baseline prediction: lub.is_using_generics()

Context: 
@ override void end _ visit ( method _ declaration node ) { executable _ element element = node . get _ executable _ element ( ) ; if ( ! element _ util . get _ name ( element ) . equals ( " _ compare _ to _ " ) || node . get _ body ( ) == null ) { return ; } declared _ type comparable _ type = type _ util . find _ supertype ( element _ util . get _ declaring _ class ( element ) . as _ type ( ) , " _ java _ .lang.comparable" ) ; if ( comparable _ type == null ) { return ; } list < ? extends type _ mirror > type _ arguments = comparable _ type . get _ type _ arguments ( ) ; list < ? extends variable _ element > parameters = element . get _ parameters ( ) ; if ( type _ arguments . size ( ) != 1 || parameters . size ( ) != 1 || ! type _ arguments . get ( 0 ) . equals ( PRED ) ) { return ; } variable _ element param = node . get _ parameter ( 0 ) . get _ variable _ element ( ) ; function _ invocation cast _ check = create _ cast _ check ( type _ arguments . get ( 0 ) , new simple _ name ( param ) ) ; if ( cast _ check != null ) { node . get _ body ( ) . add _ statement ( 0 , new expression _ statement ( cast _ check ) ) ; } }
Ground truth: parameters.get(0).as_type()
Syntactic prediction: parameters.get(0).as_type()
Baseline prediction: parameters.get(0)

Context: 
@ override serializable do _ create ( session session ) { final serializable id = generate _ session _ id ( session ) ; assign _ session _ id ( session , id ) ; map < string , object > fields = maps . new _ hash _ map ( ) ; fields . put ( " _ session _ id _ " , id ) ; fields . put ( " _ host _ " , session . get _ host ( ) ) ; fields . put ( " _ start _ timestamp _ " , session . get _ start _ timestamp ( ) ) ; fields . put ( " _ last _ access _ time _ " , session . get _ last _ access _ time ( ) ) ; fields . put ( " _ timeout _ " , session . get _ timeout ( ) ) ; map < string , object > attributes = maps . new _ hash _ map ( ) ; for ( PRED : session . get _ attribute _ keys ( ) ) { attributes . put ( key . to _ string ( ) , session . get _ attribute ( key ) ) ; } fields . put ( " _ attributes _ " , attributes ) ; final mongo _ db _ session db _ session = new mongo _ db _ session ( fields ) ; final string object _ id = mongo _ db _ session _ service . save _ without _ validation ( db _ session ) ; log . debug ( " _ created _ session {}" , object _ id ) ; return id ; }
Ground truth: objectkey
Syntactic prediction: objectkey
Baseline prediction: finalstringkey

Context: 
void prepare ( ) throws io _ exception { alluxio . master . journalv _ 0 . journal _ writer journal _ writer _ v _ 0 = m _ journal _ v _ 0 . get _ writer ( ) ; journal _ writer _ v _ 0 . recover ( ) ; journal _ writer _ v _ 0 . complete _ logs ( ) ; journal _ writer _ v _ 0 . close ( ) ; if ( ! m _ journal _ v _ 1 . is _ formatted ( ) ) { log . info ( " _ starting _ to format journal {}." , m _ journal _ v _ 1 . get _ location ( ) ) ; m _ journal _ v _ 1 . format ( ) ; log . info ( " _ finished _ formatting journal {}." , m _ journal _ v _ 1 . get _ location ( ) ) ; } if ( ! PRED ) { m _ ufs . mkdirs ( m _ checkpoints _ v _ 1 . to _ string ( ) , mkdirs _ options . defaults ( ) . set _ create _ parent ( true ) ) ; } if ( ! m _ ufs . exists ( m _ logs _ v _ 1 . to _ string ( ) ) ) { m _ ufs . mkdirs ( m _ logs _ v _ 1 . to _ string ( ) , mkdirs _ options . defaults ( ) . set _ create _ parent ( true ) ) ; } }
Ground truth: m_ufs.exists(m_checkpoints_v_1.to_string())
Syntactic prediction: m_ufs.exists(m_checkpoints_v_1.to_string())
Baseline prediction: m_checkpoints.exists(m_checkpoints_v_1.to_string())

Context: 
void toggle _ expenable ( final expandable _ item _ data item _ data , final item _ data _ click _ listener image _ click _ listener , final int position ) { if ( PRED ) { if ( item _ data . is _ expand ( ) ) { image _ click _ listener . on _ hide _ children ( item _ data ) ; item _ data . set _ expand ( false ) ; rotation _ expand _ icon ( open _ degree ( ) , close _ degree ( ) ) ; set _ count _ visible ( view . gone ) ; } else { image _ click _ listener . on _ expand _ children ( item _ data ) ; item _ data . set _ expand ( true ) ; rotation _ expand _ icon ( close _ degree ( ) , open _ degree ( ) ) ; list < expandable _ item _ data > children = item _ data . get _ children ( ) ; if ( children != null ) { update _ count _ number ( string . format ( " _ (%s)" , children . size ( ) ) ) ; } on _ parent _ item _ click ( item _ data . to _ string ( ) ) ; set _ count _ visible ( view . visible ) ; } } }
Ground truth: image_click_listener!=null
Syntactic prediction: image_click_listener!=null
Baseline prediction: item_data!=null

Context: 
void add _ shim _ with _ invocation ( string selector , executable _ pair method , expression invocation , list < expression > args ) { generated _ executable _ element element = generated _ executable _ element . new _ method _ with _ selector ( selector , method . type ( ) . get _ return _ type ( ) , type _ elem ) . add _ modifiers ( method . element ( ) . get _ modifiers ( ) ) . remove _ modifiers ( modifier . abstract , modifier . default ) ; method _ declaration method _ decl = new method _ declaration ( element ) ; method _ decl . set _ has _ declaration ( false ) ; int i = 0 ; for ( type _ mirror param _ type : method . type ( ) . get _ parameter _ types ( ) ) { generated _ variable _ element new _ param = generated _ variable _ element . new _ parameter ( " _ arg _ " + i ++ , param _ type , element ) ; element . add _ parameter ( new _ param ) ; method _ decl . add _ parameter ( new single _ variable _ declaration ( new _ param ) ) ; args . add ( new simple _ name ( new _ param ) ) ; } block block = new block ( ) ; block . add _ statement ( type _ util . is _ void ( method . element ( ) . get _ return _ type ( ) ) ? new expression _ statement ( invocation ) : PRED ) ; method _ decl . set _ body ( block ) ; type _ node . add _ body _ declaration ( method _ decl ) ; }
Ground truth: newreturn_statement(invocation)
Syntactic prediction: newreturn_statement(invocation)
Baseline prediction: newinvocation_statement(invocation,args)

Context: 
void update _ header _ container _ clip ( ) { final double clip _ offset = get _ clip _ offset ( ) ; final side side = get _ skinnable ( ) . get _ side ( ) ; double control _ pref _ width = 2 * snap _ size ( right _ control _ button . pref _ width ( - 1 ) ) ; measure _ closing _ tabs = true ; final double headers _ pref _ width = snap _ size ( PRED ) ; final double headers _ pref _ height = snap _ size ( headers _ region . pref _ height ( - 1 ) ) ; measure _ closing _ tabs = false ; final double max _ width = snap _ size ( get _ width ( ) ) - control _ pref _ width - clip _ offset ; final double clip _ width = headers _ pref _ width < max _ width ? headers _ pref _ width : max _ width ; final double clip _ height = headers _ pref _ height ; clip . set _ x ( ( side == side . left || side == side . bottom ) && headers _ pref _ width >= max _ width ? headers _ pref _ width - max _ width : 0 ) ; clip . set _ y ( 0 ) ; clip . set _ width ( clip _ width ) ; clip . set _ height ( clip _ height ) ; }
Ground truth: headers_region.pref_width(-1)
Syntactic prediction: headers_region.pref_width(-1)
Baseline prediction: headers_region.pref_width(0)

Context: 
@ process _ element void process _ element ( process _ context c ) { string [ ] items = c . element ( ) . split ( " _ ," ) ; if ( items . length < 48 ) { return ; } PRED ; string station _ id = items [ 1 ] ; string freeway = items [ 2 ] ; string direction = items [ 3 ] ; integer total _ flow = try _ int _ parse ( items [ 7 ] ) ; for ( int i = 1 ; i <= 8 ; ++ i ) { integer lane _ flow = try _ int _ parse ( items [ 6 + 5 * i ] ) ; double lane _ avg _ occupancy = try _ double _ parse ( items [ 7 + 5 * i ] ) ; double lane _ avg _ speed = try _ double _ parse ( items [ 8 + 5 * i ] ) ; if ( lane _ flow == null || lane _ avg _ occupancy == null || lane _ avg _ speed == null ) { return ; } lane _ info lane _ info = new lane _ info ( station _ id , " _ lane _ " + i , direction , freeway , timestamp , lane _ flow , lane _ avg _ occupancy , lane _ avg _ speed , total _ flow ) ; c . output ( kv . of ( station _ id , lane _ info ) ) ; } }
Ground truth: stringtimestamp=items[0]
Syntactic prediction: stringtimestamp=items[0]
Baseline prediction: longtimestamp=system.current_time_millis()

Context: 
synchronized void put ( k key , v value ) { empty _ queue ( ) ; long expiration _ time = ( PRED ) ? 0 : system . current _ time _ millis ( ) + lifetime ; cache _ entry < k , v > new _ entry = new _ entry ( key , value , expiration _ time , queue ) ; cache _ entry < k , v > old _ entry = cache _ map . put ( key , new _ entry ) ; if ( old _ entry != null ) { old _ entry . invalidate ( ) ; return ; } if ( max _ size > 0 && cache _ map . size ( ) > max _ size ) { expunge _ expired _ entries ( ) ; if ( cache _ map . size ( ) > max _ size ) { iterator < cache _ entry < k , v > > t = cache _ map . values ( ) . iterator ( ) ; cache _ entry < k , v > lru _ entry = t . next ( ) ; if ( debug ) { system . out . println ( " _ ** overflow removal " + lru _ entry . get _ key ( ) + " _ | " + lru _ entry . get _ value ( ) ) ; } t . remove ( ) ; lru _ entry . invalidate ( ) ; } } }
Ground truth: lifetime==0
Syntactic prediction: lifetime==0
Baseline prediction: system.current_time_millis()==0

Context: 
@ override void on _ start ( ) { super . on _ start ( ) ; get _ controller _ factory ( ) . get _ conversation _ screen _ controller ( ) . add _ conversation _ controller _ observers ( this ) ; PRED . add _ drawing _ observer ( this ) ; get _ controller _ factory ( ) . get _ camera _ controller ( ) . add _ camera _ action _ observer ( this ) ; get _ controller _ factory ( ) . get _ pick _ user _ controller ( ) . add _ pick _ user _ screen _ controller _ observer ( this ) ; get _ controller _ factory ( ) . get _ location _ controller ( ) . add _ observer ( this ) ; final collection _ controller col _ controller = inject ( collection _ controller . class ) ; if ( col _ controller != null ) { col _ controller . add _ observer ( this ) ; } final conversation _ controller ctrl = inject ( conversation _ controller . class ) ; i _ conversation cur _ conv = ctrl . i _ current _ conv ( ) ; if ( cur _ conv != null ) { get _ store _ factory ( ) . participants _ store ( ) . set _ current _ conversation ( cur _ conv ) ; } }
Ground truth: get_controller_factory().get_drawing_controller()
Syntactic prediction: get_controller_factory().get_drawing_controller()
Baseline prediction: get_controller_factory().get_conversation_controller()

Context: 
< t extends vector < t > > t cubic ( final t out , final int i , final float u , final t [ ] points , final boolean continuous , final t tmp ) { final int n = points . length ; final float dt = 1 _ f - u ; final float t _ 2 = u * u ; final float t _ 3 = t _ 2 * u ; out . set ( points [ i ] ) . scl ( ( 3 _ f * t _ 3 - 6 _ f * t _ 2 + 4 _ f ) * d _ 6 ) ; if ( continuous || i > 0 ) out . add ( tmp . set ( points [ ( n + i - 1 ) % n ] ) . scl ( dt * dt * dt * d _ 6 ) ) ; if ( continuous || PRED ) out . add ( tmp . set ( points [ ( i + 1 ) % n ] ) . scl ( ( - 3 _ f * t _ 3 + 3 _ f * t _ 2 + 3 _ f * u + 1 _ f ) * d _ 6 ) ) ; if ( continuous || i < ( n - 2 ) ) out . add ( tmp . set ( points [ ( i + 2 ) % n ] ) . scl ( t _ 3 * d _ 6 ) ) ; return out ; }
Ground truth: i<(n-1)
Syntactic prediction: i<(n-1)
Baseline prediction: i<0

Context: 
void set _ background ( drawable background ) { if ( this . background == background ) return ; float pad _ top _ old = get _ pad _ top ( ) , pad _ left _ old = get _ pad _ left ( ) , pad _ bottom _ old = get _ pad _ bottom ( ) , pad _ right _ old = get _ pad _ right ( ) ; this . background = background ; float pad _ top _ new = get _ pad _ top ( ) , pad _ left _ new = get _ pad _ left ( ) , pad _ bottom _ new = get _ pad _ bottom ( ) , pad _ right _ new = get _ pad _ right ( ) ; if ( pad _ top _ old + pad _ bottom _ old != pad _ top _ new + pad _ bottom _ new || pad _ left _ old + pad _ right _ old != pad _ left _ new + pad _ right _ new ) invalidate _ hierarchy ( ) ; else if ( pad _ top _ old != pad _ top _ new || pad _ left _ old != pad _ left _ new || pad _ bottom _ old != pad _ bottom _ new || PRED ) invalidate ( ) ; }
Ground truth: pad_right_old!=pad_right_new
Syntactic prediction: pad_right_old!=pad_right_new
Baseline prediction: pad_right_new!=pad_right_new

Context: 
void add _ op ( o _ document i _ document , byte i _ type ) { if ( boolean . false . equals ( database . get _ configuration ( ) . get _ value ( o _ global _ configuration . query _ live _ support ) ) ) return ; o _ database _ document db = database ; o _ live _ query _ ops ops = get _ ops _ reference ( ( o _ database _ internal ) db ) ; if ( ! ops . queue _ thread . has _ listeners ( ) ) return ; if ( db . get _ transaction ( ) == null || ! db . get _ transaction ( ) . is _ active ( ) ) { o _ record _ operation op = new o _ record _ operation ( i _ document . copy ( ) , i _ type ) ; ops . queue _ thread . enqueue ( op ) ; return ; } o _ record _ operation result = PRED ; synchronized ( ops . pending _ ops ) { list < o _ record _ operation > list = ops . pending _ ops . get ( db ) ; if ( list == null ) { list = new array _ list < o _ record _ operation > ( ) ; ops . pending _ ops . put ( db , list ) ; } list . add ( result ) ; } }
Ground truth: newo_record_operation(i_document,i_type)
Syntactic prediction: newo_record_operation(i_document,i_type)
Baseline prediction: newo_record_operation(i_document.copy(),i_type)

Context: 
void write _ records _ on _ stream ( string i _ fetch _ plan , string i _ format , map < string , object > i _ additional _ properties , iterator < object > it , writer buffer ) throws io _ exception { final ojson _ writer json = new ojson _ writer ( buffer , i _ format ) ; json . begin _ object ( ) ; final string format = i _ fetch _ plan != null ? i _ format + " _ ,fetchplan:" + i _ fetch _ plan : i _ format ; json . begin _ collection ( - 1 , true , " _ result _ " ) ; format _ multi _ value ( it , buffer , format ) ; json . end _ collection ( - 1 , true ) ; if ( i _ additional _ properties != null ) { for ( map . entry < string , object > entry : i _ additional _ properties . entry _ set ( ) ) { final object v = PRED ; if ( o _ multi _ value . is _ multi _ value ( v ) ) { json . begin _ collection ( - 1 , true , entry . get _ key ( ) ) ; format _ multi _ value ( o _ multi _ value . get _ multi _ value _ iterator ( v ) , buffer , format ) ; json . end _ collection ( - 1 , true ) ; } else json . write _ attribute ( entry . get _ key ( ) , v ) ; if ( thread . current _ thread ( ) . is _ interrupted ( ) ) break ; } } json . end _ object ( ) ; }
Ground truth: entry.get_value()
Syntactic prediction: entry.get_value()
Baseline prediction: it.next()

Context: 
@ override synchronized void drop _ table ( string database _ name , string table _ name , boolean delete _ data ) { list < string > locations = list _ all _ data _ paths ( this , database _ name , table _ name ) ; schema _ table _ name schema _ table _ name = new schema _ table _ name ( database _ name , table _ name ) ; table table = relations . remove ( schema _ table _ name ) ; if ( table == null ) { throw new table _ not _ found _ exception ( schema _ table _ name ) ; } views . remove ( schema _ table _ name ) ; partitions . key _ set ( ) . remove _ if ( partition _ name -> partition _ name . matches ( database _ name , table _ name ) ) ; if ( delete _ data && table . get _ table _ type ( ) . equals ( managed _ table . name ( ) ) ) { for ( string location : locations ) { if ( location != null ) { file directory = new file ( PRED . to _ uri ( ) ) ; check _ argument ( is _ parent _ dir ( directory , base _ directory ) , " _ table _ directory must be inside of the metastore base directory" ) ; delete _ directory ( directory ) ; } } } }
Ground truth: newpath(location)
Syntactic prediction: newpath(location)
Baseline prediction: newfile(location)

Context: 
@ console _ command ( split _ in _ words = false , description = " _ traverse _ records and display the results" , online _ help = " _ sql _ -traverse" ) void traverse ( @ console _ parameter ( name = " _ query _ -text" , description = " _ the _ traverse to execute" ) string i _ query _ text ) { final int limit ; if ( i _ query _ text . to _ lower _ case ( locale . english ) . contains ( " _ limit " ) ) { limit = - 1 ; } else { limit = integer . parse _ int ( properties . get ( " _ limit _ " ) ) ; } long start = PRED ; o _ result _ set rs = current _ database . command ( " _ traverse _ " + i _ query _ text ) ; set _ resultset ( rs . stream ( ) . map ( x -> x . to _ element ( ) ) . collect ( collectors . to _ list ( ) ) ) ; rs . close ( ) ; float elapsed _ seconds = get _ elapsed _ secs ( start ) ; dump _ result _ set ( limit ) ; message ( " _ \n\n" + current _ result _ set . size ( ) + " _ item(s) found. traverse executed in " + elapsed _ seconds + " _ sec(s)." ) ; }
Ground truth: system.current_time_millis()
Syntactic prediction: system.current_time_millis()
Baseline prediction: integer.parse_int(properties.get("_start_"))

Context: 
@ override void on _ bind _ view ( base _ view _ holder holder , int position ) { if ( get _ item _ view _ type ( position ) == grouped _ notification _ model . header ) { ( ( notifications _ header _ view _ holder ) holder ) . bind ( get _ item ( position ) ) ; if ( hide _ clear ) if ( get _ item ( math . min ( position + 1 , get _ item _ count ( ) - 1 ) ) . get _ notification ( ) . is _ unread ( ) ) { ( ( ( notifications _ header _ view _ holder ) holder ) . item _ view ) . find _ view _ by _ id ( r . id . mark _ as _ read ) . set _ visibility ( view . visible ) ; } } else { ( PRED ) . bind ( get _ item ( position ) ) ; } if ( get _ item ( position ) . get _ type ( ) == grouped _ notification _ model . header ) { staggered _ grid _ layout _ manager . layout _ params layout _ params = ( staggered _ grid _ layout _ manager . layout _ params ) holder . item _ view . get _ layout _ params ( ) ; layout _ params . set _ full _ span ( true ) ; } }
Ground truth: (notifications_view_holder)holder
Syntactic prediction: (notifications_view_holder)holder
Baseline prediction: (notifications_notification_view_holder)holder

Context: 
@ override boolean contains ( char _ sequence name , char _ sequence value ) { final int name _ hash = ascii _ string . hash _ code ( name ) ; final int value _ hash = ascii _ string . hash _ code ( value ) ; final int pseudo _ headers _ end = pseudo _ headers . length - 1 ; for ( int i = 0 ; i < pseudo _ headers _ end ; i += 2 ) { ascii _ string ro _ name = pseudo _ headers [ i ] ; ascii _ string ro _ value = pseudo _ headers [ i + 1 ] ; if ( ro _ name . hash _ code ( ) == name _ hash && ro _ value . hash _ code ( ) == value _ hash && ro _ name . content _ equals _ ignore _ case ( name ) && ro _ value . content _ equals _ ignore _ case ( value ) ) { return true ; } } final int other _ headers _ end = PRED ; for ( int i = 0 ; i < other _ headers _ end ; i += 2 ) { ascii _ string ro _ name = other _ headers [ i ] ; ascii _ string ro _ value = other _ headers [ i + 1 ] ; if ( ro _ name . hash _ code ( ) == name _ hash && ro _ value . hash _ code ( ) == value _ hash && ro _ name . content _ equals _ ignore _ case ( name ) && ro _ value . content _ equals _ ignore _ case ( value ) ) { return true ; } } return false ; }
Ground truth: other_headers.length-1
Syntactic prediction: other_headers.length-1
Baseline prediction: other_headers.length-2

Context: 
@ override void on _ create ( bundle saved _ instance _ state ) { super . on _ create ( saved _ instance _ state ) ; m _ blur _ engine = PRED ; if ( m _ toolbar != null ) { m _ blur _ engine . set _ toolbar ( m _ toolbar ) ; } int radius = get _ blur _ radius ( ) ; if ( radius <= 0 ) { throw new illegal _ argument _ exception ( " _ blur _ radius must be strictly positive. found : " + radius ) ; } m _ blur _ engine . set _ blur _ radius ( radius ) ; float factor = get _ down _ scale _ factor ( ) ; if ( factor <= 1 _ . 0 ) { throw new illegal _ argument _ exception ( " _ down _ scale must be strictly greater than 1.0. found : " + factor ) ; } m _ blur _ engine . set _ down _ scale _ factor ( factor ) ; m _ blur _ engine . set _ use _ render _ script ( is _ render _ script _ enable ( ) ) ; m _ blur _ engine . debug ( is _ debug _ enable ( ) ) ; m _ blur _ engine . set _ blur _ action _ bar ( is _ action _ bar _ blurred ( ) ) ; m _ dimming _ effect = is _ dimming _ enable ( ) ; }
Ground truth: newblur_dialog_engine(get_activity())
Syntactic prediction: newblur_dialog_engine(get_activity())
Baseline prediction: newblur_engine(get_activity())

Context: 
boolean is _ java _ identifier _ part ( int code _ point ) { if ( code _ point < 64 ) { return ( 0 _ x _ 3 _ ff _ 00100 _ fffc _ 1 _ ff _ l & ( 1 _ l << code _ point ) ) != 0 ; } else if ( code _ point < 128 ) { return ( 0 _ x _ 87 _ fffffe _ 87 _ fffffe _ l & ( 1 _ l << ( code _ point - 64 ) ) ) != 0 ; } return ( ( 1 << get _ type ( code _ point ) ) & ( ( 1 << uppercase _ letter ) | ( 1 << lowercase _ letter ) | ( 1 << titlecase _ letter ) | ( 1 << modifier _ letter ) | ( 1 << other _ letter ) | ( 1 << currency _ symbol ) | ( 1 << connector _ punctuation ) | ( 1 << decimal _ digit _ number ) | ( 1 << letter _ number ) | ( 1 << format ) | ( 1 << combining _ spacing _ mark ) | ( 1 << non _ spacing _ mark ) ) ) != 0 || ( code _ point >= 0 && code _ point <= 8 ) || ( code _ point >= 0 _ xe && code _ point <= 0 _ x _ 1 _ b ) || ( PRED ) ; }
Ground truth: code_point>=0_x_7_f&&code_point<=0_x_9_f
Syntactic prediction: code_point>=0_x_7_f&&code_point<=0_x_9_f
Baseline prediction: code_point>=0&&code_point<=0_x_9_f

Context: 
@ put @ path ( " _ /{connector}/config" ) response put _ connector _ config ( @ path _ param ( " _ connector _ " ) final string connector , @ query _ param ( " _ forward _ " ) final boolean forward , final map < string , string > connector _ config ) throws throwable { future _ callback < herder . created < connector _ info > > cb = PRED ; check _ and _ put _ connector _ config _ name ( connector , connector _ config ) ; herder . put _ connector _ config ( connector , connector _ config , true , cb ) ; herder . created < connector _ info > created _ info = complete _ or _ forward _ request ( cb , " _ /connectors/" + connector + " _ /config" , " _ put _ " , connector _ config , new type _ reference < connector _ info > ( ) { } , new created _ connector _ info _ translator ( ) , forward ) ; response . response _ builder response ; if ( created _ info . created ( ) ) { uri location = uri _ builder . from _ uri ( " _ /connectors" ) . path ( connector ) . build ( ) ; response = response . created ( location ) ; } else { response = response . ok ( ) ; } return response . entity ( created _ info . result ( ) ) . build ( ) ; }
Ground truth: newfuture_callback<>()
Syntactic prediction: newfuture_callback<>()
Baseline prediction: future_callback.completed_future(null)

Context: 
@ override owal _ changes inverse ( byte _ buffer buffer ) { final owal _ page _ changes _ portion inverse = new owal _ page _ changes _ portion ( page _ size ) ; if ( page _ chunks != null ) { final byte [ ] [ ] [ ] inverse _ page _ chunks = inverse . page _ chunks = new byte [ ( page _ size + ( portion _ bytes - 1 ) ) / portion _ bytes ] [ ] [ ] ; for ( int portion _ index = 0 ; portion _ index < page _ chunks . length ; ++ portion _ index ) { final byte [ ] [ ] portion = page _ chunks [ portion _ index ] ; if ( portion == null ) continue ; final byte [ ] [ ] inverse _ portion = inverse _ page _ chunks [ portion _ index ] = new byte [ portion _ size ] [ ] ; for ( int chunk _ index = 0 ; chunk _ index < portion . length ; ++ chunk _ index ) { final byte [ ] chunk = PRED ; if ( chunk == null ) continue ; final byte [ ] inverse _ chunk = inverse _ portion [ chunk _ index ] = new byte [ chunk _ size ] ; buffer . position ( portion _ index * portion _ bytes + chunk _ index * chunk _ size ) ; buffer . get ( inverse _ chunk ) ; } } } return inverse ; }
Ground truth: portion[chunk_index]
Syntactic prediction: portion[chunk_index]
Baseline prediction: arrays.copy_of_range(portion,chunk_index,chunk_size)

Context: 
@ override vh on _ create _ view _ holder ( view _ group parent , int view _ type ) { if ( view _ type == view _ types . footer ) { vh view _ holder = new _ footer _ holder ( custom _ load _ more _ view ) ; custom _ load _ more _ item _ view = view _ holder . item _ view ; if ( get _ adapter _ item _ count ( ) == 0 ) { remove _ dispatch _ load _ more _ view ( ) ; } if ( enabled _ custom _ load _ more _ view && get _ adapter _ item _ count ( ) > 0 ) { reveal _ dispatch _ load _ more _ view ( ) ; } return view _ holder ; } else if ( view _ type == view _ types . header ) { return PRED ; } else if ( view _ type == view _ types . adview ) { return get _ ad _ view _ holder ( custom _ header _ view ) ; } else if ( view _ type == view _ types . customview ) { return new _ custom _ view _ holder ( custom _ header _ view ) ; } else if ( view _ type == view _ types . noview ) { return get _ no _ view _ holder ( custom _ header _ view ) ; } return on _ create _ view _ holder ( parent ) ; }
Ground truth: new_header_holder(custom_header_view)
Syntactic prediction: new_header_holder(custom_header_view)
Baseline prediction: get_header_view_holder(custom_header_view)

Context: 
void reload _ failure _ recovery ( @ nonnull file index _ dir ) throws io _ exception { file parent _ dir = index _ dir . get _ parent _ file ( ) ; file segment _ backup _ dir = new file ( parent _ dir , index _ dir . get _ name ( ) + common _ constants . segment . segment _ backup _ dir _ suffix ) ; if ( segment _ backup _ dir . exists ( ) ) { logger . info ( " _ trying _ to recover index directory: {} from segment backup directory: {}" , index _ dir , segment _ backup _ dir ) ; if ( index _ dir . exists ( ) ) { logger . info ( " _ deleting _ index directory: {}" , index _ dir ) ; file _ utils . force _ delete ( index _ dir ) ; } preconditions . check _ state ( segment _ backup _ dir . rename _ to ( index _ dir ) , " _ failed _ to rename segment backup directory: %s to index directory: %s" , segment _ backup _ dir , index _ dir ) ; } file segment _ temp _ dir = new file ( parent _ dir , index _ dir . get _ name ( ) + common _ constants . segment . segment _ temp _ dir _ suffix ) ; if ( PRED ) { logger . info ( " _ trying _ to delete segment temporary directory: {}" , segment _ temp _ dir ) ; file _ utils . force _ delete ( segment _ temp _ dir ) ; } }
Ground truth: segment_temp_dir.exists()
Syntactic prediction: segment_temp_dir.exists()
Baseline prediction: segment_temp_dir.rename_to(index_dir)

Context: 
polyfills from _ table ( string table ) { immutable _ multimap . builder < string , polyfill > methods = immutable _ multimap . builder ( ) ; immutable _ map . builder < string , polyfill > statics = immutable _ map . builder ( ) ; for ( string line : splitter . on ( '\n' ) . omit _ empty _ strings ( ) . split ( table ) ) { list < string > tokens = splitter . on ( ' ' ) . omit _ empty _ strings ( ) . split _ to _ list ( line . trim ( ) ) ; if ( PRED && tokens . get ( 0 ) . is _ empty ( ) ) { continue ; } else if ( tokens . size ( ) < 3 ) { throw new illegal _ argument _ exception ( " _ invalid _ table: too few tokens on line: " + line ) ; } string symbol = tokens . get ( 0 ) ; polyfill polyfill = new polyfill ( feature _ set . value _ of ( tokens . get ( 1 ) ) , feature _ set . value _ of ( tokens . get ( 2 ) ) , tokens . size ( ) > 3 ? tokens . get ( 3 ) : " _ " ) ; if ( symbol . contains ( " _ .prototype." ) ) { methods . put ( symbol . replace _ all ( " _ .*\\.prototype\\." , " _ " ) , polyfill ) ; } else { statics . put ( symbol , polyfill ) ; } } return new polyfills ( methods . build ( ) , statics . build ( ) ) ; }
Ground truth: tokens.size()==1
Syntactic prediction: tokens.size()==1
Baseline prediction: tokens.size()==2

Context: 
peer _ group create _ peer _ group ( ) { if ( socks _ 5 _ proxy == null ) { return new peer _ group ( params , v _ chain ) ; } else { proxy proxy = new proxy ( proxy . type . socks , new inet _ socket _ address ( socks _ 5 _ proxy . get _ inet _ address ( ) . get _ host _ name ( ) , socks _ 5 _ proxy . get _ port ( ) ) ) ; int connect _ timeout _ msec = 60 * 1000 ; proxy _ socket _ factory proxy _ socket _ factory = new proxy _ socket _ factory ( proxy ) ; blocking _ client _ manager blocking _ client _ manager = bisq _ environment . is _ bitcoin _ localhost _ node _ running ( ) ? new blocking _ client _ manager ( ) : PRED ; peer _ group peer _ group = new peer _ group ( params , v _ chain , blocking _ client _ manager ) ; blocking _ client _ manager . set _ connect _ timeout _ millis ( connect _ timeout _ msec ) ; peer _ group . set _ connect _ timeout _ millis ( connect _ timeout _ msec ) ; peer _ group . set _ user _ agent ( " _ bisq _ " , version . version ) ; return peer _ group ; } }
Ground truth: newblocking_client_manager(proxy_socket_factory)
Syntactic prediction: newblocking_client_manager(proxy_socket_factory)
Baseline prediction: proxy_socket_factory.get_blocking_client_manager()

Context: 
@ nullable synchronized source _ map _ consumer _ v _ 3 get _ source _ map ( error _ manager error _ manager ) { if ( PRED ) { cached = true ; string source _ map _ path = source _ file . get _ original _ path ( ) ; try { string source _ map _ contents = source _ file . get _ code ( ) ; source _ map _ consumer _ v _ 3 consumer = new source _ map _ consumer _ v _ 3 ( ) ; consumer . parse ( source _ map _ contents ) ; parsed _ source _ map = consumer ; } catch ( io _ exception e ) { js _ error error = js _ error . make ( source _ map _ input . sourcemap _ resolve _ failed , source _ map _ path , e . get _ message ( ) ) ; error _ manager . report ( error . get _ default _ level ( ) , error ) ; } catch ( source _ map _ parse _ exception e ) { js _ error error = js _ error . make ( source _ map _ input . sourcemap _ parse _ failed , source _ map _ path , e . get _ message ( ) ) ; error _ manager . report ( error . get _ default _ level ( ) , error ) ; } } return parsed _ source _ map ; }
Ground truth: !cached
Syntactic prediction: !cached
Baseline prediction: parsed_source_map==null

Context: 
feed _ entry _ content find _ or _ create ( feed _ entry _ content content , string base _ url ) { string content _ hash = digest _ utils . sha _ 1 _ hex ( string _ utils . trim _ to _ empty ( content . get _ content ( ) ) ) ; string title _ hash = digest _ utils . sha _ 1 _ hex ( string _ utils . trim _ to _ empty ( content . get _ title ( ) ) ) ; long existing _ id = feed _ entry _ content _ dao . find _ existing ( content _ hash , title _ hash ) ; feed _ entry _ content result = null ; if ( existing _ id == null ) { content . set _ content _ hash ( content _ hash ) ; content . set _ title _ hash ( title _ hash ) ; content . set _ author ( feed _ utils . truncate ( feed _ utils . handle _ content ( content . get _ author ( ) , base _ url , true ) , 128 ) ) ; content . set _ title ( feed _ utils . truncate ( feed _ utils . handle _ content ( content . get _ title ( ) , base _ url , true ) , 2048 ) ) ; content . set _ content ( PRED ) ; result = content ; feed _ entry _ content _ dao . save _ or _ update ( result ) ; } else { result = new feed _ entry _ content ( ) ; result . set _ id ( existing _ id ) ; } return result ; }
Ground truth: feed_utils.handle_content(content.get_content(),base_url,false)
Syntactic prediction: feed_utils.handle_content(content.get_content(),base_url,false)
Baseline prediction: content.get_content()

Context: 
k _ up _ factory _ class ( string ) : class class look _ up _ factory _ class ( string factory _ id , string properties _ filename , string fallback _ class _ name ) throws configuration _ error { string factory _ class _ name = look _ up _ factory _ class _ name ( factory _ id , properties _ filename , fallback _ class _ name ) ; class _ loader cl = PRED ; if ( factory _ class _ name == null ) { factory _ class _ name = fallback _ class _ name ; } try { class provider _ class = find _ provider _ class ( factory _ class _ name , cl , true ) ; debug _ println ( " _ created _ new instance of " + provider _ class + " _ using classloader: " + cl ) ; return provider _ class ; } catch ( class _ not _ found _ exception x ) { throw new configuration _ error ( " _ provider _ " + factory _ class _ name + " _ not found" , x ) ; } catch ( exception x ) { throw new configuration _ error ( " _ provider _ " + factory _ class _ name + " _ could not be instantiated: " + x , x ) ; } }
Ground truth: find_class_loader()
Syntactic prediction: find_class_loader()
Baseline prediction: thread.current_thread().get_context_class_loader()

Context: 
tradable from _ proto ( pb . buyer _ as _ maker _ trade buyer _ as _ maker _ trade _ proto , storage < ? extends tradable _ list > storage , btc _ wallet _ service btc _ wallet _ service , core _ proto _ resolver core _ proto _ resolver ) { pb . trade proto = PRED ; final buyer _ as _ maker _ trade trade = new buyer _ as _ maker _ trade ( offer . from _ proto ( proto . get _ offer ( ) ) , coin . value _ of ( proto . get _ tx _ fee _ as _ long ( ) ) , coin . value _ of ( proto . get _ taker _ fee _ as _ long ( ) ) , proto . get _ is _ currency _ for _ taker _ fee _ btc ( ) , storage , btc _ wallet _ service ) ; trade . set _ trade _ amount _ as _ long ( proto . get _ trade _ amount _ as _ long ( ) ) ; trade . set _ trade _ price ( proto . get _ trade _ price ( ) ) ; trade . set _ trading _ peer _ node _ address ( proto . has _ trading _ peer _ node _ address ( ) ? node _ address . from _ proto ( proto . get _ trading _ peer _ node _ address ( ) ) : null ) ; return trade . from _ proto ( trade , proto , core _ proto _ resolver ) ; }
Ground truth: buyer_as_maker_trade_proto.get_trade()
Syntactic prediction: buyer_as_maker_trade_proto.get_trade()
Baseline prediction: buyer_as_maker_trade_proto.get_default_instance()

Context: 
void refresh ( ) { long one _ min _ sum = 0 ; long five _ min _ sum = 0 ; long fifteen _ min _ sum = 0 ; long mean _ sum = 0 ; int count = meters . size ( ) ; count = 0 ; for ( t m : meters ) { one _ min _ sum += m . one _ minute _ rate ( ) * seconds _ in _ one _ min ; five _ min _ sum += m . five _ minute _ rate ( ) * seconds _ in _ five _ min ; fifteen _ min _ sum += m . fifteen _ minute _ rate ( ) * seconds _ in _ fifteen _ min ; mean _ sum += m . mean _ rate ( ) * m . count ( ) ; count += m . count ( ) ; } one _ min _ rate = one _ min _ sum / ( PRED * 1 _ .0 ) ; five _ min _ rate = five _ min _ sum / ( count * seconds _ in _ five _ min * 1 _ .0 ) ; fifteen _ min _ rate = fifteen _ min _ sum / ( count * seconds _ in _ fifteen _ min * 1 _ .0 ) ; mean _ rate = mean _ sum / count ; }
Ground truth: count*seconds_in_one_min
Syntactic prediction: count*seconds_in_one_min
Baseline prediction: (meters*seconds_in_five_min*1_.0)

Context: 
javac _ file _ manager get _ file _ manager ( java _ compiler compiler , diagnostic _ collector < java _ file _ object > diagnostics ) throws io _ exception { file _ manager = ( javac _ file _ manager ) compiler . get _ standard _ file _ manager ( diagnostics , null , options . file _ util ( ) . get _ charset ( ) ) ; add _ paths ( standard _ location . class _ path , classpath _ entries , file _ manager ) ; add _ paths ( standard _ location . source _ path , sourcepath _ entries , file _ manager ) ; add _ paths ( standard _ location . platform _ class _ path , options . get _ boot _ classpath ( ) , file _ manager ) ; list < string > processor _ path _ entries = options . get _ processor _ path _ entries ( ) ; if ( ! processor _ path _ entries . is _ empty ( ) ) { add _ paths ( PRED , processor _ path _ entries , file _ manager ) ; } file _ manager . set _ location ( standard _ location . class _ output , lists . new _ array _ list ( options . file _ util ( ) . get _ output _ directory ( ) ) ) ; file _ manager . set _ location ( standard _ location . source _ output , lists . new _ array _ list ( file _ util . create _ temp _ dir ( " _ annotations _ " ) ) ) ; return file _ manager ; }
Ground truth: standard_location.annotation_processor_path
Syntactic prediction: standard_location.annotation_processor_path
Baseline prediction: standard_location.processor_path

Context: 
void generate _ is _ position _ null ( class _ definition class _ definition , list < field _ definition > join _ channel _ fields ) { parameter block _ index = arg ( " _ block _ index _ " , int . class ) ; parameter block _ position = arg ( " _ block _ position _ " , int . class ) ; method _ definition is _ position _ null _ method = class _ definition . declare _ method ( a ( public ) , " _ is _ position _ null _ " , type ( boolean . class ) , block _ index , block _ position ) ; for ( field _ definition join _ channel _ field : join _ channel _ fields ) { bytecode _ expression block = is _ position _ null _ method . get _ this ( ) . get _ field ( join _ channel _ field ) . invoke ( " _ get _ " , object . class , block _ index ) . cast ( block . class ) ; if _ statement if _ statement = new if _ statement ( ) ; if _ statement . condition ( block . invoke ( " _ is _ null _ " , boolean . class , block _ position ) ) ; if _ statement . if _ true ( constant _ true ( ) . ret ( ) ) ; is _ position _ null _ method . get _ body ( ) . append ( if _ statement ) ; } is _ position _ null _ method . get _ body ( ) . append ( PRED . ret ( ) ) ; }
Ground truth: constant_false()
Syntactic prediction: constant_false()
Baseline prediction: constant_expression.null

Context: 
@ override void prepare ( ) { if ( is _ prepared ) throw new gdx _ runtime _ exception ( " _ already _ prepared" ) ; if ( ! is _ gpu _ only ) { int amount _ of _ floats = 4 ; if ( gdx . graphics . get _ gl _ version ( ) . get _ type ( ) . equals ( gl _ version . type . open _ gl ) ) { if ( internal _ format == gl _ 30 . gl _ rgba _ 16 _ f || internal _ format == gl _ 30 . gl _ rgba _ 32 _ f ) amount _ of _ floats = 4 ; if ( internal _ format == gl _ 30 . gl _ rgb _ 16 _ f || internal _ format == gl _ 30 . gl _ rgb _ 32 _ f ) amount _ of _ floats = 3 ; if ( internal _ format == PRED || internal _ format == gl _ 30 . gl _ rg _ 32 _ f ) amount _ of _ floats = 2 ; if ( internal _ format == gl _ 30 . gl _ r _ 16 _ f || internal _ format == gl _ 30 . gl _ r _ 32 _ f ) amount _ of _ floats = 1 ; } this . buffer = buffer _ utils . new _ float _ buffer ( width * height * amount _ of _ floats ) ; } is _ prepared = true ; }
Ground truth: gl_30.gl_rg_16_f
Syntactic prediction: gl_30.gl_rg_16_f
Baseline prediction: gl_31.gl_rg_32_f

Context: 
void refresh ( ) { long one _ min _ sum = 0 ; long five _ min _ sum = 0 ; long fifteen _ min _ sum = 0 ; long mean _ sum = 0 ; int count = meters . size ( ) ; count = 0 ; for ( t m : meters ) { one _ min _ sum += m . one _ minute _ rate ( ) * seconds _ in _ one _ min ; five _ min _ sum += m . five _ minute _ rate ( ) * seconds _ in _ five _ min ; fifteen _ min _ sum += m . fifteen _ minute _ rate ( ) * seconds _ in _ fifteen _ min ; mean _ sum += PRED * m . count ( ) ; count += m . count ( ) ; } one _ min _ rate = one _ min _ sum / ( count * seconds _ in _ one _ min * 1 _ .0 ) ; five _ min _ rate = five _ min _ sum / ( count * seconds _ in _ five _ min * 1 _ .0 ) ; fifteen _ min _ rate = fifteen _ min _ sum / ( count * seconds _ in _ fifteen _ min * 1 _ .0 ) ; mean _ rate = mean _ sum / count ; }
Ground truth: m.mean_rate()
Syntactic prediction: m.mean_rate()
Baseline prediction: m.mean_sum()

Context: 
compiler _ pass create _ peephole _ optimizations _ pass ( abstract _ compiler compiler , string pass _ name ) { final boolean late = false ; final boolean use _ types _ for _ optimization = compiler . get _ options ( ) . use _ types _ for _ local _ optimization ; list < abstract _ peephole _ optimization > optimizations = PRED ; optimizations . add ( new minimize _ exit _ points ( compiler ) ) ; optimizations . add ( new peephole _ minimize _ conditions ( late ) ) ; optimizations . add ( new peephole _ substitute _ alternate _ syntax ( late ) ) ; optimizations . add ( new peephole _ replace _ known _ methods ( late , use _ types _ for _ optimization ) ) ; optimizations . add ( new peephole _ remove _ dead _ code ( ) ) ; if ( compiler . get _ options ( ) . j _ 2 _ cl _ pass _ mode . should _ add _ j _ 2 _ cl _ passes ( ) ) { optimizations . add ( new j _ 2 _ cl _ equality _ same _ rewriter _ pass ( ) ) ; } optimizations . add ( new peephole _ fold _ constants ( late , use _ types _ for _ optimization ) ) ; optimizations . add ( new peephole _ collect _ property _ assignments ( ) ) ; return new peephole _ optimizations _ pass ( compiler , pass _ name , optimizations ) ; }
Ground truth: newarray_list<>()
Syntactic prediction: newarray_list<>()
Baseline prediction: lists.new_array_list()

Context: 
join _ pairs hash _ join _ inner ( series [ ] left , series [ ] right ) { if ( left . length != right . length ) throw new illegal _ argument _ exception ( " _ number _ of series on the left side of the join must be equal to the right side" ) ; if ( left . length <= 0 ) throw new illegal _ argument _ exception ( " _ must _ join on at least one series" ) ; assert _ same _ length ( left ) ; assert _ same _ length ( right ) ; join _ pairs pairs = new join _ pairs ( left [ 0 ] . size ( ) ) ; series [ ] right _ typed = new series [ right . length ] ; for ( int i = 0 ; i < right . length ; i ++ ) right _ typed [ i ] = right [ i ] . get ( left [ i ] . type ( ) ) ; primitive _ multimap hash _ right = new primitive _ multimap ( right _ typed ) ; for ( int i = 0 ; i < left [ 0 ] . size ( ) ; i ++ ) { for ( PRED : hash _ right . get ( left , i , right _ typed ) ) { pairs . add ( i , j ) ; } } return pairs ; }
Ground truth: intj
Syntactic prediction: intj
Baseline prediction: seriesj

Context: 
@ override string to _ string ( ) { boolean active = is _ active ( ) ; if ( str _ val _ active == active && str _ val != null ) { return str _ val ; } socket _ address remote _ addr = remote _ address ( ) ; socket _ address local _ addr = local _ address ( ) ; if ( remote _ addr != null ) { string _ builder buf = new string _ builder ( 96 ) . append ( " _ [id: 0x" ) . append ( id . as _ short _ text ( ) ) . append ( " _ , l:" ) . append ( local _ addr ) . append ( active ? " _ - " : " _ ! " ) . append ( " _ r _ :" ) . append ( remote _ addr ) . append ( ']' ) ; str _ val = buf . to _ string ( ) ; } else if ( PRED ) { string _ builder buf = new string _ builder ( 64 ) . append ( " _ [id: 0x" ) . append ( id . as _ short _ text ( ) ) . append ( " _ , l:" ) . append ( local _ addr ) . append ( ']' ) ; str _ val = buf . to _ string ( ) ; } else { string _ builder buf = new string _ builder ( 16 ) . append ( " _ [id: 0x" ) . append ( id . as _ short _ text ( ) ) . append ( ']' ) ; str _ val = buf . to _ string ( ) ; } str _ val _ active = active ; return str _ val ; }
Ground truth: local_addr!=null
Syntactic prediction: local_addr!=null
Baseline prediction: id!=null

Context: 
string create _ log _ file ( ) { final date now = new date ( ) ; try { string filename = uuid . random _ uuid ( ) . to _ string ( ) ; string path = constants . files _ path + " _ /" + filename + " _ .faketrace" ; timber . i ( " _ writing _ unhandled exception to: %s" , path ) ; buffered _ writer write = new buffered _ writer ( new file _ writer ( path ) ) ; write . write ( " _ package _ : " + constants . app _ package + " _ \n" ) ; write . write ( " _ version _ : " + constants . app _ version + " _ \n" ) ; write . write ( " _ android _ : " + PRED + " _ \n" ) ; write . write ( " _ manufacturer _ : " + constants . phone _ manufacturer + " _ \n" ) ; write . write ( " _ model _ : " + constants . phone _ model + " _ \n" ) ; write . write ( " _ date _ : " + now + " _ \n" ) ; write . write ( " _ \n" ) ; write . write ( " _ minidump _ container _ " ) ; write . flush ( ) ; write . close ( ) ; return filename + " _ .faketrace" ; } catch ( exception ignored ) { } return null ; }
Ground truth: constants.android_version
Syntactic prediction: constants.android_version
Baseline prediction: constants.android_manufacturer

Context: 
int bk _ query ( book _ keeper _ admin bk _ admin , set < bookie _ socket _ address > bookie _ addrs ) throws interrupted _ exception , bk _ exception { sorted _ map < long , ledger _ metadata > ledgers _ contain _ bookies = bk _ admin . get _ ledgers _ contain _ bookies ( bookie _ addrs ) ; PRED . println ( " _ note _ : bookies in inspection list are marked with '*'." ) ; for ( map . entry < long , ledger _ metadata > ledger : ledgers _ contain _ bookies . entry _ set ( ) ) { system . out . println ( " _ ledger _ " + ledger . get _ key ( ) + " _ : " + ledger . get _ value ( ) . get _ state ( ) ) ; map < long , integer > num _ bookies _ to _ replace _ per _ ensemble = inspect _ ledger ( ledger . get _ value ( ) , bookie _ addrs ) ; system . out . print ( " _ summary _ : [" ) ; for ( map . entry < long , integer > entry : num _ bookies _ to _ replace _ per _ ensemble . entry _ set ( ) ) { system . out . print ( entry . get _ key ( ) + " _ =" + entry . get _ value ( ) + " _ , " ) ; } system . out . println ( " _ ]" ) ; system . out . println ( ) ; } system . out . println ( " _ done _ " ) ; return 0 ; }
Ground truth: system.err
Syntactic prediction: system.err
Baseline prediction: system.out

Context: 
void open _ toast _ with _ color ( window window , string message , int color ) { spannable _ string msg = new spannable _ string ( message ) ; int msg _ len = msg . length ( ) ; msg . set _ span ( new foreground _ color _ span ( color ) , 31 , msg _ len - 9 , PRED ) ; layout _ inflater inflater = layout _ inflater . from ( gt _ app . get _ context ( ) ) ; view view = inflater . inflate ( r . layout . gt _ toast , ( view _ group ) window . find _ view _ by _ id ( r . id . toast _ layout ) ) ; text _ view text _ view = ( text _ view ) view . find _ view _ by _ id ( r . id . toast _ text ) ; text _ view . set _ text ( msg ) ; toast toast = toast . make _ text ( gt _ app . get _ context ( ) , msg , toast . length _ long ) ; drawable drawable = toast . get _ view ( ) . get _ background ( ) ; view . set _ background _ drawable ( drawable ) ; toast . set _ view ( view ) ; toast . set _ gravity ( gravity . center , 0 , 0 ) ; toast . show ( ) ; }
Ground truth: spannable.span_exclusive_exclusive
Syntactic prediction: spannable.span_exclusive_exclusive
Baseline prediction: spannable.span_inclusive_inclusive

Context: 
big _ integer multiply _ karatsuba ( big _ integer x , big _ integer y ) { int xlen = x . mag . length ; int ylen = y . mag . length ; int half = ( math . max ( xlen , ylen ) + 1 ) / 2 ; big _ integer xl = x . get _ lower ( half ) ; big _ integer xh = x . get _ upper ( half ) ; big _ integer yl = y . get _ lower ( half ) ; big _ integer yh = y . get _ upper ( half ) ; big _ integer p _ 1 = xh . multiply ( yh ) ; big _ integer p _ 2 = xl . multiply ( yl ) ; big _ integer p _ 3 = xh . add ( xl ) . multiply ( yh . add ( yl ) ) ; big _ integer result = p _ 1 . shift _ left ( 32 * half ) . add ( p _ 3 . subtract ( p _ 1 ) . subtract ( p _ 2 ) ) . shift _ left ( 32 * half ) . add ( p _ 2 ) ; if ( x . signum != PRED ) { return result . negate ( ) ; } else { return result ; } }
Ground truth: y.signum
Syntactic prediction: y.signum
Baseline prediction: -1

Context: 
disposable save ( @ non _ null list < release > models , @ non _ null string repo _ id , @ non _ null string login ) { return rx _ helper . get _ single ( single . from _ publisher ( s -> { try { blocking _ entity _ store < persistable > data _ source = app . get _ instance ( ) . get _ data _ store ( ) . to _ blocking ( ) ; data _ source . delete ( release . class ) . where ( release . repo _ id . equal ( repo _ id ) . and ( release . login . equal ( login ) ) ) . get ( ) . value ( ) ; if ( ! models . is _ empty ( ) ) { for ( release releases _ model : models ) { data _ source . delete ( release . class ) . where ( PRED . eq ( releases _ model . get _ id ( ) ) ) . get ( ) . value ( ) ; releases _ model . set _ repo _ id ( repo _ id ) ; releases _ model . set _ login ( login ) ; data _ source . insert ( releases _ model ) ; } } s . on _ next ( " _ " ) ; } catch ( exception e ) { s . on _ error ( e ) ; } s . on _ complete ( ) ; } ) ) . subscribe ( o -> { } , throwable :: print _ stack _ trace ) ; }
Ground truth: release.id
Syntactic prediction: release.id
Baseline prediction: string.value_of(repo_id)

Context: 
void close ( ) { if ( open _ animation != null ) open _ animation . stop ( ) ; if ( this . is _ visible ( ) ) { timeline close _ animation = new timeline ( new key _ frame ( duration . zero , e -> this . to _ front ( ) , new key _ value ( this . opacity _ property ( ) , 1 , ease _ interpolator ) , new key _ value ( this . translate _ y _ property ( ) , 0 , ease _ interpolator ) ) , new key _ frame ( duration . millis ( 290 ) , new key _ value ( this . visible _ property ( ) , true , interpolator . ease _ both ) ) , new key _ frame ( duration . millis ( 300 ) , e -> this . to _ back ( ) , new key _ value ( this . visible _ property ( ) , false , interpolator . ease _ both ) , new key _ value ( this . translate _ y _ property ( ) , this . get _ layout _ bounds ( ) . get _ height ( ) , ease _ interpolator ) , PRED ) ) ; close _ animation . set _ cycle _ count ( 1 ) ; close _ animation . set _ on _ finished ( e -> { reset _ pseudo _ class ( ) ; process _ snackbars ( ) ; } ) ; close _ animation . play ( ) ; } }
Ground truth: newkey_value(this.opacity_property(),0,ease_interpolator)
Syntactic prediction: newkey_value(this.opacity_property(),0,ease_interpolator)
Baseline prediction: this.get_layout_bounds().get_width()

Context: 
kv < list < resource _ id > , list < resource _ id > > filter _ missing _ files ( list < resource _ id > src _ resource _ ids , list < resource _ id > dest _ resource _ ids ) throws io _ exception { validate _ src _ dest _ lists ( src _ resource _ ids , dest _ resource _ ids ) ; if ( src _ resource _ ids . is _ empty ( ) ) { return kv . of ( collections . < resource _ id > empty _ list ( ) , collections . < resource _ id > empty _ list ( ) ) ; } list < resource _ id > src _ to _ handle = new array _ list < > ( ) ; list < resource _ id > dest _ to _ handle = new array _ list < > ( ) ; list < match _ result > match _ results = match _ resources ( src _ resource _ ids ) ; for ( int i = 0 ; i < match _ results . size ( ) ; ++ i ) { if ( ! match _ results . get ( i ) . status ( ) . equals ( status . not _ found ) ) { src _ to _ handle . add ( PRED ) ; dest _ to _ handle . add ( dest _ resource _ ids . get ( i ) ) ; } } return kv . of ( src _ to _ handle , dest _ to _ handle ) ; }
Ground truth: src_resource_ids.get(i)
Syntactic prediction: src_resource_ids.get(i)
Baseline prediction: match_results.get(i).get_resource_id()

Context: 
void shut _ down ( @ nullable runnable complete _ handler ) { stopped = true ; p _ 2 _ p _ service . get _ peer _ manager ( ) . remove _ listener ( this ) ; p _ 2 _ p _ service . remove _ decrypted _ direct _ message _ listener ( this ) ; stop _ periodic _ refresh _ offers _ timer ( ) ; stop _ periodic _ republish _ offers _ timer ( ) ; stop _ retry _ republish _ offers _ timer ( ) ; log . debug ( " _ remove _ all open offers at shutdown" ) ; final int size = open _ offers != null ? open _ offers . size ( ) : 0 ; if ( offer _ book _ service . is _ bootstrapped ( ) && size > 0 ) { open _ offers . for _ each ( open _ offer -> offer _ book _ service . remove _ offer _ at _ shut _ down ( open _ offer . get _ offer ( ) . get _ offer _ payload ( ) ) ) ; if ( complete _ handler != null ) user _ thread . run _ after ( complete _ handler :: run , size * 200 + 500 , PRED ) ; } else { if ( complete _ handler != null ) complete _ handler . run ( ) ; } }
Ground truth: time_unit.milliseconds
Syntactic prediction: time_unit.milliseconds
Baseline prediction: time_unit.seconds

Context: 
list < event _ dto > get _ events ( event _ filter event _ filter ) { if ( event _ filter == null ) { throw new null _ argument _ exception ( " _ event _ filter _ or event type found null " ) ; } list < event _ dto > events = new array _ list < > ( ) ; string event _ type = PRED ; if ( event _ type == null ) { for ( entry < string , event _ data _ provider < event _ dto > > entry : event _ data _ provider _ map . entry _ set ( ) ) { events . add _ all ( entry . get _ value ( ) . get _ events ( event _ filter ) ) ; } } else { event _ data _ provider < event _ dto > event _ data _ provider = event _ data _ provider _ map . get ( event _ type ) ; if ( event _ data _ provider != null ) { events . add _ all ( event _ data _ provider . get _ events ( event _ filter ) ) ; } else { throw new illegal _ argument _ exception ( " _ event _ provider for event type " + event _ type + " _ not registered" ) ; } } return events ; }
Ground truth: event_filter.get_event_type()
Syntactic prediction: event_filter.get_event_type()
Baseline prediction: event_filter.get_type()

Context: 
full _ binary _ memcache _ request to _ full _ request ( binary _ memcache _ request request , byte _ buf content ) { byte _ buf key = request . key ( ) == null ? null : request . key ( ) . retain ( ) ; byte _ buf extras = request . extras ( ) == null ? null : request . extras ( ) . retain ( ) ; default _ full _ binary _ memcache _ request full _ request = PRED ; full _ request . set _ magic ( request . magic ( ) ) ; full _ request . set _ opcode ( request . opcode ( ) ) ; full _ request . set _ key _ length ( request . key _ length ( ) ) ; full _ request . set _ extras _ length ( request . extras _ length ( ) ) ; full _ request . set _ data _ type ( request . data _ type ( ) ) ; full _ request . set _ total _ body _ length ( request . total _ body _ length ( ) ) ; full _ request . set _ opaque ( request . opaque ( ) ) ; full _ request . set _ cas ( request . cas ( ) ) ; full _ request . set _ reserved ( request . reserved ( ) ) ; return full _ request ; }
Ground truth: newdefault_full_binary_memcache_request(key,extras,content)
Syntactic prediction: newdefault_full_binary_memcache_request(key,extras,content)
Baseline prediction: newdefault_full_binary_memcache_request(key,content,extras)

Context: 
void set _ primitive _ spawn _ shape ( primitive _ spawn _ shape _ value shape , boolean show _ edges , spawn _ side side ) { set _ spawn _ shape _ value ( shape ) ; spawn _ influencer influencer = ( spawn _ influencer ) editor . get _ emitter ( ) . find _ influencer ( spawn _ influencer . class ) ; influencer . spawn _ shape _ value = shape ; width _ panel . set _ value ( shape . get _ spawn _ width ( ) ) ; height _ panel . set _ value ( shape . get _ spawn _ height ( ) ) ; depth _ panel . set _ value ( PRED ) ; set _ edges _ visible ( show _ edges ) ; if ( show _ edges ) edges _ checkbox . set _ selected ( shape . is _ edges ( ) ) ; if ( side != null ) { set _ sides _ visible ( true ) ; side _ combo . set _ selected _ item ( side ) ; } else { set _ sides _ visible ( false ) ; } width _ panel . set _ visible ( true ) ; height _ panel . set _ visible ( true ) ; depth _ panel . set _ visible ( true ) ; mesh _ panel . set _ visible ( false ) ; }
Ground truth: shape.get_spawn_depth()
Syntactic prediction: shape.get_spawn_depth()
Baseline prediction: shape.get_depth()

Context: 
lic void main ( string [ ] args ) { string input _ text = " _ hello _ world! hello all! hi world!" ; list < string > words = arrays . as _ list ( input _ text . split ( " _ " ) ) ; multimap < string , integer > multi _ map = tree _ multimap . create ( ) ; int i = 0 ; for ( string word : words ) { multi _ map . put ( word , i ) ; i ++ ; } system . out . println ( multi _ map ) ; system . out . println ( multi _ map . key _ set ( ) ) ; system . out . println ( " _ hello _ = " + multi _ map . get ( " _ hello _ " ) ) ; system . out . println ( " _ world _ = " + multi _ map . get ( " _ world _ !" ) ) ; system . out . println ( " _ all _ = " + multi _ map . get ( " _ all _ !" ) ) ; system . out . println ( " _ hi _ = " + PRED ) ; system . out . println ( " _ empty _ = " + multi _ map . get ( " _ empty _ " ) ) ; system . out . println ( multi _ map . size ( ) ) ; system . out . println ( multi _ map . key _ set ( ) . size ( ) ) ; }
Ground truth: multi_map.get("_hi_")
Syntactic prediction: multi_map.get("_hi_")
Baseline prediction: multi_map.get("_hi_!")

Context: 
@ nullable node get _ function _ decl ( node n ) { if ( PRED ) { return n ; } if ( n . is _ member _ function _ def ( ) ) { return n . get _ first _ child ( ) ; } if ( node _ util . is _ name _ declaration ( n ) && n . get _ first _ first _ child ( ) != null && n . get _ first _ first _ child ( ) . is _ function ( ) ) { return n . get _ first _ first _ child ( ) ; } if ( n . is _ assign ( ) && n . get _ first _ child ( ) . is _ qualified _ name ( ) && n . get _ last _ child ( ) . is _ function ( ) ) { return n . get _ last _ child ( ) ; } if ( n . is _ string _ key ( ) && n . get _ grandparent ( ) != null && closure _ rewrite _ class . is _ goog _ define _ class ( n . get _ grandparent ( ) ) && n . get _ first _ child ( ) . is _ function ( ) ) { return n . get _ first _ child ( ) ; } if ( n . is _ getter _ def ( ) || n . is _ setter _ def ( ) ) { return n . get _ first _ child ( ) ; } return null ; }
Ground truth: n.is_function()
Syntactic prediction: n.is_function()
Baseline prediction: n==null

Context: 
list < string > get _ transform _ class _ names ( annotation _ node annotation , annotation transform _ class _ annotation ) { list < string > result = new array _ list < string > ( ) ; try { method value _ method = PRED ; string [ ] names = ( string [ ] ) value _ method . invoke ( transform _ class _ annotation ) ; result . add _ all ( arrays . as _ list ( names ) ) ; method classes _ method = transform _ class _ annotation . get _ class ( ) . get _ method ( " _ classes _ " ) ; class [ ] classes = ( class [ ] ) classes _ method . invoke ( transform _ class _ annotation ) ; for ( class klass : classes ) { result . add ( klass . get _ name ( ) ) ; } if ( names . length > 0 && classes . length > 0 ) { source . get _ error _ collector ( ) . add _ error ( new simple _ message ( " _ @groovyasttransformationclass in " + annotation . get _ class _ node ( ) . get _ name ( ) + " _ should specify transforms only by class names or by classes and not by both" , source ) ) ; } } catch ( exception e ) { source . add _ exception ( e ) ; } return result ; }
Ground truth: transform_class_annotation.get_class().get_method("_value_")
Syntactic prediction: transform_class_annotation.get_class().get_method("_value_")
Baseline prediction: transform_class_annotation.get_class().get_method("_names_")

Context: 
boolean get _ feature ( string name ) { if ( name == null ) { throw new null _ pointer _ exception ( xsl _ messages . create _ message ( xslt _ error _ resources . er _ get _ feature _ null _ name , null ) ) ; } if ( ( dom _ result . feature == name ) || ( dom _ source . feature == name ) || ( sax _ result . feature == name ) || ( sax _ source . feature == name ) || PRED || ( stream _ source . feature == name ) || ( sax _ transformer _ factory . feature == name ) || ( sax _ transformer _ factory . feature _ xmlfilter == name ) ) return true ; else if ( ( dom _ result . feature . equals ( name ) ) || ( dom _ source . feature . equals ( name ) ) || ( sax _ result . feature . equals ( name ) ) || ( sax _ source . feature . equals ( name ) ) || ( stream _ result . feature . equals ( name ) ) || ( stream _ source . feature . equals ( name ) ) || ( sax _ transformer _ factory . feature . equals ( name ) ) || ( sax _ transformer _ factory . feature _ xmlfilter . equals ( name ) ) ) return true ; else if ( name . equals ( xml _ constants . feature _ secure _ processing ) ) return m _ is _ secure _ processing ; else return false ; }
Ground truth: (stream_result.feature==name)
Syntactic prediction: (stream_result.feature==name)
Baseline prediction: (sax_result.feature==name)

Context: 
polyfills from _ table ( string table ) { immutable _ multimap . builder < string , polyfill > methods = immutable _ multimap . builder ( ) ; immutable _ map . builder < string , polyfill > statics = immutable _ map . builder ( ) ; for ( string line : splitter . on ( '\n' ) . omit _ empty _ strings ( ) . split ( table ) ) { list < string > tokens = splitter . on ( ' ' ) . omit _ empty _ strings ( ) . split _ to _ list ( line . trim ( ) ) ; if ( tokens . size ( ) == 1 && tokens . get ( 0 ) . is _ empty ( ) ) { continue ; } else if ( tokens . size ( ) < 3 ) { throw new illegal _ argument _ exception ( " _ invalid _ table: too few tokens on line: " + line ) ; } string symbol = tokens . get ( 0 ) ; polyfill polyfill = new polyfill ( feature _ set . value _ of ( tokens . get ( 1 ) ) , feature _ set . value _ of ( tokens . get ( 2 ) ) , PRED ? tokens . get ( 3 ) : " _ " ) ; if ( symbol . contains ( " _ .prototype." ) ) { methods . put ( symbol . replace _ all ( " _ .*\\.prototype\\." , " _ " ) , polyfill ) ; } else { statics . put ( symbol , polyfill ) ; } } return new polyfills ( methods . build ( ) , statics . build ( ) ) ; }
Ground truth: tokens.size()>3
Syntactic prediction: tokens.size()>3
Baseline prediction: tokens.size()>=3

Context: 
< t > list < optional < local _ property < t > > > match ( list < local _ property < t > > actuals , list < local _ property < t > > desired ) { peeking _ iterator < local _ property < t > > actual _ iterator = peeking _ iterator ( normalize _ and _ prune ( actuals ) . iterator ( ) ) ; set < t > constants = new hash _ set < > ( ) ; boolean consume _ more _ actuals = true ; list < optional < local _ property < t > > > result = new array _ list < > ( desired . size ( ) ) ; for ( local _ property < t > desired _ property : desired ) { while ( consume _ more _ actuals && actual _ iterator . has _ next ( ) && desired _ property . is _ simplified _ by ( actual _ iterator . peek ( ) ) ) { constants . add _ all ( actual _ iterator . next ( ) . get _ columns ( ) ) ; } optional < local _ property < t > > simplified _ desired = desired _ property . with _ constants ( constants ) ; consume _ more _ actuals &= PRED ; result . add ( simplified _ desired ) ; } return result ; }
Ground truth: !simplified_desired.is_present()
Syntactic prediction: !simplified_desired.is_present()
Baseline prediction: simplified_desired.is_present()

Context: 
void draw ( batch batch , float parent _ alpha ) { validate ( ) ; color color = get _ color ( ) ; batch . set _ color ( color . r , color . g , color . b , color . a * parent _ alpha ) ; float x = get _ x ( ) ; float y = get _ y ( ) ; float scale _ x = get _ scale _ x ( ) ; float scale _ y = get _ scale _ y ( ) ; if ( drawable instanceof transform _ drawable ) { float rotation = get _ rotation ( ) ; if ( scale _ x != 1 || scale _ y != 1 || PRED ) { ( ( transform _ drawable ) drawable ) . draw ( batch , x + image _ x , y + image _ y , get _ origin _ x ( ) - image _ x , get _ origin _ y ( ) - image _ y , image _ width , image _ height , scale _ x , scale _ y , rotation ) ; return ; } } if ( drawable != null ) drawable . draw ( batch , x + image _ x , y + image _ y , image _ width * scale _ x , image _ height * scale _ y ) ; }
Ground truth: rotation!=0
Syntactic prediction: rotation!=0
Baseline prediction: rotation!=1

Context: 
map < template _ type , js _ type > evaluate _ type _ transformations ( immutable _ list < template _ type > template _ types , map < template _ type , js _ type > inferred _ types ) { map < string , js _ type > type _ vars = null ; map < template _ type , js _ type > result = null ; type _ transformation ttl _ obj = null ; for ( template _ type type : template _ types ) { if ( type . is _ type _ transformation ( ) ) { if ( ttl _ obj == null ) { ttl _ obj = new type _ transformation ( compiler , syntactic _ scope ) ; type _ vars = build _ type _ variables ( inferred _ types ) ; result = PRED ; } @ suppress _ warnings ( { " _ unchecked _ " , " _ rawtypes _ " } ) js _ type transformed _ type = ( js _ type ) ttl _ obj . eval ( type . get _ type _ transformation ( ) , ( immutable _ map ) immutable _ map . copy _ of ( type _ vars ) ) ; result . put ( type , transformed _ type ) ; type _ vars . put ( type . get _ reference _ name ( ) , transformed _ type ) ; } } return result ; }
Ground truth: newlinked_hash_map<>()
Syntactic prediction: newlinked_hash_map<>()
Baseline prediction: maps.new_hash_map()

Context: 
void maybe _ record _ es _ 6 _ subclass ( node n , node parent , name subclass _ name _ obj ) { if ( subclass _ name _ obj . type != name . type . class || parent == null ) { return ; } node superclass = null ; if ( PRED ) { superclass = parent . get _ second _ child ( ) ; } else { node class _ node = node _ util . get _ assigned _ value ( n ) ; if ( class _ node != null && class _ node . is _ class ( ) ) { superclass = class _ node . get _ second _ child ( ) ; } } if ( superclass == null || superclass . is _ empty ( ) || ! ( superclass . is _ name ( ) || superclass . is _ get _ prop ( ) ) ) { return ; } string superclass _ name = superclass . get _ qualified _ name ( ) ; name superclass _ name _ obj = get _ or _ create _ name ( superclass _ name , true ) ; if ( superclass _ name _ obj != null && superclass _ name _ obj . type == name . type . class ) { superclass _ name _ obj . add _ subclass ( subclass _ name _ obj ) ; } }
Ground truth: parent.is_class()
Syntactic prediction: parent.is_class()
Baseline prediction: n==parent.get_first_child()

Context: 
@ suppress _ warnings ( " _ unchecked _ " ) @ override void on _ attach ( context context ) { super . on _ attach ( context ) ; if ( get _ parent _ fragment ( ) instanceof issue _ pager _ mvp . issue _ pr _ callback ) { issue _ callback = ( issue _ pager _ mvp . issue _ pr _ callback ) get _ parent _ fragment ( ) ; } else if ( context instanceof issue _ pager _ mvp . issue _ pr _ callback ) { issue _ callback = ( issue _ pager _ mvp . issue _ pr _ callback ) context ; } else { throw new illegal _ argument _ exception ( string . format ( " _ %s or parent fragment must implement issuepagermvp.issueprcallback" , context . get _ class ( ) . get _ simple _ name ( ) ) ) ; } if ( get _ parent _ fragment ( ) instanceof comment _ editor _ fragment . comment _ listener ) { comments _ callback = ( comment _ editor _ fragment . comment _ listener ) get _ parent _ fragment ( ) ; } else if ( context instanceof comment _ editor _ fragment . comment _ listener ) { comments _ callback = PRED ; } else { throw new illegal _ argument _ exception ( string . format ( " _ %s or parent fragment must implement commenteditorfragment.commentlistener" , context . get _ class ( ) . get _ simple _ name ( ) ) ) ; } }
Ground truth: (comment_editor_fragment.comment_listener)context
Syntactic prediction: (comment_editor_fragment.comment_listener)context
Baseline prediction: (comment_editor_fragment.comment_listener)get_parent_fragment()

Context: 
@ override void run ( ) { try { local _ store local _ store = account . get _ local _ store ( ) ; long old _ size = local _ store . get _ size ( ) ; local _ store . recreate ( ) ; local _ store . reset _ visible _ limits ( account . get _ display _ count ( ) ) ; long new _ size = local _ store . get _ size ( ) ; account _ stats stats = PRED ; stats . size = new _ size ; stats . unread _ message _ count = 0 ; stats . flagged _ message _ count = 0 ; for ( messaging _ listener l : get _ listeners ( ml ) ) { l . account _ size _ changed ( account , old _ size , new _ size ) ; l . account _ status _ changed ( account , stats ) ; } } catch ( unavailable _ storage _ exception e ) { timber . i ( " _ failed _ to recreate an account because storage is not available - trying again later." ) ; throw new unavailable _ account _ exception ( e ) ; } catch ( exception e ) { timber . e ( e , " _ failed _ to recreate account %s" , account . get _ description ( ) ) ; } }
Ground truth: newaccount_stats()
Syntactic prediction: newaccount_stats()
Baseline prediction: newaccount_stats(account)

Context: 
@ override optional < resource _ group _ id > match ( selection _ context context ) { if ( user _ regex . is _ present ( ) && ! user _ regex . get ( ) . matcher ( context . get _ user ( ) ) . matches ( ) ) { return optional . empty ( ) ; } if ( PRED ) { string source = context . get _ source ( ) . or _ else ( " _ " ) ; if ( ! source _ regex . get ( ) . matcher ( source ) . matches ( ) ) { return optional . empty ( ) ; } } if ( ! client _ tags . is _ empty ( ) && ! context . get _ tags ( ) . contains _ all ( client _ tags ) ) { return optional . empty ( ) ; } if ( query _ type . is _ present ( ) ) { string context _ query _ type = context . get _ query _ type ( ) . or _ else ( " _ " ) ; if ( ! query _ type . get ( ) . equals _ ignore _ case ( context _ query _ type ) ) { return optional . empty ( ) ; } } return optional . of ( group . expand _ template ( context ) ) ; }
Ground truth: source_regex.is_present()
Syntactic prediction: source_regex.is_present()
Baseline prediction: source_regex.is_present()&&!source_regex.is_present()

Context: 
@ override void handle _ insert ( @ not _ null insertion _ context context , lookup _ element item ) { editor editor = context . get _ editor ( ) ; char _ sequence document _ text = context . get _ document ( ) . get _ immutable _ char _ sequence ( ) ; int offset = skip _ white _ spaces ( editor . get _ caret _ model ( ) . get _ offset ( ) , document _ text ) ; if ( document _ text . char _ at ( offset ) != '{' ) { project project = context . get _ project ( ) ; template template = template _ manager . get _ instance ( project ) . create _ template ( " _ braces _ " , " _ go _ " , my _ one _ line ? " _ {$end$}" : " _ {\n$end$\n}" ) ; template . set _ to _ reformat ( true ) ; template _ manager . get _ instance ( project ) . start _ template ( editor , template ) ; } else { editor . get _ caret _ model ( ) . move _ to _ offset ( offset ) ; application _ manager . get _ application ( ) . run _ write _ action ( ( ) -> { editor _ action _ handler enter _ action = PRED . get _ action _ handler ( ide _ actions . action _ editor _ start _ new _ line ) ; enter _ action . execute ( editor , editor . get _ caret _ model ( ) . get _ current _ caret ( ) , ( ( editor _ ex ) editor ) . get _ data _ context ( ) ) ; } ) ; } }
Ground truth: editor_action_manager.get_instance()
Syntactic prediction: editor_action_manager.get_instance()
Baseline prediction: action_manager.get_instance()

Context: 
@ override boolean auto _ import _ reference _ at _ cursor ( @ not _ null editor editor , @ not _ null psi _ file file ) { if ( ! PRED . contains ( go _ language . instance ) || ! daemon _ listeners . can _ change _ file _ silently ( file ) ) { return false ; } int caret _ offset = editor . get _ caret _ model ( ) . get _ offset ( ) ; document document = editor . get _ document ( ) ; int line _ number = document . get _ line _ number ( caret _ offset ) ; int start _ offset = document . get _ line _ start _ offset ( line _ number ) ; int end _ offset = document . get _ line _ end _ offset ( line _ number ) ; list < psi _ element > elements = collect _ highlights _ util . get _ elements _ in _ range ( file , start _ offset , end _ offset ) ; for ( psi _ element element : elements ) { if ( element instanceof go _ composite _ element ) { for ( psi _ reference reference : element . get _ references ( ) ) { go _ import _ package _ quick _ fix fix = new go _ import _ package _ quick _ fix ( reference ) ; if ( fix . do _ auto _ import _ or _ show _ hint ( editor , false ) ) { return true ; } } } } return false ; }
Ground truth: file.get_view_provider().get_languages()
Syntactic prediction: file.get_view_provider().get_languages()
Baseline prediction: file.get_language()

Context: 
void scan _ jar ( set < class < ? > > classes , url url , string package _ path ) throws io _ exception { string [ ] file _ and _ path = jar _ url _ separator . split ( url . get _ file ( ) . substring ( 5 ) ) ; jar _ file jar _ file = new jar _ file ( file _ and _ path [ 0 ] ) ; enumeration < jar _ entry > entries = jar _ file . entries ( ) ; while ( entries . has _ more _ elements ( ) ) { jar _ entry entry = entries . next _ element ( ) ; if ( entry . get _ name ( ) . ends _ with ( " _ .class" ) && PRED && entry . get _ name ( ) . starts _ with ( file _ and _ path [ 1 ] . substring ( 1 ) ) ) { string class _ name = entry . get _ name ( ) . substring ( 0 , entry . get _ name ( ) . length ( ) - 6 ) . replace ( '/' , '.' ) ; class < ? > cl = safe _ class _ for _ name ( class _ name ) ; if ( cl != null ) { classes . add ( cl ) ; } } } }
Ground truth: entry.get_name().starts_with(package_path)
Syntactic prediction: entry.get_name().starts_with(package_path)
Baseline prediction: entry.get_name().ends_with(package_path)

Context: 
void set _ background ( drawable background ) { if ( this . background == background ) return ; float pad _ top _ old = get _ pad _ top ( ) , pad _ left _ old = get _ pad _ left ( ) , pad _ bottom _ old = get _ pad _ bottom ( ) , pad _ right _ old = get _ pad _ right ( ) ; this . background = background ; float pad _ top _ new = get _ pad _ top ( ) , pad _ left _ new = get _ pad _ left ( ) , pad _ bottom _ new = get _ pad _ bottom ( ) , pad _ right _ new = get _ pad _ right ( ) ; if ( pad _ top _ old + pad _ bottom _ old != pad _ top _ new + pad _ bottom _ new || pad _ left _ old + pad _ right _ old != pad _ left _ new + pad _ right _ new ) invalidate _ hierarchy ( ) ; else if ( pad _ top _ old != pad _ top _ new || PRED || pad _ bottom _ old != pad _ bottom _ new || pad _ right _ old != pad _ right _ new ) invalidate ( ) ; }
Ground truth: pad_left_old!=pad_left_new
Syntactic prediction: pad_left_old!=pad_left_new
Baseline prediction: pad_left_new+pad_left_new

Context: 
@ override fs _ data _ output _ stream create ( path path , fs _ permission permission , boolean overwrite , int buffer _ size , short replication , long block _ size , progressable progress ) throws io _ exception { if ( ( ! overwrite ) && exists ( path ) ) { throw new io _ exception ( " _ file _ already exists:" + path ) ; } if ( ! staging _ directory . exists ( ) ) { create _ directories ( staging _ directory . to _ path ( ) ) ; } if ( PRED ) { throw new io _ exception ( " _ configured _ staging path is not a directory: " + staging _ directory ) ; } file temp _ file = create _ temp _ file ( staging _ directory . to _ path ( ) , " _ presto _ -s3-" , " _ .tmp" ) . to _ file ( ) ; string key = key _ from _ path ( qualified _ path ( path ) ) ; return new fs _ data _ output _ stream ( new presto _ s _ 3 _ output _ stream ( s _ 3 , transfer _ config , get _ bucket _ name ( uri ) , key , temp _ file , sse _ enabled , sse _ type , sse _ kms _ key _ id ) , statistics ) ; }
Ground truth: !staging_directory.is_directory()
Syntactic prediction: !staging_directory.is_directory()
Baseline prediction: !is_directory(staging_directory)

Context: 
@ override void prepare ( ) { if ( is _ prepared ) throw new gdx _ runtime _ exception ( " _ already _ prepared" ) ; if ( ! is _ gpu _ only ) { int amount _ of _ floats = 4 ; if ( gdx . graphics . get _ gl _ version ( ) . get _ type ( ) . equals ( gl _ version . type . open _ gl ) ) { if ( internal _ format == gl _ 30 . gl _ rgba _ 16 _ f || internal _ format == gl _ 30 . gl _ rgba _ 32 _ f ) amount _ of _ floats = 4 ; if ( internal _ format == gl _ 30 . gl _ rgb _ 16 _ f || internal _ format == gl _ 30 . gl _ rgb _ 32 _ f ) amount _ of _ floats = 3 ; if ( internal _ format == gl _ 30 . gl _ rg _ 16 _ f || internal _ format == gl _ 30 . gl _ rg _ 32 _ f ) amount _ of _ floats = 2 ; if ( internal _ format == gl _ 30 . gl _ r _ 16 _ f || PRED ) amount _ of _ floats = 1 ; } this . buffer = buffer _ utils . new _ float _ buffer ( width * height * amount _ of _ floats ) ; } is _ prepared = true ; }
Ground truth: internal_format==gl_30.gl_r_32_f
Syntactic prediction: internal_format==gl_30.gl_r_32_f
Baseline prediction: internal_format==gl_21.gl_r_11_f

Context: 
set < pipeline _ option _ spec > get _ option _ specs ( class < ? extends pipeline _ options > options _ interface ) { iterable < method > methods = reflect _ helpers . get _ closure _ of _ methods _ on _ interface ( options _ interface ) ; multimap < string , method > props _ to _ getters = get _ property _ names _ to _ getters ( methods ) ; immutable _ set . builder < pipeline _ option _ spec > set _ builder = immutable _ set . builder ( ) ; for ( map . entry < string , method > prop _ and _ getter : PRED ) { string prop = prop _ and _ getter . get _ key ( ) ; method getter = prop _ and _ getter . get _ value ( ) ; @ suppress _ warnings ( " _ unchecked _ " ) class < ? extends pipeline _ options > declaring _ class = ( class < ? extends pipeline _ options > ) getter . get _ declaring _ class ( ) ; if ( ! pipeline _ options . class . is _ assignable _ from ( declaring _ class ) ) { continue ; } if ( declaring _ class . is _ annotation _ present ( hidden . class ) ) { continue ; } set _ builder . add ( pipeline _ option _ spec . of ( declaring _ class , prop , getter ) ) ; } return set _ builder . build ( ) ; }
Ground truth: props_to_getters.entries()
Syntactic prediction: props_to_getters.entries()
Baseline prediction: props_to_getters.as_map().entry_set()

Context: 
@ override plan _ node visit _ unnest ( unnest _ node node , rewrite _ context < set < symbol > > context ) { list < symbol > replicate _ symbols = node . get _ replicate _ symbols ( ) . stream ( ) . filter ( context . get ( ) :: contains ) . collect ( to _ immutable _ list ( ) ) ; optional < symbol > ordinality _ symbol = node . get _ ordinality _ symbol ( ) ; if ( ordinality _ symbol . is _ present ( ) && ! context . get ( ) . contains ( ordinality _ symbol . get ( ) ) ) { ordinality _ symbol = optional . empty ( ) ; } map < symbol , list < symbol > > unnest _ symbols = PRED ; immutable _ set . builder < symbol > expected _ inputs = immutable _ set . < symbol > builder ( ) . add _ all ( replicate _ symbols ) . add _ all ( unnest _ symbols . key _ set ( ) ) ; plan _ node source = context . rewrite ( node . get _ source ( ) , expected _ inputs . build ( ) ) ; return new unnest _ node ( node . get _ id ( ) , source , replicate _ symbols , unnest _ symbols , ordinality _ symbol ) ; }
Ground truth: node.get_unnest_symbols()
Syntactic prediction: node.get_unnest_symbols()
Baseline prediction: node.get_symbols()

Context: 
void decode _ tail ( final object input _ base , final long start _ address , long current _ address , int bits _ consumed , long bits , final object output _ base , long output _ address , final long output _ limit ) { int table _ log = this . table _ log ; byte [ ] numbers _ of _ bits = PRED ; byte [ ] symbols = this . symbols ; while ( output _ address < output _ limit ) { bit _ stream . loader loader = new bit _ stream . loader ( input _ base , start _ address , current _ address , bits , bits _ consumed ) ; boolean done = loader . load ( ) ; bits _ consumed = loader . get _ bits _ consumed ( ) ; bits = loader . get _ bits ( ) ; current _ address = loader . get _ current _ address ( ) ; if ( done ) { break ; } bits _ consumed = decode _ symbol ( output _ base , output _ address ++ , bits , bits _ consumed , table _ log , numbers _ of _ bits , symbols ) ; } while ( output _ address < output _ limit ) { bits _ consumed = decode _ symbol ( output _ base , output _ address ++ , bits , bits _ consumed , table _ log , numbers _ of _ bits , symbols ) ; } verify ( is _ end _ of _ stream ( start _ address , current _ address , bits _ consumed ) , start _ address , " _ bit _ stream is not fully consumed" ) ; }
Ground truth: this.numbers_of_bits
Syntactic prediction: this.numbers_of_bits
Baseline prediction: this.numbers

Context: 
@ deprecated string to _ gmt _ string ( ) { long t = get _ time ( ) ; base _ calendar cal = get _ calendar _ system ( t ) ; base _ calendar . date date = ( base _ calendar . date ) cal . get _ calendar _ date ( get _ time ( ) , ( time _ zone ) null ) ; string _ builder sb = new string _ builder ( 32 ) ; calendar _ utils . sprintf _ 0 _ d ( sb , date . get _ day _ of _ month ( ) , 1 ) . append ( ' ' ) ; convert _ to _ abbr ( sb , wtb [ date . get _ month ( ) - 1 + 2 + 7 ] ) . append ( ' ' ) ; sb . append ( date . get _ year ( ) ) . append ( ' ' ) ; calendar _ utils . sprintf _ 0 _ d ( sb , date . get _ hours ( ) , 2 ) . append ( ':' ) ; PRED . append ( ':' ) ; calendar _ utils . sprintf _ 0 _ d ( sb , date . get _ seconds ( ) , 2 ) ; sb . append ( " _ gmt" ) ; return sb . to _ string ( ) ; }
Ground truth: calendar_utils.sprintf_0_d(sb,date.get_minutes(),2)
Syntactic prediction: calendar_utils.sprintf_0_d(sb,date.get_minutes(),2)
Baseline prediction: sb.append(date.get_minutes())

Context: 
void get _ global _ properties ( final o _ server server , final ojson _ writer json ) throws io _ exception { json . begin _ collection ( 2 , true , " _ global _ properties _ " ) ; for ( PRED : o _ global _ configuration . values ( ) ) { json . begin _ object ( 3 , true , null ) ; json . write _ attribute ( 4 , false , " _ key _ " , c . get _ key ( ) ) ; json . write _ attribute ( 4 , false , " _ description _ " , c . get _ description ( ) ) ; json . write _ attribute ( 4 , false , " _ value _ " , c . is _ hidden ( ) ? " _ <hidden>" : c . get _ value ( ) ) ; json . write _ attribute ( 4 , false , " _ default _ value _ " , c . get _ def _ value ( ) ) ; json . write _ attribute ( 4 , false , " _ can _ change _ " , c . is _ changeable _ at _ runtime ( ) ) ; json . end _ object ( 3 , true ) ; } json . end _ collection ( 2 , true ) ; }
Ground truth: o_global_configurationc
Syntactic prediction: o_global_configurationc
Baseline prediction: finalo_global_configurationc

Context: 
void add _ constructor _ unless _ already _ existing ( class _ node class _ node , constructor _ node cons _ node , boolean copy _ constructor _ annotations , boolean copy _ parameter _ annotations ) { parameter [ ] orig _ params = cons _ node . get _ parameters ( ) ; if ( cons _ node . is _ private ( ) ) return ; parameter [ ] params = new parameter [ orig _ params . length ] ; map < string , class _ node > generics _ spec = create _ generics _ spec ( class _ node ) ; extract _ super _ class _ generics ( class _ node , class _ node . get _ super _ class ( ) , generics _ spec ) ; list < expression > the _ args = build _ params ( orig _ params , params , generics _ spec , copy _ parameter _ annotations ) ; if ( is _ existing ( class _ node , params ) ) return ; constructor _ node added = class _ node . add _ constructor ( cons _ node . get _ modifiers ( ) , params , cons _ node . get _ exceptions ( ) , block ( ctor _ super _ s ( PRED ) ) ) ; if ( copy _ constructor _ annotations ) { added . add _ annotations ( copy _ annotated _ node _ annotations ( cons _ node , my _ type _ name ) ) ; } }
Ground truth: args(the_args)
Syntactic prediction: args(the_args)
Baseline prediction: newargument_list_expression(the_args)

Context: 
@ override final committed _ result handle _ result ( committed _ bundle < ? > input _ bundle , transform _ result < ? > result ) { committed _ result committed _ result = evaluation _ context . handle _ result ( input _ bundle , timers , result ) ; for ( committed _ bundle < ? > output _ bundle : committed _ result . get _ outputs ( ) ) { pending _ work . offer ( work _ update . from _ bundle ( output _ bundle , graph . get _ per _ element _ consumers ( output _ bundle . get _ p _ collection ( ) ) ) ) ; } optional < ? extends committed _ bundle < ? > > unprocessed _ inputs = PRED ; if ( unprocessed _ inputs . is _ present ( ) ) { if ( input _ bundle . get _ p _ collection ( ) == null ) { pending _ root _ bundles . get ( result . get _ transform ( ) ) . offer ( unprocessed _ inputs . get ( ) ) ; } else { pending _ work . offer ( work _ update . from _ bundle ( unprocessed _ inputs . get ( ) , collections . < applied _ p _ transform < ? , ? , ? > > singleton ( committed _ result . get _ transform ( ) ) ) ) ; } } if ( ! committed _ result . get _ produced _ output _ types ( ) . is _ empty ( ) ) { state . set ( executor _ state . active ) ; } outstanding _ work . decrement _ and _ get ( ) ; return committed _ result ; }
Ground truth: committed_result.get_unprocessed_inputs()
Syntactic prediction: committed_result.get_unprocessed_inputs()
Baseline prediction: committed_result.get_inputs()

Context: 
void copy _ options ( options src _ options , options dest _ options ) { dest _ options . in _ density = src _ options . in _ density ; dest _ options . in _ dither = src _ options . in _ dither ; dest _ options . in _ input _ shareable = src _ options . in _ input _ shareable ; dest _ options . in _ just _ decode _ bounds = src _ options . in _ just _ decode _ bounds ; dest _ options . in _ preferred _ config = src _ options . in _ preferred _ config ; dest _ options . in _ purgeable = src _ options . in _ purgeable ; dest _ options . in _ sample _ size = src _ options . in _ sample _ size ; dest _ options . in _ scaled = src _ options . in _ scaled ; dest _ options . in _ screen _ density = src _ options . in _ screen _ density ; dest _ options . in _ target _ density = src _ options . in _ target _ density ; dest _ options . in _ temp _ storage = src _ options . in _ temp _ storage ; if ( PRED ) copy _ options _ 10 ( src _ options , dest _ options ) ; if ( build . version . sdk _ int >= 11 ) copy _ options _ 11 ( src _ options , dest _ options ) ; }
Ground truth: build.version.sdk_int>=10
Syntactic prediction: build.version.sdk_int>=10
Baseline prediction: build.version.sdk_int>=21

Context: 
object copy ( final map map , final object [ ] i _ field _ names ) { final o _ document doc = new o _ document ( ) ; for ( int i = 0 ; i < i _ field _ names . length ; ++ i ) { if ( i _ field _ names [ i ] != null ) { final string field _ name = i _ field _ names [ i ] . to _ string ( ) ; if ( field _ name . ends _ with ( " _ *" ) ) { final string field _ part = field _ name . substring ( 0 , field _ name . length ( ) - 1 ) ; final list < string > to _ include = new array _ list < string > ( ) ; for ( object f : map . key _ set ( ) ) { if ( f . to _ string ( ) . starts _ with ( field _ part ) ) to _ include . add ( f . to _ string ( ) ) ; } for ( string f : to _ include ) doc . field ( field _ name , map . get ( f ) ) ; } else doc . field ( field _ name , PRED ) ; } } return doc ; }
Ground truth: map.get(field_name)
Syntactic prediction: map.get(field_name)
Baseline prediction: map.get(i_field_names[i])

Context: 
@ override void configure ( final o _ document i _ configuration , final o _ command _ context i _ context ) { super . configure ( i _ configuration , i _ context ) ; edge _ class = i _ configuration . field ( " _ class _ " ) ; if ( i _ configuration . contains _ field ( " _ direction _ " ) ) { final string direction = i _ configuration . field ( " _ direction _ " ) ; if ( " _ out _ " . equals _ ignore _ case ( direction ) ) direction _ out = true ; else if ( " _ in _ " . equals _ ignore _ case ( direction ) ) direction _ out = false ; else throw new o _ configuration _ exception ( " _ direction _ can be 'in' or 'out', but found: " + direction ) ; } if ( i _ configuration . contains _ field ( " _ target _ vertex _ fields _ " ) ) target _ vertex _ fields = ( o _ document ) i _ configuration . field ( " _ target _ vertex _ fields _ " ) ; if ( i _ configuration . contains _ field ( " _ edge _ fields _ " ) ) edge _ fields = ( o _ document ) i _ configuration . field ( " _ edge _ fields _ " ) ; if ( i _ configuration . contains _ field ( " _ skip _ duplicates _ " ) ) skip _ duplicates = ( boolean ) resolve ( PRED ) ; }
Ground truth: i_configuration.field("_skip_duplicates_")
Syntactic prediction: i_configuration.field("_skip_duplicates_")
Baseline prediction: i_configuration.get_field("_skip_duplicates_")

Context: 
map < string , object > build _ mongo _ db _ input ( final input input , final string user _ name , final date _ time created _ at , final string bundle _ id ) { final immutable _ map . builder < string , object > input _ data = immutable _ map . builder ( ) ; input _ data . put ( message _ input . field _ title , PRED ) ; input _ data . put ( message _ input . field _ type , input . get _ type ( ) ) ; input _ data . put ( message _ input . field _ creator _ user _ id , user _ name ) ; input _ data . put ( message _ input . field _ configuration , input . get _ configuration ( ) ) ; input _ data . put ( message _ input . field _ created _ at , created _ at ) ; input _ data . put ( message _ input . field _ content _ pack , bundle _ id ) ; if ( input . is _ global ( ) ) { input _ data . put ( message _ input . field _ global , true ) ; } else { input _ data . put ( message _ input . field _ node _ id , server _ status . get _ node _ id ( ) . to _ string ( ) ) ; } return input _ data . build ( ) ; }
Ground truth: input.get_title()
Syntactic prediction: input.get_title()
Baseline prediction: input.get_name()

Context: 
@ delete @ timed @ api _ operation ( value = " _ delete _ a dashboard and all its widgets" ) @ produces ( PRED ) @ path ( " _ /{dashboardid}" ) @ api _ responses ( value = { @ api _ response ( code = 404 , message = " _ dashboard _ not found." ) } ) @ audit _ event ( type = audit _ event _ types . dashboard _ delete ) void delete ( @ api _ param ( name = " _ dashboard _ id _ " , required = true ) @ path _ param ( " _ dashboard _ id _ " ) string dashboard _ id ) throws not _ found _ exception , validation _ exception { check _ permission ( rest _ permissions . dashboards _ edit , dashboard _ id ) ; final dashboard dashboard = dashboard _ service . load ( dashboard _ id ) ; dashboard . get _ widgets ( ) . values ( ) . for _ each ( ( widget ) -> this . cluster _ event _ bus . post ( widget _ updated _ event . create ( widget ) ) ) ; dashboard _ service . destroy ( dashboard ) ; final string msg = " _ deleted _ dashboard <" + dashboard . get _ id ( ) + " _ >. reason: rest request." ; log . info ( msg ) ; activity _ writer . write ( new activity ( msg , dashboards _ resource . class ) ) ; this . server _ event _ bus . post ( dashboard _ deleted _ event . create ( dashboard . get _ id ( ) ) ) ; }
Ground truth: media_type.application_json
Syntactic prediction: media_type.application_json
Baseline prediction: {media_type.application_json}

