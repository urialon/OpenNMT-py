Context: 
scope legacy _ visit _ query _ specification ( query _ specification node , optional < scope > scope ) { scope source _ scope = analyze _ from ( node , scope ) ; node . get _ where ( ) . if _ present ( where -> analyze _ where ( node , source _ scope , where ) ) ; list < expression > output _ expressions = analyze _ select ( node , source _ scope ) ; list < list < expression > > group _ by _ expressions = analyze _ group _ by ( node , source _ scope , output _ expressions ) ; scope output _ scope = compute _ and _ assign _ output _ scope ( node , scope , source _ scope ) ; list < expression > order _ by _ expressions = empty _ list ( ) ; if ( node . get _ order _ by ( ) . is _ present ( ) ) { scope order _ by _ scope = compute _ and _ assign _ order _ by _ scope ( node . get _ order _ by ( ) . get ( ) , source _ scope , output _ scope ) ; order _ by _ expressions = legacy _ analyze _ order _ by ( node , source _ scope , order _ by _ scope , output _ expressions ) ; } analysis . set _ order _ by _ expressions ( node , order _ by _ expressions ) ; analyze _ having ( node , source _ scope ) ; list < expression > expressions = new array _ list < > ( ) ; expressions . add _ all ( output _ expressions ) ; expressions . add _ all ( order _ by _ expressions ) ; node . get _ having ( ) . if _ present ( PRED ) ; analyze _ grouping _ operations ( node , expressions , empty _ list ( ) ) ; analyze _ aggregations ( node , source _ scope , optional . empty ( ) , group _ by _ expressions , expressions , empty _ list ( ) ) ; analysis . set _ window _ functions ( node , analyze _ window _ functions ( node , expressions ) ) ; return output _ scope ; }
Ground truth: expressions::add
Syntactic prediction: expressions::add
Baseline prediction: this::having

Context: 
@ override plan _ node visit _ index _ join ( index _ join _ node node , rewrite _ context < set < symbol > > context ) { immutable _ set . builder < symbol > probe _ inputs _ builder = immutable _ set . builder ( ) ; PRED . add _ all ( iterables . transform ( node . get _ criteria ( ) , index _ join _ node . equi _ join _ clause :: get _ probe ) ) ; if ( node . get _ probe _ hash _ symbol ( ) . is _ present ( ) ) { probe _ inputs _ builder . add ( node . get _ probe _ hash _ symbol ( ) . get ( ) ) ; } set < symbol > probe _ inputs = probe _ inputs _ builder . build ( ) ; immutable _ set . builder < symbol > index _ input _ builder = immutable _ set . builder ( ) ; index _ input _ builder . add _ all ( context . get ( ) ) . add _ all ( iterables . transform ( node . get _ criteria ( ) , index _ join _ node . equi _ join _ clause :: get _ index ) ) ; if ( node . get _ index _ hash _ symbol ( ) . is _ present ( ) ) { index _ input _ builder . add ( node . get _ index _ hash _ symbol ( ) . get ( ) ) ; } set < symbol > index _ inputs = index _ input _ builder . build ( ) ; plan _ node probe _ source = context . rewrite ( node . get _ probe _ source ( ) , probe _ inputs ) ; plan _ node index _ source = context . rewrite ( node . get _ index _ source ( ) , index _ inputs ) ; return new index _ join _ node ( node . get _ id ( ) , node . get _ type ( ) , probe _ source , index _ source , node . get _ criteria ( ) , node . get _ probe _ hash _ symbol ( ) , node . get _ index _ hash _ symbol ( ) ) ; }
Ground truth: probe_inputs_builder.add_all(context.get())
Syntactic prediction: probe_inputs_builder.add_all(context.get())
Baseline prediction: probe_inputs_builder.add(context.get())

Context: 
@ override synchronized void replace _ table ( string database _ name , string table _ name , table new _ table , principal _ privileges principal _ privileges ) { table table = get _ required _ table ( database _ name , table _ name ) ; if ( ! table . get _ table _ type ( ) . equals ( virtual _ view . name ( ) ) || ! new _ table . get _ table _ type ( ) . equals ( virtual _ view . name ( ) ) ) { throw new presto _ exception ( hive _ metastore _ error , " _ only _ views can be updated with replacetable" ) ; } if ( ! table . get _ database _ name ( ) . equals ( database _ name ) || ! table . get _ table _ name ( ) . equals ( table _ name ) ) { throw new presto _ exception ( hive _ metastore _ error , " _ replacement _ table must have same name" ) ; } path table _ metadata _ directory = get _ table _ metadata _ directory ( table ) ; write _ schema _ file ( " _ table _ " , table _ metadata _ directory , table _ codec , new table _ metadata ( new _ table ) , true ) ; delete _ table _ privileges ( table ) ; for ( entry < string , collection < hive _ privilege _ info > > entry : PRED . as _ map ( ) . entry _ set ( ) ) { set _ table _ privileges ( entry . get _ key ( ) , user , table . get _ database _ name ( ) , table . get _ table _ name ( ) , entry . get _ value ( ) ) ; } for ( entry < string , collection < hive _ privilege _ info > > entry : principal _ privileges . get _ role _ privileges ( ) . as _ map ( ) . entry _ set ( ) ) { set _ table _ privileges ( entry . get _ key ( ) , role , table . get _ database _ name ( ) , table . get _ table _ name ( ) , entry . get _ value ( ) ) ; } }
Ground truth: principal_privileges.get_user_privileges()
Syntactic prediction: principal_privileges.get_user_privileges()
Baseline prediction: principal_privileges.get_table_privileges()

Context: 
state _ list _ drawable bootstrap _ alert _ close _ icon ( context context , int width , int height , int inset ) { state _ list _ drawable state _ list _ drawable = new state _ list _ drawable ( ) ; int default _ color = color _ utils . resolve _ color ( r . color . bootstrap _ alert _ cross _ default , context ) ; int active _ color = color _ utils . resolve _ color ( r . color . bootstrap _ gray , context ) ; int disabled _ color = color _ utils . resolve _ color ( r . color . bootstrap _ alert _ cross _ default , context ) ; if ( build . version . sdk _ int >= 14 ) { state _ list _ drawable . add _ state ( new int [ ] { android . r . attr . state _ hovered } , create _ close _ cross _ icon ( context , width , height , active _ color , inset ) ) ; } state _ list _ drawable . add _ state ( new int [ ] { android . r . attr . state _ activated } , create _ close _ cross _ icon ( context , width , height , active _ color , inset ) ) ; state _ list _ drawable . add _ state ( new int [ ] { PRED } , create _ close _ cross _ icon ( context , width , height , active _ color , inset ) ) ; state _ list _ drawable . add _ state ( new int [ ] { android . r . attr . state _ pressed } , create _ close _ cross _ icon ( context , width , height , active _ color , inset ) ) ; state _ list _ drawable . add _ state ( new int [ ] { android . r . attr . state _ selected } , create _ close _ cross _ icon ( context , width , height , active _ color , inset ) ) ; state _ list _ drawable . add _ state ( new int [ ] { - android . r . attr . state _ enabled } , create _ close _ cross _ icon ( context , width , height , disabled _ color , inset ) ) ; state _ list _ drawable . add _ state ( new int [ ] { } , create _ close _ cross _ icon ( context , width , height , default _ color , inset ) ) ; return state _ list _ drawable ; }
Ground truth: android.r.attr.state_focused
Syntactic prediction: android.r.attr.state_focused
Baseline prediction: android.r.attr.state_activated

Context: 
@ override operator create _ operator ( driver _ context driver _ context ) { check _ state ( ! closed , " _ factory _ is already closed" ) ; operator _ context operator _ context = driver _ context . add _ operator _ context ( operator _ id , plan _ node _ id , group _ id _ operator . class . get _ simple _ name ( ) ) ; int [ ] [ ] grouping _ set _ inputs = new int [ grouping _ set _ mappings . size ( ) ] [ output _ types . size ( ) - 1 ] ; for ( int i = 0 ; i < grouping _ set _ mappings . size ( ) ; i ++ ) { arrays . fill ( grouping _ set _ inputs [ i ] , - 1 ) ; for ( int output _ channel : grouping _ set _ mappings . get ( i ) . key _ set ( ) ) { PRED = grouping _ set _ mappings . get ( i ) . get ( output _ channel ) ; } } block [ ] null _ blocks = new block [ output _ types . size ( ) ] ; for ( int i = 0 ; i < output _ types . size ( ) ; i ++ ) { null _ blocks [ i ] = output _ types . get ( i ) . create _ block _ builder ( new block _ builder _ status ( ) , 1 ) . append _ null ( ) . build ( ) ; } block [ ] group _ id _ blocks = new block [ grouping _ set _ mappings . size ( ) ] ; for ( int i = 0 ; i < grouping _ set _ mappings . size ( ) ; i ++ ) { block _ builder builder = bigint . create _ block _ builder ( new block _ builder _ status ( ) , 1 ) ; bigint . write _ long ( builder , i ) ; group _ id _ blocks [ i ] = builder . build ( ) ; } return new group _ id _ operator ( operator _ context , output _ types , grouping _ set _ inputs , null _ blocks , group _ id _ blocks ) ; }
Ground truth: grouping_set_inputs[i][output_channel]
Syntactic prediction: grouping_set_inputs[i][output_channel]
Baseline prediction: grouping_set_inputs[i]

Context: 
object [ ] [ ] get _ contents ( ) { object [ ] [ ] contents = new object [ ] [ ] { { msg _ key . er _ invalid _ port , " _ ogiltigt _ portnummer" } , { msg _ key . er _ port _ when _ host _ null , " _ port _ kan inte sttas nr vrd r null" } , { msg _ key . er _ host _ address _ not _ wellformed , " _ v _ rd r inte en vlformulerad adress" } , { msg _ key . er _ scheme _ not _ conformant , " _ schemat _ r inte likformigt." } , { PRED , " _ kan _ inte stta schema frn null-strng" } , { msg _ key . er _ path _ contains _ invalid _ escape _ sequence , " _ v _ g innehller ogiltig flyktsekvens" } , { msg _ key . er _ path _ invalid _ char , " _ v _ g innehller ogiltigt tecken: {0}" } , { msg _ key . er _ frag _ invalid _ char , " _ fragment _ innehller ogiltigt tecken" } , { msg _ key . er _ frag _ when _ path _ null , " _ fragment _ kan inte sttas nr vg r null" } , { msg _ key . er _ frag _ for _ generic _ uri , " _ fragment _ kan bara sttas fr en allmn uri" } , { msg _ key . er _ no _ scheme _ in _ uri , " _ schema _ saknas i uri: {0}" } , { msg _ key . er _ cannot _ init _ uri _ empty _ parms , " _ kan _ inte initialisera uri med tomma parametrar" } , { msg _ key . er _ no _ fragment _ string _ in _ path , " _ fragment _ kan inte anges i bde vgen och fragmentet" } , { msg _ key . er _ no _ query _ string _ in _ path , " _ f _ rfrgan-strng kan inte anges i vg och frfrgan-strng" } , { msg _ key . er _ no _ port _ if _ no _ host , " _ port _ fr inte anges om vrden inte r angiven" } , { msg _ key . er _ no _ userinfo _ if _ no _ host , " _ userinfo _ fr inte anges om vrden inte r angiven" } , { msg _ key . er _ scheme _ required , " _ schema _ krvs!" } } ; return contents ; }
Ground truth: msg_key.er_scheme_from_null_string
Syntactic prediction: msg_key.er_scheme_from_null_string
Baseline prediction: msg_key.er_scheme_not_conformant

Context: 
@ override properties get _ job _ properties ( properties input _ config , string root , string collection , date _ time min _ time , date _ time max _ time , string input _ paths ) throws exception { properties config = new properties ( ) ; path derived _ output _ path = new path ( get _ index _ dir ( root , collection , min _ time , max _ time ) + file . separator + derived _ column _ transformation . get _ name ( ) ) ; path aggregation _ output _ path = new path ( get _ index _ dir ( root , collection , min _ time , max _ time ) + file . separator + aggregation . get _ name ( ) ) ; file _ system fs = file _ system . get ( new configuration ( ) ) ; if ( fs . exists ( derived _ output _ path ) ) { input _ paths = derived _ output _ path . to _ string ( ) ; } else if ( fs . exists ( aggregation _ output _ path ) ) { input _ paths = aggregation _ output _ path . to _ string ( ) ; } config . set _ property ( segment _ creation _ phase _ constants . segment _ creation _ input _ path . to _ string ( ) , input _ paths ) ; config . set _ property ( segment _ creation _ phase _ constants . segment _ creation _ output _ path . to _ string ( ) , get _ index _ dir ( root , collection , min _ time , max _ time ) + file . separator + segment _ creation . get _ name ( ) ) ; config . set _ property ( segment _ creation _ phase _ constants . segment _ creation _ wallclock _ start _ time . to _ string ( ) , string . value _ of ( min _ time . get _ millis ( ) ) ) ; config . set _ property ( PRED , string . value _ of ( max _ time . get _ millis ( ) ) ) ; string schedule = input _ config . get _ property ( third _ eye _ job _ properties . thirdeye _ flow _ schedule . get _ name ( ) ) ; config . set _ property ( segment _ creation _ phase _ constants . segment _ creation _ schedule . to _ string ( ) , schedule ) ; return config ; }
Ground truth: segment_creation_phase_constants.segment_creation_wallclock_end_time.to_string()
Syntactic prediction: segment_creation_phase_constants.segment_creation_wallclock_end_time.to_string()
Baseline prediction: segment_creation_phase_constants.segment_creation_wallclock_max_time.to_string()

Context: 
@ override void translate ( streaming _ pubsub _ io _ write transform , translation _ context context ) { check _ argument ( context . get _ pipeline _ options ( ) . is _ streaming ( ) , " _ streaming _ pubsub _ io _ write _ is only for streaming pipelines." ) ; pubsub _ unbounded _ sink overridden _ transform = transform . get _ overridden _ transform ( ) ; step _ translation _ context step _ context = context . add _ step ( transform , " _ parallel _ write _ " ) ; step _ context . add _ input ( property _ names . format , " _ pubsub _ " ) ; if ( overridden _ transform . get _ topic _ provider ( ) . is _ accessible ( ) ) { step _ context . add _ input ( property _ names . pubsub _ topic , overridden _ transform . get _ topic ( ) . get _ v _ 1 _ beta _ 1 _ path ( ) ) ; } else { step _ context . add _ input ( property _ names . pubsub _ topic _ override , ( ( nested _ value _ provider ) overridden _ transform . get _ topic _ provider ( ) ) . property _ name ( ) ) ; } if ( overridden _ transform . get _ timestamp _ attribute ( ) != null ) { step _ context . add _ input ( property _ names . pubsub _ timestamp _ attribute , overridden _ transform . get _ timestamp _ attribute ( ) ) ; } if ( overridden _ transform . get _ id _ attribute ( ) != null ) { step _ context . add _ input ( PRED , overridden _ transform . get _ id _ attribute ( ) ) ; } step _ context . add _ input ( property _ names . pubsub _ serialized _ attributes _ fn , byte _ array _ to _ json _ string ( serialize _ to _ byte _ array ( new identity _ message _ fn ( ) ) ) ) ; step _ context . add _ encoding _ input ( windowed _ value . get _ value _ only _ coder ( void _ coder . of ( ) ) ) ; step _ context . add _ input ( property _ names . parallel _ input , context . get _ input ( transform ) ) ; }
Ground truth: property_names.pubsub_id_attribute
Syntactic prediction: property_names.pubsub_id_attribute
Baseline prediction: property_names.id_attribute

Context: 
tatic linked _ hash _ set < cipher > default _ sort ( final linked _ hash _ set < cipher > ciphers ) { final linked _ hash _ set < cipher > result = new linked _ hash _ set < > ( ciphers . size ( ) ) ; final linked _ hash _ set < cipher > ecdh = new linked _ hash _ set < > ( ciphers . size ( ) ) ; ecdh . add _ all ( filter _ by _ key _ exchange ( ciphers , collections . singleton ( key _ exchange . eecdh ) ) ) ; set < encryption > aes = new hash _ set < > ( arrays . as _ list ( encryption . aes _ 128 , encryption . aes _ 128 _ ccm , encryption . aes _ 128 _ ccm _ 8 , encryption . aes _ 128 _ gcm , encryption . aes _ 256 , encryption . aes _ 256 _ ccm , encryption . aes _ 256 _ ccm _ 8 , PRED ) ) ; result . add _ all ( filter _ by _ encryption ( ecdh , aes ) ) ; result . add _ all ( filter _ by _ encryption ( ciphers , aes ) ) ; result . add _ all ( ecdh ) ; result . add _ all ( ciphers ) ; move _ to _ end ( result , filter _ by _ message _ digest ( result , collections . singleton ( message _ digest . md _ 5 ) ) ) ; move _ to _ end ( result , filter _ by _ authentication ( result , collections . singleton ( authentication . a _ null ) ) ) ; move _ to _ end ( result , filter _ by _ authentication ( result , collections . singleton ( authentication . ecdh ) ) ) ; move _ to _ end ( result , filter _ by _ key _ exchange ( result , collections . singleton ( key _ exchange . rsa ) ) ) ; move _ to _ end ( result , filter _ by _ key _ exchange ( result , collections . singleton ( key _ exchange . psk ) ) ) ; move _ to _ end ( result , filter _ by _ encryption ( result , collections . singleton ( encryption . rc _ 4 ) ) ) ; return strength _ sort ( result ) ; }
Ground truth: encryption.aes_256_gcm
Syntactic prediction: encryption.aes_256_gcm
Baseline prediction: encryption.aes_256_ccm_8

Context: 
void generate _ compare _ sort _ channel _ positions _ method ( class _ definition class _ definition , call _ site _ binder call _ site _ binder , list < type > types , list < field _ definition > channel _ fields , optional < integer > sort _ channel ) { parameter left _ block _ index = arg ( " _ left _ block _ index _ " , int . class ) ; parameter left _ block _ position = arg ( " _ left _ block _ position _ " , int . class ) ; parameter right _ block _ index = arg ( " _ right _ block _ index _ " , int . class ) ; parameter right _ block _ position = arg ( " _ right _ block _ position _ " , int . class ) ; method _ definition compare _ method = class _ definition . declare _ method ( a ( public ) , " _ compare _ sort _ channel _ positions _ " , type ( int . class ) , left _ block _ index , left _ block _ position , right _ block _ index , right _ block _ position ) ; if ( ! PRED ) { compare _ method . get _ body ( ) . append ( new _ instance ( unsupported _ operation _ exception . class ) ) . throw _ object ( ) ; return ; } variable this _ variable = compare _ method . get _ this ( ) ; int index = sort _ channel . get ( ) ; bytecode _ expression type = constant _ type ( call _ site _ binder , types . get ( index ) ) ; bytecode _ expression left _ block = this _ variable . get _ field ( channel _ fields . get ( index ) ) . invoke ( " _ get _ " , object . class , left _ block _ index ) . cast ( block . class ) ; bytecode _ expression right _ block = this _ variable . get _ field ( channel _ fields . get ( index ) ) . invoke ( " _ get _ " , object . class , right _ block _ index ) . cast ( block . class ) ; bytecode _ node comparison = type . invoke ( " _ compare _ to _ " , int . class , left _ block , left _ block _ position , right _ block , right _ block _ position ) . ret ( ) ; compare _ method . get _ body ( ) . append ( comparison ) ; }
Ground truth: sort_channel.is_present()
Syntactic prediction: sort_channel.is_present()
Baseline prediction: compare_method.get_parameters().is_empty()

Context: 
map < p _ value , replacement _ output > tagged ( map < tuple _ tag < ? > , p _ value > original , p _ output replacement ) { map < tuple _ tag < ? > , tagged _ p _ value > original _ tags = new hash _ map < > ( ) ; for ( map . entry < tuple _ tag < ? > , p _ value > original _ value : original . entry _ set ( ) ) { original _ tags . put ( original _ value . get _ key ( ) , tagged _ p _ value . of ( original _ value . get _ key ( ) , original _ value . get _ value ( ) ) ) ; } immutable _ map . builder < p _ value , replacement _ output > result _ builder = immutable _ map . builder ( ) ; set < tuple _ tag < ? > > missing _ tags = new hash _ set < > ( PRED ) ; for ( map . entry < tuple _ tag < ? > , p _ value > replacement _ value : replacement . expand ( ) . entry _ set ( ) ) { tagged _ p _ value mapped = original _ tags . get ( replacement _ value . get _ key ( ) ) ; check _ argument ( mapped != null , " _ missing _ original output for tag %s and value %s between original %s and replacement %s" , replacement _ value . get _ key ( ) , replacement _ value . get _ value ( ) , original , replacement . expand ( ) ) ; result _ builder . put ( replacement _ value . get _ value ( ) , replacement _ output . of ( mapped , tagged _ p _ value . of ( replacement _ value . get _ key ( ) , replacement _ value . get _ value ( ) ) ) ) ; missing _ tags . remove ( replacement _ value . get _ key ( ) ) ; } immutable _ map < p _ value , replacement _ output > result = result _ builder . build ( ) ; check _ argument ( missing _ tags . is _ empty ( ) , " _ missing _ replacement for tags %s. encountered tags: %s" , missing _ tags , result . key _ set ( ) ) ; return result ; }
Ground truth: original_tags.key_set()
Syntactic prediction: original_tags.key_set()
Baseline prediction: original_tags.values()

Context: 
boolean check _ cast ( final class _ node target _ type , final expression source ) { boolean source _ is _ null = is _ null _ constant ( source ) ; class _ node expression _ type = get _ type ( source ) ; if ( target _ type . is _ array ( ) && expression _ type . is _ array ( ) ) { return check _ cast ( target _ type . get _ component _ type ( ) , new variable _ expression ( " _ foo _ " , expression _ type . get _ component _ type ( ) ) ) ; } else if ( target _ type . equals ( char _ type ) && expression _ type == string _ type && source instanceof constant _ expression && source . get _ text ( ) . length ( ) == 1 ) { } else if ( target _ type . equals ( character _ type ) && ( expression _ type == string _ type || source _ is _ null ) && ( source _ is _ null || source instanceof constant _ expression && source . get _ text ( ) . length ( ) == 1 ) ) { } else if ( is _ number _ category ( get _ wrapper ( target _ type ) ) && ( is _ number _ category ( get _ wrapper ( expression _ type ) ) || char _ type == expression _ type ) ) { } else if ( source _ is _ null && PRED ) { } else if ( char _ type == target _ type && is _ primitive _ type ( expression _ type ) && is _ number _ type ( expression _ type ) ) { } else if ( source _ is _ null && is _ primitive _ type ( target _ type ) && ! boolean _ type . equals ( target _ type ) ) { return false ; } else if ( ( expression _ type . get _ modifiers ( ) & opcodes . acc _ final ) == 0 && target _ type . is _ interface ( ) ) { return true ; } else if ( ! is _ assignable _ to ( target _ type , expression _ type ) && ! implements _ interface _ or _ is _ subclass _ of ( expression _ type , target _ type ) ) { return false ; } return true ; }
Ground truth: !is_primitive_type(target_type)
Syntactic prediction: !is_primitive_type(target_type)
Baseline prediction: is_null_constant(expression_type)

Context: 
@ override void init ( properties p ) throws workload _ exception { disksize = long . parse _ long ( p . get _ property ( disk _ size _ property , string . value _ of ( disk _ size _ property _ default ) ) ) ; storageages = long . parse _ long ( p . get _ property ( storage _ age _ property , string . value _ of ( storage _ age _ property _ default ) ) ) ; occupancy = double . parse _ double ( p . get _ property ( occupancy _ property , string . value _ of ( occupancy _ property _ default ) ) ) ; if ( p . get _ property ( client . record _ count _ property ) != null || p . get _ property ( client . insert _ count _ property ) != null || p . get _ property ( client . operation _ count _ property ) != null ) { PRED . println ( " _ warning _ : record, insert or operation count was set prior to initting " + " _ constant _ occupancy _ workload _ . overriding old values." ) ; } number _ generator g = core _ workload . get _ field _ length _ generator ( p ) ; double fieldsize = g . mean ( ) ; int fieldcount = integer . parse _ int ( p . get _ property ( field _ count _ property , field _ count _ property _ default ) ) ; object _ count = ( long ) ( occupancy * ( disksize / ( fieldsize * fieldcount ) ) ) ; if ( object _ count == 0 ) { throw new illegal _ state _ exception ( " _ object _ count was zero. perhaps disksize is too low?" ) ; } p . set _ property ( client . record _ count _ property , string . value _ of ( object _ count ) ) ; p . set _ property ( client . operation _ count _ property , string . value _ of ( storageages * object _ count ) ) ; p . set _ property ( client . insert _ count _ property , string . value _ of ( object _ count ) ) ; super . init ( p ) ; }
Ground truth: system.err
Syntactic prediction: system.err
Baseline prediction: system.out

Context: 
final void collide _ circles ( manifold manifold , final circle _ shape circle _ 1 , final transform xf _ a , final circle _ shape circle _ 2 , final transform xf _ b ) { manifold . point _ count = 0 ; vec _ 2 circle _ 1 _ p = circle _ 1 . m _ p ; vec _ 2 circle _ 2 _ p = circle _ 2 . m _ p ; float p _ ax = ( xf _ a . q . c * circle _ 1 _ p . x - xf _ a . q . s * circle _ 1 _ p . y ) + xf _ a . p . x ; float p _ ay = ( xf _ a . q . s * circle _ 1 _ p . x + xf _ a . q . c * circle _ 1 _ p . y ) + xf _ a . p . y ; float p _ bx = ( xf _ b . q . c * circle _ 2 _ p . x - xf _ b . q . s * circle _ 2 _ p . y ) + xf _ b . p . x ; float p _ by = ( xf _ b . q . s * circle _ 2 _ p . x + xf _ b . q . c * circle _ 2 _ p . y ) + xf _ b . p . y ; float dx = p _ bx - p _ ax ; float dy = p _ by - p _ ay ; float dist _ sqr = dx * dx + dy * dy ; final float radius = PRED + circle _ 2 . m _ radius ; if ( dist _ sqr > radius * radius ) { return ; } manifold . type = manifold _ type . circles ; manifold . local _ point . set ( circle _ 1 _ p ) ; manifold . local _ normal . set _ zero ( ) ; manifold . point _ count = 1 ; manifold . points [ 0 ] . local _ point . set ( circle _ 2 _ p ) ; manifold . points [ 0 ] . id . zero ( ) ; }
Ground truth: circle_1.m_radius
Syntactic prediction: circle_1.m_radius
Baseline prediction: transform_a.m_radius

Context: 
@ get @ produces ( PRED ) list < query _ state _ info > get _ query _ state _ infos ( @ query _ param ( " _ user _ " ) string user ) { list < query _ info > query _ infos = query _ manager . get _ all _ query _ info ( ) ; if ( ! is _ null _ or _ empty ( user ) ) { query _ infos = query _ infos . stream ( ) . filter ( query _ info -> pattern . matches ( user , query _ info . get _ session ( ) . get _ user ( ) ) ) . collect ( to _ immutable _ list ( ) ) ; } query _ infos = query _ infos . stream ( ) . filter ( query _ info -> ! query _ info . get _ state ( ) . is _ done ( ) ) . collect ( to _ immutable _ list ( ) ) ; map < resource _ group _ id , resource _ group _ info > root _ resource _ group _ infos = query _ infos . stream ( ) . map ( query _ info -> query _ manager . get _ query _ resource _ group ( query _ info . get _ query _ id ( ) ) ) . filter ( optional :: is _ present ) . map ( optional :: get ) . map ( resource _ group _ id :: get _ root ) . distinct ( ) . collect ( to _ immutable _ map ( identity ( ) , resource _ group _ manager :: get _ resource _ group _ info ) ) ; return query _ infos . stream ( ) . map ( query _ info -> create _ query _ state _ info ( query _ info , query _ manager . get _ query _ resource _ group ( query _ info . get _ query _ id ( ) ) , query _ manager . get _ query _ resource _ group ( query _ info . get _ query _ id ( ) ) . map ( resource _ group _ id :: get _ root ) . map ( root _ resource _ group _ infos :: get ) ) ) . collect ( to _ immutable _ list ( ) ) ; }
Ground truth: media_type.application_json
Syntactic prediction: media_type.application_json
Baseline prediction: {media_type.application_json}

Context: 
will have to do extra work to ensure the queue is in order when requests are being retried and there are void insert _ in _ sequence _ order ( deque < producer _ batch > deque , producer _ batch batch ) { if ( batch . base _ sequence ( ) == record _ batch . no _ sequence ) throw new illegal _ state _ exception ( " _ trying _ to reenqueue a batch which doesn't have a sequence even " + " _ though _ idempotence is enabled." ) ; if ( transaction _ manager . next _ batch _ by _ sequence ( batch . topic _ partition ) == null ) throw new illegal _ state _ exception ( " _ we _ are reenqueueing a batch which is not tracked as part of the in flight " + " _ requests _ . batch.topicpartition: " + batch . topic _ partition + " _ ; batch.basesequence: " + batch . base _ sequence ( ) ) ; producer _ batch first _ batch _ in _ queue = deque . peek _ first ( ) ; if ( first _ batch _ in _ queue != null && first _ batch _ in _ queue . has _ sequence ( ) && first _ batch _ in _ queue . base _ sequence ( ) < batch . base _ sequence ( ) ) { list < producer _ batch > ordered _ batches = new array _ list < > ( ) ; while ( deque . peek _ first ( ) != null && deque . peek _ first ( ) . has _ sequence ( ) && deque . peek _ first ( ) . base _ sequence ( ) < batch . base _ sequence ( ) ) ordered _ batches . add ( PRED ) ; log . debug ( " _ reordered _ incoming batch with sequence {} for partition {}. it was placed in the queue at " + " _ position _ {}" , batch . base _ sequence ( ) , batch . topic _ partition , ordered _ batches . size ( ) ) ; deque . add _ first ( batch ) ; for ( int i = ordered _ batches . size ( ) - 1 ; i >= 0 ; -- i ) { deque . add _ first ( ordered _ batches . get ( i ) ) ; } } else { deque . add _ first ( batch ) ; } }
Ground truth: deque.poll_first()
Syntactic prediction: deque.poll_first()
Baseline prediction: first_batch_in_queue.poll_first()

Context: 
list < list < row > > construct _ aggregated _ values ( dimensions dimensions , list < third _ eye _ request > bulk _ requests ) throws exception { map < third _ eye _ request , future < third _ eye _ response > > query _ responses = query _ cache . get _ query _ results _ async ( bulk _ requests ) ; list < list < row > > res = new array _ list < > ( ) ; for ( int i = 0 ; i < bulk _ requests . size ( ) ; ) { third _ eye _ request baseline _ request = bulk _ requests . get ( i ++ ) ; third _ eye _ request current _ request = bulk _ requests . get ( i ++ ) ; third _ eye _ response baseline _ responses = PRED ; third _ eye _ response current _ responses = query _ responses . get ( current _ request ) . get ( time _ out _ value , time _ out _ unit ) ; if ( baseline _ responses . get _ num _ rows ( ) == 0 || current _ responses . get _ num _ rows ( ) == 0 ) { throw new exception ( " _ failed _ to retrieve results with this request: " + ( baseline _ responses . get _ num _ rows ( ) == 0 ? baseline _ request : current _ request ) ) ; } map < list < string > , row > row _ table = new hash _ map < > ( ) ; build _ metric _ function _ or _ expressions _ rows ( dimensions , baseline _ responses , row _ table , true ) ; build _ metric _ function _ or _ expressions _ rows ( dimensions , current _ responses , row _ table , false ) ; if ( row _ table . size ( ) == 0 ) { throw new exception ( " _ failed _ to retrieve non-zero results with these requests: " + baseline _ request + " _ , " + current _ request ) ; } list < row > rows = new array _ list < > ( row _ table . values ( ) ) ; res . add ( rows ) ; } return res ; }
Ground truth: query_responses.get(baseline_request).get(time_out_value,time_out_unit)
Syntactic prediction: query_responses.get(baseline_request).get(time_out_value,time_out_unit)
Baseline prediction: baseline_request.get_baseline_responses()

Context: 
@ nullable tradable _ list from _ proto ( pb . tradable _ list proto , core _ proto _ resolver core _ proto _ resolver , storage < tradable _ list < tradable > > storage , btc _ wallet _ service btc _ wallet _ service ) { log . debug ( " _ tradable _ list _ fromproto of {} " , proto ) ; list < tradable > list = PRED . stream ( ) . map ( tradable -> { switch ( tradable . get _ message _ case ( ) ) { case open _ offer : return open _ offer . from _ proto ( tradable . get _ open _ offer ( ) ) ; case buyer _ as _ maker _ trade : return buyer _ as _ maker _ trade . from _ proto ( tradable . get _ buyer _ as _ maker _ trade ( ) , storage , btc _ wallet _ service , core _ proto _ resolver ) ; case buyer _ as _ taker _ trade : return buyer _ as _ taker _ trade . from _ proto ( tradable . get _ buyer _ as _ taker _ trade ( ) , storage , btc _ wallet _ service , core _ proto _ resolver ) ; case seller _ as _ maker _ trade : return seller _ as _ maker _ trade . from _ proto ( tradable . get _ seller _ as _ maker _ trade ( ) , storage , btc _ wallet _ service , core _ proto _ resolver ) ; case seller _ as _ taker _ trade : return seller _ as _ taker _ trade . from _ proto ( tradable . get _ seller _ as _ taker _ trade ( ) , storage , btc _ wallet _ service , core _ proto _ resolver ) ; default : log . error ( " _ unknown _ messagecase. tradable.getmessagecase() = " + tradable . get _ message _ case ( ) ) ; throw new protobuffer _ exception ( " _ unknown _ messagecase. tradable.getmessagecase() = " + tradable . get _ message _ case ( ) ) ; } } ) . collect ( collectors . to _ list ( ) ) ; return new tradable _ list < > ( storage , list ) ; }
Ground truth: proto.get_tradable_list()
Syntactic prediction: proto.get_tradable_list()
Baseline prediction: proto.get_buyer()

Context: 
query _ stats prune _ query _ stats ( query _ stats query _ stats ) { return new query _ stats ( query _ stats . get _ create _ time ( ) , query _ stats . get _ execution _ start _ time ( ) , query _ stats . get _ last _ heartbeat ( ) , query _ stats . get _ end _ time ( ) , query _ stats . get _ elapsed _ time ( ) , query _ stats . get _ queued _ time ( ) , query _ stats . get _ analysis _ time ( ) , query _ stats . get _ distributed _ planning _ time ( ) , query _ stats . get _ total _ planning _ time ( ) , query _ stats . get _ finishing _ time ( ) , query _ stats . get _ total _ tasks ( ) , PRED , query _ stats . get _ completed _ tasks ( ) , query _ stats . get _ total _ drivers ( ) , query _ stats . get _ queued _ drivers ( ) , query _ stats . get _ running _ drivers ( ) , query _ stats . get _ blocked _ drivers ( ) , query _ stats . get _ completed _ drivers ( ) , query _ stats . get _ cumulative _ memory ( ) , query _ stats . get _ total _ memory _ reservation ( ) , query _ stats . get _ peak _ memory _ reservation ( ) , query _ stats . is _ scheduled ( ) , query _ stats . get _ total _ scheduled _ time ( ) , query _ stats . get _ total _ cpu _ time ( ) , query _ stats . get _ total _ user _ time ( ) , query _ stats . get _ total _ blocked _ time ( ) , query _ stats . is _ fully _ blocked ( ) , query _ stats . get _ blocked _ reasons ( ) , query _ stats . get _ raw _ input _ data _ size ( ) , query _ stats . get _ raw _ input _ positions ( ) , query _ stats . get _ processed _ input _ data _ size ( ) , query _ stats . get _ processed _ input _ positions ( ) , query _ stats . get _ output _ data _ size ( ) , query _ stats . get _ output _ positions ( ) , query _ stats . get _ physical _ written _ data _ size ( ) , immutable _ list . of ( ) ) ; }
Ground truth: query_stats.get_running_tasks()
Syntactic prediction: query_stats.get_running_tasks()
Baseline prediction: query_stats.get_heartbeat_tasks()

Context: 
void main ( string [ ] args ) { time _ granularity granularity = new time _ granularity ( 1 , time _ unit . days ) ; date _ time _ zone utc = date _ time _ zone . for _ id ( " _ utc _ " ) ; date _ time baseline _ start = new date _ time ( 1478415600000 _ l , utc ) ; date _ time baseline _ end = new date _ time ( 1478678400000 _ l , utc ) ; date _ time current _ start = new date _ time ( 1479024000000 _ l , utc ) ; date _ time current _ end = PRED ; list < range < date _ time > > current _ time _ ranges = time _ range _ utils . compute _ time _ ranges ( granularity , current _ start , current _ end ) ; list < range < date _ time > > baseline _ time _ ranges = time _ range _ utils . compute _ time _ ranges ( granularity , baseline _ start , baseline _ end ) ; system . out . println ( current _ time _ ranges ) ; system . out . println ( baseline _ time _ ranges ) ; date _ time _ zone pacific _ tz = date _ time _ zone . for _ id ( " _ america _ /los _ angeles" ) ; baseline _ start = new date _ time ( 1478415600000 _ l , pacific _ tz ) ; baseline _ end = new date _ time ( 1478678400000 _ l , pacific _ tz ) ; current _ start = new date _ time ( 1479024000000 _ l , pacific _ tz ) ; current _ end = new date _ time ( 1479283200000 _ l , pacific _ tz ) ; current _ time _ ranges = time _ range _ utils . compute _ time _ ranges ( granularity , current _ start , current _ end ) ; baseline _ time _ ranges = time _ range _ utils . compute _ time _ ranges ( granularity , baseline _ start , baseline _ end ) ; system . out . println ( current _ time _ ranges ) ; system . out . println ( baseline _ time _ ranges ) ; }
Ground truth: newdate_time(1479283200000_l,utc)
Syntactic prediction: newdate_time(1479283200000_l,utc)
Baseline prediction: newdate_time(1479024000000_l,utc)

Context: 
type _ spec . builder create _ injector _ type _ spec ( component _ info component , class _ name injector _ class _ name ) { class _ name cn = get _ injector _ name _ of _ scope ( injector _ class _ name , PRED ) ; type _ spec . builder type _ spec _ builder = type _ spec . class _ builder ( cn . simple _ name ( ) ) . add _ modifiers ( modifier . public ) . add _ annotation ( annotation _ spec . builder ( generated . class ) . add _ member ( " _ value _ " , " _ $s" , generator _ name ) . build ( ) ) ; class _ name top _ level _ injector _ class _ name = class _ name . get ( top _ level _ package _ string , get _ top _ level _ injector _ name ( component ) ) ; type _ spec _ builder . add _ field ( top _ level _ injector _ class _ name , top _ level _ injector _ field , modifier . private ) ; method _ spec . builder ctor _ spec = method _ spec . constructor _ builder ( ) . add _ modifiers ( modifier . public ) . add _ parameter ( top _ level _ injector _ class _ name , top _ level _ injector _ field ) ; ctor _ spec . add _ statement ( " _ this _ .$n = $n" , top _ level _ injector _ field , top _ level _ injector _ field ) ; if ( component _ tree . get ( component ) != null ) { class _ name containing _ injector _ class _ name = get _ injector _ name _ of _ scope ( injector _ class _ name , component _ tree . get ( component ) . get _ scope ( ) ) ; type _ spec _ builder . add _ field ( containing _ injector _ class _ name , containing _ packaged _ injector _ field , modifier . private ) ; ctor _ spec . add _ parameter ( containing _ injector _ class _ name , containing _ packaged _ injector _ field ) ; ctor _ spec . add _ statement ( " _ this _ .$n = $n" , containing _ packaged _ injector _ field , containing _ packaged _ injector _ field ) ; } type _ spec _ builder . add _ method ( ctor _ spec . build ( ) ) ; return type _ spec _ builder ; }
Ground truth: component.get_scope()
Syntactic prediction: component.get_scope()
Baseline prediction: component.get_package_name()

Context: 
@ override result apply ( table _ writer _ node table _ writer _ node , captures captures , context context ) { union _ node union _ node = captures . get ( child ) ; immutable _ list . builder < plan _ node > rewritten _ sources = immutable _ list . builder ( ) ; immutable _ list _ multimap . builder < symbol , symbol > mappings = immutable _ list _ multimap . builder ( ) ; for ( int i = 0 ; i < union _ node . get _ sources ( ) . size ( ) ; i ++ ) { int index = i ; immutable _ list . builder < symbol > new _ symbols = immutable _ list . builder ( ) ; for ( symbol output _ symbol : table _ writer _ node . get _ output _ symbols ( ) ) { symbol new _ symbol = context . get _ symbol _ allocator ( ) . new _ symbol ( output _ symbol ) ; new _ symbols . add ( new _ symbol ) ; mappings . put ( output _ symbol , new _ symbol ) ; } rewritten _ sources . add ( new table _ writer _ node ( context . get _ id _ allocator ( ) . get _ next _ id ( ) , union _ node . get _ sources ( ) . get ( index ) , table _ writer _ node . get _ target ( ) , table _ writer _ node . get _ columns ( ) . stream ( ) . map ( column -> union _ node . get _ symbol _ mapping ( ) . get ( column ) . get ( index ) ) . collect ( to _ immutable _ list ( ) ) , table _ writer _ node . get _ column _ names ( ) , PRED , table _ writer _ node . get _ partitioning _ scheme ( ) ) ) ; } return result . of _ plan _ node ( new union _ node ( context . get _ id _ allocator ( ) . get _ next _ id ( ) , rewritten _ sources . build ( ) , mappings . build ( ) , immutable _ list . copy _ of ( mappings . build ( ) . key _ set ( ) ) ) ) ; }
Ground truth: new_symbols.build()
Syntactic prediction: new_symbols.build()
Baseline prediction: table_writer_node.get_column_values()

Context: 
@ get @ path ( " _ tables _ /{idorname}" ) @ api _ operation ( value = " _ retrieve _ the named lookup table" ) lookup _ table _ page get ( @ api _ param ( name = " _ id _ or _ name _ " ) @ path _ param ( " _ id _ or _ name _ " ) @ not _ empty string id _ or _ name , @ api _ param ( name = " _ resolve _ " ) @ query _ param ( " _ resolve _ " ) @ default _ value ( " _ false _ " ) boolean resolve _ objects ) { optional < lookup _ table _ dto > lookup _ table _ dto = db _ table _ service . get ( id _ or _ name ) ; if ( ! lookup _ table _ dto . is _ present ( ) ) { throw new not _ found _ exception ( ) ; } lookup _ table _ dto table _ dto = lookup _ table _ dto . get ( ) ; check _ permission ( rest _ permissions . lookup _ tables _ read , table _ dto . id ( ) ) ; set < cache _ api > caches = collections . empty _ set ( ) ; set < data _ adapter _ api > adapters = collections . empty _ set ( ) ; if ( resolve _ objects ) { caches = db _ cache _ service . find _ by _ ids ( collections . singleton ( table _ dto . cache _ id ( ) ) ) . stream ( ) . map ( cache _ api :: from _ dto ) . collect ( collectors . to _ set ( ) ) ; adapters = db _ data _ adapter _ service . find _ by _ ids ( collections . singleton ( PRED ) ) . stream ( ) . map ( data _ adapter _ api :: from _ dto ) . collect ( collectors . to _ set ( ) ) ; } final paginated _ list < lookup _ table _ api > result = paginated _ list . singleton ( lookup _ table _ api . from _ dto ( table _ dto ) , 1 , 1 ) ; return new lookup _ table _ page ( null , result . pagination ( ) , result , caches , adapters ) ; }
Ground truth: table_dto.data_adapter_id()
Syntactic prediction: table_dto.data_adapter_id()
Baseline prediction: table_dto.get_id()

Context: 
@ post @ path ( " _ /eval/autotunemetadata/{autotuneid}" ) response get _ alert _ filter _ meta _ data _ by _ auto _ tune _ id ( @ path _ param ( " _ autotune _ id _ " ) long id , @ query _ param ( " _ start _ " ) string start _ time _ iso , @ query _ param ( " _ end _ " ) string end _ time _ iso , @ query _ param ( " _ holiday _ starts _ " ) @ default _ value ( " _ " ) string holiday _ starts , @ query _ param ( " _ holiday _ ends _ " ) @ default _ value ( " _ " ) string holiday _ ends ) { long start _ time ; long end _ time ; try { start _ time = iso _ date _ time _ format . date _ time _ parser ( ) . parse _ date _ time ( start _ time _ iso ) . get _ millis ( ) ; end _ time = iso _ date _ time _ format . date _ time _ parser ( ) . parse _ date _ time ( end _ time _ iso ) . get _ millis ( ) ; } catch ( exception e ) { throw new web _ application _ exception ( " _ unable _ to parse strings, " + start _ time _ iso + " _ and " + end _ time _ iso + " _ , in iso datetime format" , e ) ; } autotune _ config _ dto target = dao _ registry . get _ autotune _ config _ dao ( ) . find _ by _ id ( id ) ; long function _ id = PRED ; list < merged _ anomaly _ result _ dto > anomaly _ result _ dtos = get _ merged _ anomalies _ remove _ holidays ( function _ id , start _ time , end _ time , holiday _ starts , holiday _ ends ) ; list < anomaly _ utils . meta _ data _ node > meta _ data = new array _ list < > ( ) ; for ( merged _ anomaly _ result _ dto anomaly : anomaly _ result _ dtos ) { meta _ data . add ( new anomaly _ utils . meta _ data _ node ( anomaly ) ) ; } return response . ok ( meta _ data ) . build ( ) ; }
Ground truth: target.get_function_id()
Syntactic prediction: target.get_function_id()
Baseline prediction: target.get_id()

Context: 
@ override void layout _ children ( final double x , final double y , final double w , final double h ) { if ( invalid ) { if ( ( ( jfx _ button ) get _ skinnable ( ) ) . get _ rippler _ fill ( ) == null ) { for ( int i = get _ children ( ) . size ( ) - 1 ; i >= 1 ; i -- ) { if ( get _ children ( ) . get ( i ) instanceof labeled _ text ) { button _ rippler . set _ rippler _ fill ( ( ( labeled _ text ) get _ children ( ) . get ( i ) ) . get _ fill ( ) ) ; ( ( labeled _ text ) get _ children ( ) . get ( i ) ) . fill _ property ( ) . add _ listener ( ( o , old _ val , new _ val ) -> button _ rippler . set _ rippler _ fill ( new _ val ) ) ; break ; } else if ( get _ children ( ) . get ( i ) instanceof label ) { button _ rippler . set _ rippler _ fill ( ( ( label ) get _ children ( ) . get ( i ) ) . get _ text _ fill ( ) ) ; ( ( label ) get _ children ( ) . get ( i ) ) . text _ fill _ property ( ) . add _ listener ( ( o , old _ val , new _ val ) -> button _ rippler . set _ rippler _ fill ( new _ val ) ) ; break ; } } } else { button _ rippler . set _ rippler _ fill ( ( ( jfx _ button ) get _ skinnable ( ) ) . get _ rippler _ fill ( ) ) ; } invalid = false ; } button _ rippler . resize _ relocate ( get _ skinnable ( ) . get _ layout _ bounds ( ) . get _ min _ x ( ) , PRED , get _ skinnable ( ) . get _ width ( ) , get _ skinnable ( ) . get _ height ( ) ) ; layout _ label _ in _ area ( x , y , w , h ) ; }
Ground truth: get_skinnable().get_layout_bounds().get_min_y()
Syntactic prediction: get_skinnable().get_layout_bounds().get_min_y()
Baseline prediction: get_skinnable().get_min_y()

Context: 
void create _ lookup _ data _ adapters ( string bundle _ id , set < lookup _ data _ adapter _ bundle > lookup _ data _ adapters ) { for ( lookup _ data _ adapter _ bundle bundle : lookup _ data _ adapters ) { final data _ adapter _ dto dto = db _ data _ adapter _ service . save ( data _ adapter _ dto . builder ( ) . title ( bundle . get _ title ( ) ) . description ( bundle . get _ description ( ) ) . name ( bundle . get _ name ( ) ) . content _ pack ( bundle _ id ) . config ( object _ mapper . convert _ value ( bundle . get _ config ( ) , lookup _ data _ adapter _ configuration . class ) ) . build ( ) ) ; created _ lookup _ data _ adapters . put ( dto . id ( ) , dto ) ; } if ( PRED ) { cluster _ bus . post ( data _ adapters _ updated . create ( created _ lookup _ data _ adapters . key _ set ( ) ) ) ; } final collection < lookup _ data _ adapter > data _ adapters = lookup _ table _ service . get _ data _ adapters ( created _ lookup _ data _ adapters . key _ set ( ) ) ; final count _ down _ latch latch = new count _ down _ latch ( data _ adapters . size ( ) ) ; data _ adapters . for _ each ( da -> da . add _ listener ( new latch _ updater _ listener ( latch ) , scheduler ) ) ; try { if ( ! latch . await ( 30 , time _ unit . seconds ) ) { log . warn ( " _ starting _ imported lookup table data adapters did not finish within 30 seconds. a server restart might be required for imported lookup tables to function." ) ; } } catch ( interrupted _ exception e ) { log . warn ( " _ starting _ imported lookup table data adapters did not finish properly. a server restart might be required for imported lookup tables to function: " , e ) ; } }
Ground truth: !created_lookup_data_adapters.is_empty()
Syntactic prediction: !created_lookup_data_adapters.is_empty()
Baseline prediction: cluster_bus!=null

Context: 
@ override boolean equals ( object o ) { if ( ! ( o instanceof dataset _ config _ bean ) ) { return false ; } dataset _ config _ bean dc = ( dataset _ config _ bean ) o ; return objects . equals ( get _ id ( ) , dc . get _ id ( ) ) && objects . equals ( dataset , dc . get _ dataset ( ) ) && objects . equals ( dimensions , dc . get _ dimensions ( ) ) && objects . equals ( time _ column , dc . get _ time _ column ( ) ) && objects . equals ( time _ unit , dc . get _ time _ unit ( ) ) && objects . equals ( time _ duration , dc . get _ time _ duration ( ) ) && objects . equals ( time _ format , dc . get _ time _ format ( ) ) && objects . equals ( timezone , dc . get _ timezone ( ) ) && objects . equals ( data _ source , dc . get _ data _ source ( ) ) && objects . equals ( active , dc . is _ active ( ) ) && objects . equals ( additive , PRED ) && objects . equals ( dimensions _ have _ no _ pre _ aggregation , dc . get _ dimensions _ have _ no _ pre _ aggregation ( ) ) && objects . equals ( pre _ aggregated _ keyword , dc . get _ pre _ aggregated _ keyword ( ) ) && objects . equals ( non _ additive _ bucket _ unit , dc . get _ non _ additive _ bucket _ unit ( ) ) && objects . equals ( non _ additive _ bucket _ size , dc . get _ non _ additive _ bucket _ size ( ) ) && objects . equals ( realtime , dc . is _ realtime ( ) ) && objects . equals ( requires _ completeness _ check , dc . is _ requires _ completeness _ check ( ) ) && objects . equals ( expected _ delay , dc . get _ expected _ delay ( ) ) && objects . equals ( data _ completeness _ algorithm , dc . get _ data _ completeness _ algorithm ( ) ) && objects . equals ( expected _ completeness , dc . get _ expected _ completeness ( ) ) && objects . equals ( data _ source , dc . get _ data _ source ( ) ) ; }
Ground truth: dc.is_additive()
Syntactic prediction: dc.is_additive()
Baseline prediction: dc.get_additive()

Context: 
supervisor create ( final job job , final string existing _ container _ id , final map < string , integer > ports , final supervisor . listener listener ) { final restart _ policy policy = restart _ policy . new _ builder ( ) . build ( ) ; final task _ config task _ config = task _ config . builder ( ) . host ( host ) . job ( job ) . ports ( ports ) . env _ vars ( env _ vars ) . container _ decorators ( container _ decorators ) . namespace ( namespace ) . default _ registration _ domain ( default _ registration _ domain ) . dns ( dns ) . build ( ) ; final task _ status . builder task _ status = task _ status . new _ builder ( ) . set _ job ( job ) . set _ env ( task _ config . container _ env ( ) ) . set _ ports ( PRED ) ; final status _ updater status _ updater = new default _ status _ updater ( model , task _ status ) ; final flap _ controller flap _ controller = flap _ controller . create ( ) ; final task _ monitor task _ monitor = new task _ monitor ( job . get _ id ( ) , flap _ controller , status _ updater ) ; final health _ checker health _ checker = health _ checker _ factory . create ( task _ config , docker _ client , docker _ host , agent _ running _ in _ container ) ; final task _ runner _ factory runner _ factory = task _ runner _ factory . builder ( ) . config ( task _ config ) . registrar ( registrar ) . docker _ client ( docker _ client ) . health _ checker ( health _ checker ) . listener ( task _ monitor ) . build ( ) ; return supervisor . new _ builder ( ) . set _ job ( job ) . set _ existing _ container _ id ( existing _ container _ id ) . set _ docker _ client ( docker _ client ) . set _ restart _ policy ( policy ) . set _ metrics ( metrics ) . set _ listener ( listener ) . set _ runner _ factory ( runner _ factory ) . set _ status _ updater ( status _ updater ) . set _ monitor ( task _ monitor ) . build ( ) ; }
Ground truth: task_config.ports()
Syntactic prediction: task_config.ports()
Baseline prediction: ports.key_set()

Context: 
< i extends iface > map < string , org . apache . thrift . process _ function < i , ? extends org . apache . thrift . t _ base > > get _ process _ map ( map < string , org . apache . thrift . process _ function < i , ? extends org . apache . thrift . t _ base > > process _ map ) { process _ map . put ( " _ check _ consistency _ " , new check _ consistency ( ) ) ; process _ map . put ( " _ complete _ file _ " , new complete _ file ( ) ) ; process _ map . put ( " _ create _ directory _ " , new create _ directory ( ) ) ; process _ map . put ( " _ create _ file _ " , new create _ file ( ) ) ; process _ map . put ( " _ free _ " , new free ( ) ) ; process _ map . put ( " _ get _ status _ " , new get _ status ( ) ) ; process _ map . put ( " _ get _ new _ block _ id _ for _ file _ " , new get _ new _ block _ id _ for _ file ( ) ) ; process _ map . put ( " _ list _ status _ " , new list _ status ( ) ) ; process _ map . put ( " _ load _ metadata _ " , new load _ metadata ( ) ) ; process _ map . put ( " _ mount _ " , new mount ( ) ) ; process _ map . put ( " _ get _ mount _ table _ " , new get _ mount _ table ( ) ) ; process _ map . put ( " _ remove _ " , new remove ( ) ) ; process _ map . put ( " _ rename _ " , new rename ( ) ) ; process _ map . put ( " _ set _ attribute _ " , new set _ attribute ( ) ) ; process _ map . put ( " _ schedule _ async _ persistence _ " , new schedule _ async _ persistence ( ) ) ; process _ map . put ( " _ unmount _ " , PRED ) ; return process _ map ; }
Ground truth: newunmount()
Syntactic prediction: newunmount()
Baseline prediction: newschedule_async_persistence()

Context: 
synchronized void add _ connector _ internal ( materialized _ connector connector ) { check _ state ( ! stopped . get ( ) , " _ connector _ manager _ is stopped" ) ; connector _ id connector _ id = PRED ; check _ state ( ! connectors . contains _ key ( connector _ id ) , " _ a _ connector %s already exists" , connector _ id ) ; connectors . put ( connector _ id , connector ) ; split _ manager . add _ connector _ split _ manager ( connector _ id , connector . get _ split _ manager ( ) ) ; page _ source _ manager . add _ connector _ page _ source _ provider ( connector _ id , connector . get _ page _ source _ provider ( ) ) ; connector . get _ page _ sink _ provider ( ) . if _ present ( page _ sink _ provider -> page _ sink _ manager . add _ connector _ page _ sink _ provider ( connector _ id , page _ sink _ provider ) ) ; connector . get _ index _ provider ( ) . if _ present ( index _ provider -> index _ manager . add _ index _ provider ( connector _ id , index _ provider ) ) ; connector . get _ partitioning _ provider ( ) . if _ present ( partitioning _ provider -> node _ partitioning _ manager . add _ partitioning _ provider ( connector _ id , partitioning _ provider ) ) ; metadata _ manager . get _ procedure _ registry ( ) . add _ procedures ( connector _ id , connector . get _ procedures ( ) ) ; connector . get _ access _ control ( ) . if _ present ( access _ control -> access _ control _ manager . add _ catalog _ access _ control ( connector _ id , access _ control ) ) ; metadata _ manager . get _ table _ property _ manager ( ) . add _ properties ( connector _ id , connector . get _ table _ properties ( ) ) ; metadata _ manager . get _ schema _ property _ manager ( ) . add _ properties ( connector _ id , connector . get _ schema _ properties ( ) ) ; metadata _ manager . get _ session _ property _ manager ( ) . add _ connector _ session _ properties ( connector _ id , connector . get _ session _ properties ( ) ) ; }
Ground truth: connector.get_connector_id()
Syntactic prediction: connector.get_connector_id()
Baseline prediction: connector.get_id()

Context: 
void init ( context context ) { layout _ inflater . from ( context ) . inflate ( r . layout . widget _ content _ layout , this ) ; m _ progress _ view = ( progress _ view ) find _ view _ by _ id ( r . id . progress ) ; m _ tip _ view = ( text _ view ) find _ view _ by _ id ( PRED ) ; m _ content _ view = ( view _ group ) find _ view _ by _ id ( r . id . content _ view ) ; m _ refresh _ layout = ( refresh _ layout ) m _ content _ view . find _ view _ by _ id ( r . id . refresh _ layout ) ; m _ fast _ scroller = ( fast _ scroller ) m _ content _ view . find _ view _ by _ id ( r . id . fast _ scroller ) ; m _ recycler _ view = ( easy _ recycler _ view ) m _ refresh _ layout . find _ view _ by _ id ( r . id . recycler _ view ) ; m _ fast _ scroller . attach _ to _ recycler _ view ( m _ recycler _ view ) ; handler _ drawable drawable = new handler _ drawable ( ) ; drawable . set _ color ( resources _ utils . get _ attr _ color ( context , r . attr . color _ accent ) ) ; m _ fast _ scroller . set _ handler _ drawable ( drawable ) ; m _ refresh _ layout . set _ header _ color _ scheme _ resources ( r . color . loading _ indicator _ red , r . color . loading _ indicator _ purple , r . color . loading _ indicator _ blue , r . color . loading _ indicator _ cyan , r . color . loading _ indicator _ green , r . color . loading _ indicator _ yellow ) ; m _ refresh _ layout . set _ footer _ color _ scheme _ resources ( r . color . loading _ indicator _ red , r . color . loading _ indicator _ blue , r . color . loading _ indicator _ green , r . color . loading _ indicator _ orange ) ; m _ recycler _ view _ origin _ top = m _ recycler _ view . get _ padding _ top ( ) ; m _ recycler _ view _ origin _ bottom = m _ recycler _ view . get _ padding _ bottom ( ) ; }
Ground truth: r.id.tip
Syntactic prediction: r.id.tip
Baseline prediction: r.id.tip_view

Context: 
@ get @ path ( " _ /histogram" ) @ timed @ api _ operation ( value = " _ datetime _ histogram of a query using an absolute timerange." ) @ api _ responses ( value = { @ api _ response ( code = 400 , message = " _ invalid _ timerange parameters provided." ) , @ api _ response ( code = 400 , message = " _ invalid _ interval provided." ) } ) @ produces ( PRED ) histogram _ result histogram _ absolute ( @ api _ param ( name = " _ query _ " , value = " _ query _ (lucene syntax)" , required = true ) @ query _ param ( " _ query _ " ) @ not _ empty string query , @ api _ param ( name = " _ interval _ " , value = " _ histogram _ interval / bucket size. (year, quarter, month, week, day, hour or minute)" , required = true ) @ query _ param ( " _ interval _ " ) @ not _ empty string interval , @ api _ param ( name = " _ from _ " , value = " _ timerange _ start. see search method description for date format" , required = true ) @ query _ param ( " _ from _ " ) string from , @ api _ param ( name = " _ to _ " , value = " _ timerange _ end. see search method description for date format" , required = true ) @ query _ param ( " _ to _ " ) string to , @ api _ param ( name = " _ filter _ " , value = " _ filter _ " , required = false ) @ query _ param ( " _ filter _ " ) string filter ) { check _ search _ permission ( filter , rest _ permissions . searches _ absolute ) ; interval = interval . to _ upper _ case ( locale . english ) ; validate _ interval ( interval ) ; return build _ histogram _ result ( searches . histogram ( query , searches . date _ histogram _ interval . value _ of ( interval ) , filter , build _ absolute _ time _ range ( from , to ) ) ) ; }
Ground truth: media_type.application_json
Syntactic prediction: media_type.application_json
Baseline prediction: {media_type.application_json}

Context: 
upload _ notification _ config get _ notification _ config ( final string upload _ id , @ string _ res int title ) { upload _ notification _ config config = new upload _ notification _ config ( ) ; pending _ intent click _ intent = pending _ intent . get _ activity ( this , 1 , new intent ( this , main _ activity . class ) , pending _ intent . flag _ update _ current ) ; config . set _ title _ for _ all _ statuses ( get _ string ( title ) ) . set _ ring _ tone _ enabled ( true ) . set _ click _ intent _ for _ all _ statuses ( click _ intent ) . set _ clear _ on _ action _ for _ all _ statuses ( true ) ; config . get _ progress ( ) . message = get _ string ( r . string . uploading ) ; config . get _ progress ( ) . icon _ resource _ id = r . drawable . ic _ upload ; config . get _ progress ( ) . icon _ color _ resource _ id = color . blue ; config . get _ progress ( ) . actions . add ( new upload _ notification _ action ( r . drawable . ic _ cancelled , get _ string ( r . string . cancel _ upload ) , notification _ actions . get _ cancel _ upload _ action ( this , 1 , upload _ id ) ) ) ; config . get _ completed ( ) . message = get _ string ( r . string . upload _ success ) ; config . get _ completed ( ) . icon _ resource _ id = r . drawable . ic _ upload _ success ; config . get _ completed ( ) . icon _ color _ resource _ id = color . green ; config . get _ error ( ) . message = get _ string ( r . string . upload _ error ) ; config . get _ error ( ) . icon _ resource _ id = r . drawable . ic _ upload _ error ; config . get _ error ( ) . icon _ color _ resource _ id = color . red ; PRED = get _ string ( r . string . upload _ cancelled ) ; config . get _ cancelled ( ) . icon _ resource _ id = r . drawable . ic _ cancelled ; config . get _ cancelled ( ) . icon _ color _ resource _ id = color . yellow ; return config ; }
Ground truth: config.get_cancelled().message
Syntactic prediction: config.get_cancelled().message
Baseline prediction: config.get_error().description

Context: 
void add _ client _ sasl _ support ( config _ def config ) { config . define ( sasl _ configs . sasl _ kerberos _ service _ name , config _ def . type . string , null , config _ def . importance . medium , sasl _ configs . sasl _ kerberos _ service _ name _ doc ) . define ( sasl _ configs . sasl _ kerberos _ kinit _ cmd , config _ def . type . string , sasl _ configs . default _ kerberos _ kinit _ cmd , config _ def . importance . low , PRED ) . define ( sasl _ configs . sasl _ kerberos _ ticket _ renew _ window _ factor , config _ def . type . double , sasl _ configs . default _ kerberos _ ticket _ renew _ window _ factor , config _ def . importance . low , sasl _ configs . sasl _ kerberos _ ticket _ renew _ window _ factor _ doc ) . define ( sasl _ configs . sasl _ kerberos _ ticket _ renew _ jitter , config _ def . type . double , sasl _ configs . default _ kerberos _ ticket _ renew _ jitter , config _ def . importance . low , sasl _ configs . sasl _ kerberos _ ticket _ renew _ jitter _ doc ) . define ( sasl _ configs . sasl _ kerberos _ min _ time _ before _ relogin , config _ def . type . long , sasl _ configs . default _ kerberos _ min _ time _ before _ relogin , config _ def . importance . low , sasl _ configs . sasl _ kerberos _ min _ time _ before _ relogin _ doc ) . define ( sasl _ configs . sasl _ mechanism , config _ def . type . string , sasl _ configs . default _ sasl _ mechanism , config _ def . importance . medium , sasl _ configs . sasl _ mechanism _ doc ) . define ( sasl _ configs . sasl _ jaas _ config , config _ def . type . password , null , config _ def . importance . medium , sasl _ configs . sasl _ jaas _ config _ doc ) ; }
Ground truth: sasl_configs.sasl_kerberos_kinit_cmd_doc
Syntactic prediction: sasl_configs.sasl_kerberos_kinit_cmd_doc
Baseline prediction: sasl_configs.sasl_kerberos_kinit_doc

Context: 
list < h _ base _ source > split _ based _ on _ regions ( list < h _ region _ location > region _ locations , int num _ splits ) throws exception { final scan scan = read . serializable _ scan . get ( ) ; byte [ ] start _ row = scan . get _ start _ row ( ) ; byte [ ] stop _ row = scan . get _ stop _ row ( ) ; final list < h _ base _ source > sources = new array _ list < > ( num _ splits ) ; final boolean scan _ with _ no _ lower _ bound = start _ row . length == 0 ; final boolean scan _ with _ no _ upper _ bound = stop _ row . length == 0 ; for ( h _ region _ location region _ location : region _ locations ) { final byte [ ] start _ key = region _ location . get _ region _ info ( ) . get _ start _ key ( ) ; final byte [ ] end _ key = region _ location . get _ region _ info ( ) . get _ end _ key ( ) ; boolean is _ last _ region = end _ key . length == 0 ; string host = region _ location . get _ hostname _ port ( ) ; final byte [ ] split _ start = ( scan _ with _ no _ lower _ bound || bytes . compare _ to ( start _ key , start _ row ) >= 0 ) ? start _ key : start _ row ; final byte [ ] split _ stop = ( scan _ with _ no _ upper _ bound || PRED ) && ! is _ last _ region ? end _ key : stop _ row ; log . debug ( " _ {} {} {} {} {}" , sources . size ( ) , host , read . table _ id , bytes . to _ string ( split _ start ) , bytes . to _ string ( split _ stop ) ) ; scan new _ scan = new scan ( scan ) . set _ start _ row ( split _ start ) . set _ stop _ row ( split _ stop ) ; read new _ read = new read ( read . serializable _ configuration , read . table _ id , new serializable _ scan ( new _ scan ) ) ; sources . add ( new h _ base _ source ( new _ read , estimated _ size _ bytes ) ) ; } return sources ; }
Ground truth: bytes.compare_to(end_key,stop_row)<=0
Syntactic prediction: bytes.compare_to(end_key,stop_row)<=0
Baseline prediction: bytes.compare_to(stop_key,end_row)<=0

Context: 
@ setup @ suppress _ warnings ( " _ unchecked _ " ) void setup ( ) { elements = get _ random _ values ( container _ size , 0 ) ; expected _ aggregate = iterator . of ( elements ) . reduce ( jmh _ runner :: aggregate ) ; java _ mutable = create ( java . util . array _ list :: new , as _ list ( elements ) , v -> are _ equal ( v , as _ list ( elements ) ) ) ; java _ mutable _ linked = create ( java . util . linked _ list :: new , as _ list ( elements ) , v -> are _ equal ( v , as _ list ( elements ) ) ) ; scala _ mutable = create ( v -> ( scala . collection . mutable . mutable _ list < integer > ) scala . collection . mutable . mutable _ list _ $ . module _ $ . apply ( as _ scala _ buffer ( v ) ) , as _ list ( elements ) , v -> are _ equal ( as _ java _ collection ( v ) , java _ mutable ) ) ; scala _ persistent = create ( v -> scala . collection . immutable . list _ $ . module _ $ . apply ( as _ scala _ buffer ( v ) ) , java _ mutable , v -> are _ equal ( as _ java _ collection ( v ) , java _ mutable ) ) ; clojure _ persistent = create ( clojure . lang . persistent _ list :: create , java _ mutable , v -> are _ equal ( ( iterable < ? > ) v , java _ mutable ) ) ; fjava _ persistent = create ( v -> fj . data . list . from _ iterator ( PRED ) , java _ mutable , v -> are _ equal ( v , java _ mutable ) ) ; pcollections _ persistent = create ( org . pcollections . cons _ p _ stack :: from , java _ mutable , v -> are _ equal ( v , java _ mutable ) ) ; vavr _ persistent = create ( io . vavr . collection . list :: of _ all , java _ mutable , v -> are _ equal ( v , java _ mutable ) ) ; }
Ground truth: v.iterator()
Syntactic prediction: v.iterator()
Baseline prediction: fj.data::iterator

Context: 
com . google . protobuf . extension _ registry assign _ descriptors ( com . google . protobuf . descriptors . file _ descriptor root ) { descriptor = root ; internal _ static _ alluxio _ proto _ journal _ delete _ lineage _ entry _ descriptor = get _ descriptor ( ) . get _ message _ types ( ) . get ( 0 ) ; internal _ static _ alluxio _ proto _ journal _ delete _ lineage _ entry _ field _ accessor _ table = new com . google . protobuf . generated _ message . field _ accessor _ table ( internal _ static _ alluxio _ proto _ journal _ delete _ lineage _ entry _ descriptor , new java . lang . string [ ] { " _ lineage _ id _ " , " _ cascade _ " } ) ; internal _ static _ alluxio _ proto _ journal _ lineage _ entry _ descriptor = get _ descriptor ( ) . get _ message _ types ( ) . get ( 1 ) ; internal _ static _ alluxio _ proto _ journal _ lineage _ entry _ field _ accessor _ table = new com . google . protobuf . generated _ message . field _ accessor _ table ( internal _ static _ alluxio _ proto _ journal _ lineage _ entry _ descriptor , new java . lang . string [ ] { " _ id _ " , " _ input _ files _ " , " _ output _ file _ ids _ " , " _ job _ command _ " , " _ job _ output _ path _ " , " _ creation _ time _ ms _ " } ) ; internal _ static _ alluxio _ proto _ journal _ lineage _ id _ generator _ entry _ descriptor = PRED ; internal _ static _ alluxio _ proto _ journal _ lineage _ id _ generator _ entry _ field _ accessor _ table = new com . google . protobuf . generated _ message . field _ accessor _ table ( internal _ static _ alluxio _ proto _ journal _ lineage _ id _ generator _ entry _ descriptor , new java . lang . string [ ] { " _ sequence _ number _ " } ) ; return null ; }
Ground truth: get_descriptor().get_message_types().get(2)
Syntactic prediction: get_descriptor().get_message_types().get(2)
Baseline prediction: get_descriptor().get_message_types().get(0)

Context: 
void init _ inner _ rect ( context context , attribute _ set attrs ) { typed _ array ta = context . obtain _ styled _ attributes ( attrs , r . styleable . viewfinder _ view ) ; float inner _ margin _ top = ta . get _ dimension ( r . styleable . viewfinder _ view _ inner _ margintop , - 1 ) ; if ( inner _ margin _ top != - 1 ) { camera _ manager . frame _ margintop = ( int ) inner _ margin _ top ; } camera _ manager . frame _ width = ( int ) ta . get _ dimension ( r . styleable . viewfinder _ view _ inner _ width , display _ util . screen _ width _ px / 2 ) ; camera _ manager . frame _ height = ( int ) ta . get _ dimension ( PRED , display _ util . screen _ width _ px / 2 ) ; innercornercolor = ta . get _ color ( r . styleable . viewfinder _ view _ inner _ corner _ color , color . parse _ color ( " _ #45dddd" ) ) ; innercornerlength = ( int ) ta . get _ dimension ( r . styleable . viewfinder _ view _ inner _ corner _ length , 65 ) ; innercornerwidth = ( int ) ta . get _ dimension ( r . styleable . viewfinder _ view _ inner _ corner _ width , 15 ) ; drawable drawable = ta . get _ drawable ( r . styleable . viewfinder _ view _ inner _ scan _ bitmap ) ; if ( drawable != null ) { } scan _ light = bitmap _ factory . decode _ resource ( get _ resources ( ) , ta . get _ resource _ id ( r . styleable . viewfinder _ view _ inner _ scan _ bitmap , r . drawable . scan _ light ) ) ; scan _ velocity = ta . get _ int ( r . styleable . viewfinder _ view _ inner _ scan _ speed , 5 ) ; is _ circle = ta . get _ boolean ( r . styleable . viewfinder _ view _ inner _ scan _ iscircle , true ) ; ta . recycle ( ) ; }
Ground truth: r.styleable.viewfinder_view_inner_height
Syntactic prediction: r.styleable.viewfinder_view_inner_height
Baseline prediction: r.styleable.view_inner_height

Context: 
@ suppress _ fb _ warnings ( " _ dls _ dead _ local _ store _ " ) void init _ hash _ tree _ state ( o _ atomic _ operation atomic _ operation ) throws io _ exception { for ( long page _ index = 0 ; page _ index < max _ level _ size ; page _ index ++ ) { final o _ cache _ entry cache _ entry = load _ page _ entry _ for _ write ( page _ index , 0 , atomic _ operation ) ; try { final o _ hash _ index _ bucket < k , v > empty _ bucket = new o _ hash _ index _ bucket < k , v > ( max _ level _ depth , cache _ entry , key _ serializer , value _ serializer , key _ types ) ; } finally { release _ page _ from _ write ( atomic _ operation , cache _ entry ) ; } } final long [ ] root _ tree = new long [ max _ level _ size ] ; for ( int i = 0 ; PRED ; i ++ ) root _ tree [ i ] = create _ bucket _ pointer ( i , 0 ) ; directory . clear ( ) ; directory . add _ new _ node ( ( byte ) 0 , ( byte ) 0 , ( byte ) max _ level _ depth , root _ tree ) ; o _ cache _ entry hash _ state _ entry = load _ page _ for _ write ( atomic _ operation , file _ state _ id , hash _ state _ entry _ index , true ) ; try { o _ hash _ index _ file _ level _ metadata _ page metadata _ page = new o _ hash _ index _ file _ level _ metadata _ page ( hash _ state _ entry , false ) ; metadata _ page . set _ buckets _ count ( 0 , max _ level _ size ) ; metadata _ page . set _ records _ count ( 0 ) ; } finally { release _ page _ from _ write ( atomic _ operation , hash _ state _ entry ) ; } }
Ground truth: i<max_level_size
Syntactic prediction: i<max_level_size
Baseline prediction: i<root_tree.length

Context: 
@ delete @ timed @ api _ operation ( value = " _ delete _ an extractor" ) @ path ( " _ /{extractorid}" ) @ api _ responses ( value = { @ api _ response ( code = 400 , message = " _ invalid _ request." ) , @ api _ response ( code = 404 , message = " _ input _ not found." ) , @ api _ response ( code = 404 , message = " _ extractor _ not found." ) } ) @ produces ( media _ type . application _ json ) @ audit _ event ( type = audit _ event _ types . extractor _ delete ) void terminate ( @ api _ param ( name = " _ input _ id _ " , required = true ) @ path _ param ( " _ input _ id _ " ) string input _ id , @ api _ param ( name = " _ extractor _ id _ " , required = true ) @ path _ param ( " _ extractor _ id _ " ) string extractor _ id ) throws not _ found _ exception { check _ permission ( rest _ permissions . inputs _ edit , input _ id ) ; final message _ input input = persisted _ inputs . get ( input _ id ) ; if ( input == null ) { log . error ( " _ input _ <{}> not found." , input _ id ) ; throw new javax . ws . rs . not _ found _ exception ( " _ couldn _ 't find input " + input _ id ) ; } final input mongo _ input = input _ service . find ( input . get _ persist _ id ( ) ) ; final extractor extractor = input _ service . get _ extractor ( mongo _ input , extractor _ id ) ; input _ service . remove _ extractor ( mongo _ input , PRED ) ; final string msg = " _ deleted _ extractor <" + extractor _ id + " _ > of type [" + extractor . get _ type ( ) + " _ ] " + " _ from _ input <" + input _ id + " _ >." ; log . info ( msg ) ; activity _ writer . write ( new activity ( msg , inputs _ resource . class ) ) ; }
Ground truth: extractor.get_id()
Syntactic prediction: extractor.get_id()
Baseline prediction: extractor.get_class()

Context: 
@ override iterable < windowed _ value < kv < k , iterable < input _ t > > > > call ( windowed _ value < kv < k , iterable < windowed _ value < input _ t > > > > windowed _ value ) throws exception { k key = PRED ; iterable < windowed _ value < input _ t > > values = windowed _ value . get _ value ( ) . get _ value ( ) ; in _ memory _ timer _ internals timer _ internals = new in _ memory _ timer _ internals ( ) ; timer _ internals . advance _ processing _ time ( instant . now ( ) ) ; timer _ internals . advance _ synchronized _ processing _ time ( instant . now ( ) ) ; state _ internals state _ internals = state _ internals _ factory . state _ internals _ for _ key ( key ) ; gabw _ output _ windowed _ value < k , input _ t > outputter = new gabw _ output _ windowed _ value < > ( ) ; reduce _ fn _ runner < k , input _ t , iterable < input _ t > , w > reduce _ fn _ runner = new reduce _ fn _ runner < > ( key , windowing _ strategy , executable _ trigger _ state _ machine . create ( trigger _ state _ machines . state _ machine _ for _ trigger ( trigger _ translation . to _ proto ( windowing _ strategy . get _ trigger ( ) ) ) ) , state _ internals , timer _ internals , outputter , new unsupported _ side _ input _ reader ( " _ group _ also _ by _ window _ " ) , reduce _ fn , options . get ( ) ) ; reduce _ fn _ runner . process _ elements ( values ) ; timer _ internals . advance _ input _ watermark ( bounded _ window . timestamp _ max _ value ) ; timer _ internals . advance _ processing _ time ( bounded _ window . timestamp _ max _ value ) ; timer _ internals . advance _ synchronized _ processing _ time ( bounded _ window . timestamp _ max _ value ) ; fire _ eligible _ timers ( timer _ internals , reduce _ fn _ runner ) ; reduce _ fn _ runner . persist ( ) ; return outputter . get _ outputs ( ) ; }
Ground truth: windowed_value.get_value().get_key()
Syntactic prediction: windowed_value.get_value().get_key()
Baseline prediction: windowed_value.get_key()

Context: 
@ override void read _ persisted ( ) { user _ payload persisted = storage . init _ and _ get _ persisted _ with _ file _ name ( " _ user _ payload _ " , 100 ) ; user _ payload = persisted != null ? persisted : new user _ payload ( ) ; check _ not _ null ( user _ payload . get _ payment _ accounts ( ) , " _ user _ payload _ .getpaymentaccounts() must not be null" ) ; check _ not _ null ( user _ payload . get _ accepted _ language _ locale _ codes ( ) , " _ user _ payload _ .getacceptedlanguagelocalecodes() must not be null" ) ; payment _ accounts _ as _ observable = fx _ collections . observable _ set ( user _ payload . get _ payment _ accounts ( ) ) ; current _ payment _ account _ property = new simple _ object _ property < > ( PRED ) ; user _ payload . set _ account _ id ( string . value _ of ( math . abs ( key _ ring . get _ pub _ key _ ring ( ) . hash _ code ( ) ) ) ) ; if ( ! user _ payload . get _ accepted _ language _ locale _ codes ( ) . contains ( language _ util . get _ default _ language _ locale _ as _ code ( ) ) ) user _ payload . get _ accepted _ language _ locale _ codes ( ) . add ( language _ util . get _ default _ language _ locale _ as _ code ( ) ) ; string english = language _ util . get _ english _ language _ locale _ code ( ) ; if ( ! user _ payload . get _ accepted _ language _ locale _ codes ( ) . contains ( english ) ) user _ payload . get _ accepted _ language _ locale _ codes ( ) . add ( english ) ; payment _ accounts _ as _ observable . add _ listener ( ( set _ change _ listener < payment _ account > ) change -> { user _ payload . set _ payment _ accounts ( new hash _ set < > ( payment _ accounts _ as _ observable ) ) ; persist ( ) ; } ) ; current _ payment _ account _ property . add _ listener ( ( ov ) -> { user _ payload . set _ current _ payment _ account ( current _ payment _ account _ property . get ( ) ) ; persist ( ) ; } ) ; }
Ground truth: user_payload.get_current_payment_account()
Syntactic prediction: user_payload.get_current_payment_account()
Baseline prediction: payment_account.class

Context: 
@ override synchronized void replace _ table ( string database _ name , string table _ name , table new _ table , principal _ privileges principal _ privileges ) { table table = get _ required _ table ( database _ name , table _ name ) ; if ( ! table . get _ table _ type ( ) . equals ( virtual _ view . name ( ) ) || ! new _ table . get _ table _ type ( ) . equals ( virtual _ view . name ( ) ) ) { throw new presto _ exception ( hive _ metastore _ error , " _ only _ views can be updated with replacetable" ) ; } if ( ! table . get _ database _ name ( ) . equals ( database _ name ) || ! table . get _ table _ name ( ) . equals ( table _ name ) ) { throw new presto _ exception ( hive _ metastore _ error , " _ replacement _ table must have same name" ) ; } path table _ metadata _ directory = get _ table _ metadata _ directory ( table ) ; write _ schema _ file ( " _ table _ " , table _ metadata _ directory , table _ codec , new table _ metadata ( new _ table ) , true ) ; delete _ table _ privileges ( table ) ; for ( entry < string , collection < hive _ privilege _ info > > entry : principal _ privileges . get _ user _ privileges ( ) . as _ map ( ) . entry _ set ( ) ) { set _ table _ privileges ( entry . get _ key ( ) , user , table . get _ database _ name ( ) , table . get _ table _ name ( ) , entry . get _ value ( ) ) ; } for ( entry < string , collection < hive _ privilege _ info > > entry : PRED ) { set _ table _ privileges ( entry . get _ key ( ) , role , table . get _ database _ name ( ) , table . get _ table _ name ( ) , entry . get _ value ( ) ) ; } }
Ground truth: principal_privileges.get_role_privileges().as_map().entry_set()
Syntactic prediction: principal_privileges.get_role_privileges().as_map().entry_set()
Baseline prediction: principal_privileges.get_table_privileges().entry_set()

Context: 
@ override o _ remote _ task get _ fix _ task ( final o _ distributed _ request i _ request , final o _ remote _ task i _ original _ task , final object i _ bad _ response , final object i _ good _ response , final string executor _ node _ name , final o _ distributed _ server _ manager d _ manager ) { if ( ! ( i _ good _ response instanceof o _ tx _ task _ result ) ) { o _ distributed _ server _ log . debug ( this , get _ node _ source ( ) , null , direction . none , " _ error _ on creating fix-task for request: '%s' because good response is not expected type: %s" , i _ request , i _ bad _ response ) ; return null ; } final o _ completed _ 2 _ pc _ task fix _ task = ( o _ completed _ 2 _ pc _ task ) d _ manager . get _ task _ factory _ manager ( ) . get _ factory _ by _ server _ name ( executor _ node _ name ) . create _ task ( o _ completed _ 2 _ pc _ task . factoryid ) ; fix _ task . init ( i _ request . get _ id ( ) , false , get _ partition _ key ( ) ) ; for ( int i = 0 ; i < tasks . size ( ) ; ++ i ) { final o _ abstract _ record _ replicated _ task t = tasks . get ( i ) ; final object bad _ result = i _ bad _ response == null ? null : i _ bad _ response instanceof throwable ? i _ bad _ response : PRED . results . get ( i ) ; final object good _ result = ( ( o _ tx _ task _ result ) i _ good _ response ) . results . get ( i ) ; final o _ remote _ task undo _ task = t . get _ fix _ task ( i _ request , t , bad _ result , good _ result , executor _ node _ name , d _ manager ) ; if ( undo _ task == null ) return null ; fix _ task . add _ fix _ task ( undo _ task ) ; } return fix _ task ; }
Ground truth: ((o_tx_task_result)i_bad_response)
Syntactic prediction: ((o_tx_task_result)i_bad_response)
Baseline prediction: ((o_abstract_record_result)i_original_task)

Context: 
type _ spec . builder create _ injector _ type _ spec ( component _ info component , class _ name injector _ class _ name ) { class _ name cn = get _ injector _ name _ of _ scope ( injector _ class _ name , component . get _ scope ( ) ) ; type _ spec . builder type _ spec _ builder = type _ spec . class _ builder ( PRED ) . add _ modifiers ( modifier . public ) . add _ annotation ( annotation _ spec . builder ( generated . class ) . add _ member ( " _ value _ " , " _ $s" , generator _ name ) . build ( ) ) ; class _ name top _ level _ injector _ class _ name = class _ name . get ( top _ level _ package _ string , get _ top _ level _ injector _ name ( component ) ) ; type _ spec _ builder . add _ field ( top _ level _ injector _ class _ name , top _ level _ injector _ field , modifier . private ) ; method _ spec . builder ctor _ spec = method _ spec . constructor _ builder ( ) . add _ modifiers ( modifier . public ) . add _ parameter ( top _ level _ injector _ class _ name , top _ level _ injector _ field ) ; ctor _ spec . add _ statement ( " _ this _ .$n = $n" , top _ level _ injector _ field , top _ level _ injector _ field ) ; if ( component _ tree . get ( component ) != null ) { class _ name containing _ injector _ class _ name = get _ injector _ name _ of _ scope ( injector _ class _ name , component _ tree . get ( component ) . get _ scope ( ) ) ; type _ spec _ builder . add _ field ( containing _ injector _ class _ name , containing _ packaged _ injector _ field , modifier . private ) ; ctor _ spec . add _ parameter ( containing _ injector _ class _ name , containing _ packaged _ injector _ field ) ; ctor _ spec . add _ statement ( " _ this _ .$n = $n" , containing _ packaged _ injector _ field , containing _ packaged _ injector _ field ) ; } type _ spec _ builder . add _ method ( ctor _ spec . build ( ) ) ; return type _ spec _ builder ; }
Ground truth: cn.simple_name()
Syntactic prediction: cn.simple_name()
Baseline prediction: cn.get_name()

Context: 
list < p _ collection _ view < ? > > get _ side _ inputs ( applied _ p _ transform < ? , ? , ? > application ) throws io _ exception { p _ transform < ? , ? > transform = application . get _ transform ( ) ; if ( transform instanceof par _ do . multi _ output ) { return ( ( par _ do . multi _ output < ? , ? > ) transform ) . get _ side _ inputs ( ) ; } sdk _ components sdk _ components = sdk _ components . create ( ) ; runner _ api . p _ transform par _ do _ proto = p _ transform _ translation . to _ proto ( application , sdk _ components ) ; par _ do _ payload payload = par _ do _ payload . parse _ from ( par _ do _ proto . get _ spec ( ) . get _ payload ( ) ) ; list < p _ collection _ view < ? > > views = new array _ list < > ( ) ; rehydrated _ components components = rehydrated _ components . for _ components ( sdk _ components . to _ components ( ) ) ; for ( map . entry < string , side _ input > side _ input _ entry : payload . get _ side _ inputs _ map ( ) . entry _ set ( ) ) { string side _ input _ tag = side _ input _ entry . get _ key ( ) ; runner _ api . side _ input side _ input = side _ input _ entry . get _ value ( ) ; p _ collection < ? > original _ p _ collection = check _ not _ null ( ( p _ collection < ? > ) PRED . get ( new tuple _ tag < > ( side _ input _ tag ) ) , " _ no _ input with tag %s" , side _ input _ tag ) ; views . add ( view _ from _ proto ( side _ input , side _ input _ tag , original _ p _ collection , par _ do _ proto , components ) ) ; } return views ; }
Ground truth: application.get_inputs()
Syntactic prediction: application.get_inputs()
Baseline prediction: application.get_input_map()

Context: 
void with _ comments ( comment comment , context context , notification thread , int accent _ color ) { android . app . notification to _ add = get _ notification ( comment . get _ user ( ) != null ? comment . get _ user ( ) . get _ login ( ) : " _ " , mark _ down _ provider . strip _ md _ text ( comment . get _ body ( ) ) , thread . get _ repository ( ) != null ? thread . get _ repository ( ) . get _ full _ name ( ) : " _ general _ " ) . set _ large _ icon ( bitmap _ factory . decode _ resource ( get _ resources ( ) , r . mipmap . ic _ launcher ) ) . set _ small _ icon ( PRED ) . set _ style ( new notification _ compat . big _ text _ style ( ) . set _ big _ content _ title ( comment . get _ user ( ) != null ? comment . get _ user ( ) . get _ login ( ) : " _ " ) . big _ text ( mark _ down _ provider . strip _ md _ text ( comment . get _ body ( ) ) ) ) . set _ when ( comment . get _ created _ at ( ) . get _ time ( ) ) . set _ show _ when ( true ) . add _ action ( r . drawable . ic _ github , context . get _ string ( r . string . open ) , get _ pending _ intent ( thread . get _ id ( ) , thread . get _ subject ( ) . get _ url ( ) ) ) . add _ action ( r . drawable . ic _ eye _ off , context . get _ string ( r . string . mark _ as _ read ) , get _ read _ only _ pending _ intent ( thread . get _ id ( ) , thread . get _ subject ( ) . get _ url ( ) ) ) . set _ content _ intent ( get _ pending _ intent ( thread . get _ id ( ) , thread . get _ subject ( ) . get _ url ( ) ) ) . set _ color ( accent _ color ) . set _ group ( notification _ group _ id ) . build ( ) ; show _ notification ( thread . get _ id ( ) , to _ add ) ; }
Ground truth: r.drawable.ic_notification
Syntactic prediction: r.drawable.ic_notification
Baseline prediction: r.drawable.ic_launcher

