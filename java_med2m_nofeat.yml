data:
    corpus_1:
        path_src: ../synpos/data/java-med2m-processed/train_src.txt
        path_tgt: ../synpos/data/java-med2m/java-med2m.TargetType.seq.train.target.txt
        transforms: [filterfeats, onmt_tokenize, inferfeats] #, filtertoolong]
        weight: 1
    valid:
        path_src: ../synpos/data/java-med-processed/val_src.txt
        path_tgt: ../synpos/data/java-med/java-med.TargetType.seq.val.target.txt
        transforms: [filterfeats, onmt_tokenize, inferfeats]

src_onmttok_kwargs: "{'mode': 'space'}"
tgt_onmttok_kwargs: "{'mode': 'space'}"

# # Vocab opts
src_vocab: java_med/data_shared/data.vocab.src
tgt_vocab: java_med/data_shared/data.vocab.src

save_data: java_med_data
overwrite: False

save_checkpoint_steps: 1000
keep_checkpoint: 20
seed: 3435
train_steps: 500000
valid_steps: 1000
warmup_steps: 8000
report_every: 100
share_embeddings: True
share_vocab: True
copy_attn: True
# Uri:
#early_stopping: 10
#early_stopping_criteria: accuracy
#src_vocab_size: 2000
src_vocab_size: 32000
valid_batch_size: 8192

decoder_type: transformer
encoder_type: transformer
word_vec_size: 512
rnn_size: 512
#layers: 6
enc_layers: 6
dec_layers: 6
transformer_ff: 2048
heads: 8

accum_count: [4]
accum_steps: [0]
optim: adam
adam_beta1: 0.9
adam_beta2: 0.998
decay_method: noam
learning_rate: 2.0
max_grad_norm: 0.0

queue_size: 1000
bucket_size: 32768
batch_size: 4096
batch_type: tokens
normalization: tokens
dropout: 0.1
attention_dropout: [0.1]
label_smoothing: 0.1

max_generator_batches: 2

param_init: 0.0
param_init_glorot: 'true'
position_encoding: 'true'

world_size: 2
gpu_ranks:
- 0
- 1

